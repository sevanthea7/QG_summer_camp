{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-13T00:43:48.770168800Z",
     "start_time": "2024-07-13T00:43:44.391307500Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#               enc_input         dec_input          dec_output\n",
    "sentences = [ [ '我 是 学 生 P', 'S I am a student', 'I am a student E' ],\n",
    "              [ '我 喜 欢 学 习', 'S I like learning P', 'I like learning P E' ],\n",
    "              [ '我 是 女 生 P', 'S I am a girl', 'I am a girl E' ] ]\n",
    "# S 开始符号，E 结束符号， P 占位符号（凑够一句话5个字）\n",
    "src_vocab = { 'P': 0, '我': 1, '是': 2, '学': 3, '生': 4, '喜': 5, '欢': 6, '习': 7, '女': 8 }\n",
    "src_i2w = { src_vocab[i] for i in src_vocab }\n",
    "src_size = len( src_vocab )                                     # 源字典尺寸\n",
    "\n",
    "tgt_vocab = { 'P': 0, 'S': 1, 'E': 2, 'I': 3, 'am': 4, 'a': 5, 'student': 6, 'like': 7, 'learning': 8, 'girl': 9 }\n",
    "tgt_i2w = { tgt_vocab[i] for i in tgt_vocab }\n",
    "tgt_size = len( tgt_vocab )                                     # 目标字典尺寸\n",
    "\n",
    "src_len = len( sentences[0][0].split( ' ' ) )                   # Encoder 输出的最大长度\n",
    "tgt_len = len( sentences[0][1].split( ' ' ) )                   # Decoder 输入输出的最大长度\n",
    "# sentences[0][1].split( ' ' )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-13T00:44:31.028952100Z",
     "start_time": "2024-07-13T00:44:31.001310900Z"
    }
   },
   "id": "2b1e4aa7aa9f1ce",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "把sentence转换成字典索引"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f63e93165299e7cc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "([[1, 2, 3, 4, 0], [1, 5, 6, 3, 7], [1, 2, 8, 4, 0]],\n [[1, 3, 4, 5, 6], [1, 3, 7, 8, 0], [1, 3, 4, 5, 9]],\n [[3, 4, 5, 6, 2], [3, 7, 8, 0, 2], [3, 4, 5, 9, 2]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_dict():\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "    for i in range( len( sentences ) ):\n",
    "        enc_input = [ [ src_vocab[n] for n in sentences[i][0].split() ] ]\n",
    "        dec_input = [ [ tgt_vocab[n] for n in sentences[i][1].split() ] ]\n",
    "        dec_output = [ [ tgt_vocab[n] for n in sentences[i][2].split() ] ]\n",
    "        enc_inputs.append( enc_input )\n",
    "        dec_inputs.append( dec_input )\n",
    "        dec_outputs.append( dec_output )\n",
    "    return torch.LongTensor( enc_inputs ), torch.LongTensor( dec_inputs ), torch.LongTensor( dec_outputs )\n",
    "# enc_inputs, dec_inputs, dec_outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-13T00:45:07.544972500Z",
     "start_time": "2024-07-13T00:45:07.521186900Z"
    }
   },
   "id": "6d98745edf8dc3d3",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MDataset( data.Dataset ):\n",
    "    def __init__( self, enc_inputs, dec_inputs, dec_outputs ):\n",
    "        super( MDataset, self ).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "        \n",
    "    def __getitem__( self, idx ):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "        \n",
    "    def __len__( self ):\n",
    "        return self.enc_inputs.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-13T00:45:07.700906300Z",
     "start_time": "2024-07-13T00:45:07.686686800Z"
    }
   },
   "id": "cffde3dd09039a9c",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d_model = 512               # 字符embedding的维度数\n",
    "d_ff = 2048                 # 向前传播的隐藏层维度\n",
    "d_q = 64\n",
    "d_v = 64                    # Q、V的维度\n",
    "n_layers = 6                # encoder, decoder 的个数\n",
    "n_heads = 8                 # Multi-Head Attention设置为8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-13T00:43:34.574471100Z"
    }
   },
   "id": "bdb499214f347601"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**位置编码**\n",
    "pos_table[1:, 0::2] 表示从第1行开始（跳过第0行），每一行的偶数列（从第0列开始，每隔两列）\n",
    "pos_table[1:, 1::2] 表示从第1行开始，每一行的奇数列（从第1列开始，每隔两列）\n",
    "偶数位置：$PE_{pos, i} = sin( pos/10000^{2i/d_{model}})$\n",
    "奇数位置：$PE(pos, i) = cos( pos/10000^{2i/d_{model}})$\n",
    "\n",
    "enc_inputs -> 数据张量 -> [ batch_size, seq_len, d_model ]      seq_len -> 每个batch的长度\n",
    "enc_inputs.size(1) -> 获得enc_inputs的序列长度\n",
    "pos_bas -> [ max_len, d_model ]     最大序列长度，模型维度"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55e3deff85922070"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class PositionalEncoding( nn.Module ):                                                  # 位置编码\n",
    "    def __init__( self, d_model, dropout = 0.1, max_len = 5000 ):\n",
    "        super( PositionalEncoding, self ).__init__()\n",
    "        for pos in range( max_len ):\n",
    "            if pos != 0:\n",
    "                pos_bas = np.array( [ pos / np.power( 10000, 2*i / d_model ) for i in range( d_model ) ] )\n",
    "            else:\n",
    "                np.zeros( d_model )\n",
    "        pos_bas[ 1:, 0::2 ] = np.sin( pos_bas[ 1:, 0::2 ] )\n",
    "        pos_bas[ 0, 1::2 ] = np.cos( pos_bas[ 0, 1::2 ] )\n",
    "        self.pos_bas = torch.FloatTensor( pos_bas )\n",
    "        \n",
    "        self.dropout = nn.Dropout( p = dropout )\n",
    "        \n",
    "        \n",
    "    def forward( self, enc_inputs ):\n",
    "        enc_inputs += self.pos_bas[ :enc_inputs.size(1), : ]\n",
    "        return self.dropout( enc_inputs )\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-13T00:43:34.575605900Z"
    }
   },
   "id": "9f871b3ed18c9be9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "input_Q/K/V -> 经过线性变换先变成W_Q/K/V -> view重塑张量[ batch_size, seq_len, n_heads, d_k ]，seq_len = -1表示自动推断重塑之后这个维度的大小 -> transpose 交换第一个维度和第二个维度 变为[ batch_size, n_heads, seq_len, d_k ]\n",
    "\n",
    "将注意力掩码 attn_mask 的维度扩展一维，并在第二维度上重复 n_heads 次，以匹配注意力张量的维度 -> [batch_size, n_heads, seq_len, seq_len]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcf7042cbc66694b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__( self ):\n",
    "        super( MultiHeadAttention, self ).__init__()\n",
    "        self.W_Q = nn.Linear( d_model, d_q * n_heads, bias = False )\n",
    "        self.W_K = nn.Linear( d_model, d_q * n_heads, bias = False )\n",
    "        self.W_V = nn.Linear( d_model, d_v * n_heads, bias = False )\n",
    "        self.fc = nn.Linear( d_v * n_heads, d_model, bias = False )\n",
    "        \n",
    "    def forward( self, input_Q, input_K, input_V, attn_mask ):\n",
    "        residual, batch_size = input_Q, input_Q.size( 0 )\n",
    "        Q = self.W_Q( input_Q ).view( batch_size, -1, n_heads, d_q ).transpose( 1, 2 )\n",
    "        K = self.W_K( input_K ).view( batch_size, -1, n_heads, d_q ).transpose( 1, 2 )\n",
    "        V = self.W_V( input_V ).view( batch_size, -1, n_heads, d_v ).transpose( 1, 2 )\n",
    "        \n",
    "        attn_mask = attn_mask.unsqueeze( 1 ).repeat( 1, n_heads, 1, 1 )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f004c7aee3366b10"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Transformer( nn.Module ):\n",
    "    def __init__( self ):\n",
    "        super( Transformer, self ).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.projection = nn.Linear( d_model, tgt_size, bias = False )\n",
    "        \n",
    "    def forward( self, enc_input, dec_input ):\n",
    "        enc_output, enc_self_attns = self.encoder( enc_input )\n",
    "        dec_output, dec_self_attns, dec_enc_attns = self.Decoder( dec_input, enc_input, enc_output )\n",
    "        dec_logits = self.projection( dec_output )\n",
    "        out = dec_logits.view( -1, dec_logits.size( -1 ) ), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-13T00:43:34.589204200Z",
     "start_time": "2024-07-13T00:43:34.576693100Z"
    }
   },
   "id": "e1377b8139ba6b3f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "class Encoder( nn.Module ):\n",
    "    def __init__( self ):\n",
    "        super( Encoder, self ).__init__()\n",
    "        self.src_emb = nn.Embedding( src_size, d_model )                # 源语言的vocabulary size\n",
    "        self.pos_emb = PositionalEmbedding( d_model )\n",
    "        self.layers = nn.ModuleList( [ EncoderLayer() for i in range(len())])\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-13T00:43:34.577764300Z"
    }
   },
   "id": "921150526a89de55",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "class Decoder( nn.Module ):\n",
    "    def __init__( self ):\n",
    "        super( Decoder, self ).__init__()\n",
    "        self.tgt_emb = nn.Embedding( tgt_size, d_model )                # 目标语言的vocabulary size\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-13T00:43:34.578766200Z"
    }
   },
   "id": "7e47c1680e570a0d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "225f86bc097efc3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
