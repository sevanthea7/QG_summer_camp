## 一. RNN 循环神经网络

- 隐藏层节点间有链接
- 考虑时间先后顺序的问题都可以使用RNN来解决

eg.

$h_1 = ux_1+ws_0$

$h_2 = ux_2 + ws_1$

$h_t = ux_t + ws_{t-1}$

- $w、u$始终相等（权重共享）
- 隐藏状态：现有输入 + 过去记忆   ->   只有短时记忆
- 链式



初始化隐藏层为torch.zeros -> 统一初始值，标准化，避免初始偏差

## 二. LSTM

- 重复单一的神经网络层 -> 四个交互的层： 3个Sigmoid（压缩到0、1之间）和2个tanh（压缩到-1、1之间）

- 记住重要的信息，遗忘不重要的 -> 如经过Sigmoid，1会保存，0会遗忘

#### 1.忘记门

*选择从细胞状态中丢弃什么信息

1）读取上一个输出$h_{t-1}$和当前输入的$x_t$

2）经过公式 & Sigmoid  -> 0 丢弃，1保存

$f_t = \sigma ( W_f ·[h_{t-1}, x_ t] + b_f )$

#### 2.输入门

1）读取上一个输出$h_{t-1}$和当前输入的$x_t$经过

2）经过sigmoid和tanh两个函数 -> sigmoid决定什么值将要更新，tanh生成一个新的向量$\tilde{C}_t$，加入到状态中 -> 细胞状态 $C_{t-1}$ 更新为 $C_t$

#### 3.细胞状态

旧状态与 $f_t$ 相乘，判断是否丢弃

$ C_t = f_t * C_{t-1} + i_t * \tilde{ C }_t$

#### 4.输出门

1）一个sigmoid层确定细胞状态的哪个部分输出出去

2）细胞状态经过tanh函数，结果与sigmoid的输出结果相乘，输出相乘结果

![](https://sevanthea7.oss-cn-beijing.aliyuncs.com/QGworks/202407110945457.png)