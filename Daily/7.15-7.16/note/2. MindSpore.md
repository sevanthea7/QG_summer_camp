# MindSpore

## 一. 基本语法

### （一）张量

#### 1.创建张量

- 直接数据生成

- numpy数组生成

- init初始化构造

  - `init`: 支持传入`initializer`的子类。如：下方示例中的`One()`和 `Normal()`
  - `shape`: 支持传入 `list`、`tuple`、 `int`。
  - `dtype`: 支持传入`mindspore.dtype`

  ~~~python
  from mindspore.common.initializer import One, Normal
  
  # 初始化一个全为一的张量
  tensor1 = mindspore.Tensor(shape=(2, 2), dtype=mindspore.float32, init=One())
  
  # 初始化一个正态分布的张量	
  tensor2 = mindspore.Tensor(shape=(2, 2), dtype=mindspore.float32, init=Normal())
  ~~~

  ~~~python
  tensor1.init_data()
  # -> Tensor(shape=[2, 2], dtype=Float32, value=
  [[1., 1.],
   [1., 1.]])
  # 仅输入一个 tensor1输出value会是uninitialized，查看经过init函数的结果需要.init_data()
  ~~~

- 继承另一个张量的属性（形状，dtype）形成新的张量

  传入的一定要是张量

  ~~~python
  x_data = Tensor( [ 1, 2, 3, 4 ] )
  x_ones = ops.ones_like( x_data )			# [ 1, 1, 1, 1 ]
  x_zeros = ops.zeros_like( x_data )			# [ 0, 0, 0, 0 ]
  ~~~

#### 2.索引

从0开始，负索引表示按倒序编制，`:`和`...`切片（效果相同）

#### 3.运算

`+、-、*、/、//`

#### 4.拼接

~~~python
data1 = Tensor( np.array( [[0, 1], [2, 3]] ).astype( np.float32 ) )
data2 = Tensor( np.array( [[4, 5], [6, 7]] ).astype( np.float32 ) )

output1 = ops.concat( ( data1, data2 ), axis = 0 )			# 规定在0轴的方向上拼接
output2 = ops.stack( [ data1, data2 ] ) 					# 叠起来形成三维的矩阵
~~~

#### 5.转换

`n = t.asnumpy()` & `t = Tensor.from_numpy( n )`

#### 6.稀疏张量

##### （1）CSRTensor

- `indptr`: 一维整数张量, 表示稀疏数据每一行的非零元素在`values`中的起始位置和终止位置,
- `indices`: 一维整数张量，表示稀疏张量非零元素在**列中**的位置, 与`values`长度相等
- `values`: 一维张量，表示`CSRTensor`相对应的非零元素的值，与`indices`长度相等。
- `shape`: 表示被压缩的稀疏张量的形状，数据类型为`Tuple`，目前仅支持二维`CSRTensor`。

~~~python
indptr = Tensor( [0, 1, 2] )
indices = Tensor( [0, 1] )
values = Tensor( [1, 2], dtype=mindspore.float32 )
shape = ( 2, 4 )

csr_tensor = CSRTensor( indptr, indices, values, shape )

'''
1 0 0 0
0 2 0 0
'''
~~~

##### （2）COOTensor

- `indices`: 二维整数张量，每行代表非零元素下标。形状：`[N, ndims]`
- `values`: 一维张量，表示相对应的非零元素的值。形状：`[N]`
- `shape`: 表示被压缩的稀疏张量的形状，目前仅支持二维`COOTensor`

~~~python
indices = Tensor( [[0, 1], [1, 2]], dtype=mindspore.int32 )		# (0, 1) & (1, 2)
values = Tensor( [1, 2], dtype=mindspore.float32 )
shape = ( 3, 4 )

coo_tensor = COOTensor( indices, values, shape )
'''
0 1 0 0
0 0 2 0
0 0 0 0
'''
~~~



### （二）数据集

#### 1.下载数据集

~~~python
from download import download
url = '...' \ '...'
path = download( url, '下载地址', kind = 'zip', repalce = True )
~~~

#### 2.划分训练集测试集

~~~python
train_dataset = MnistDataset( 'MNIST_Data/train', shuffle = False )
~~~

引号内为文件地址，如果是存在了`../data`文件夹中则要写成`../data/MNIST_Data/train`

#### 3.数据集迭代

~~~python
def visualize( dataset ):
        figure = plt.figure( figsize = ( 4, 4 ) )
        cols, rows = 3, 3
    
        plt.subplots_adjust( wspace = 0.5, hspace = 0.5 )		# 设置子图间距
    
        for i, ( img, label ) in enumerate( dataset.create_tuple_iterator() ):
                figure.add_subplot( rows, cols, i + 1 )			
                # rows&cols决定网格大小，i+1决定图片位置
                plt.title( int( label ) )
                plt.imshow( img.asnumpy().squeeze(), cmap = 'gray' )
                if i == cols * rows - 1:
                        break
        plt.show()
~~~

#### 4.数据集常用操作

##### （1）shuffle

~~~pythom
train_dataset = train_dataset.shuffle( buffer_size = 64 )
~~~

buffer_size -> 混洗数据过程中使用的缓冲区大小：混洗算法会先读取 `buffer_size` 个样本到缓冲区，然后从中随机选择样本进行输出，并不断添加新的样本到缓冲区，直到整个数据集都被处理完

##### （2）map

针对数据集指定列添加Transforms，将数据变换应用于该列数据的每个元素，并返回包含变换后元素的新数据集

##### （3）batch

~~~python
train_dataset = train_dataset.batch( batch_size = 32 )
image, label = next(train_dataset.create_tuple_iterator())
# shape -> [ 32, 28, 28, 1 ]
~~~

#### 5.自定义数据集

~~~python
class RandomAccessDataset:
        def __init__( self ):
                self._data = np.ones( ( 5, 2 ) )
                self._label = np.ones( ( 5, 1 ) )
                
        def __getitem__( self, idx ):
                return self._data[idx], self._label[idx]
        
        def __len__( self ):
                return len( self._data )


loader = RandomAccessDataset()
dataset = GeneratorDataset( source = loader, column_names = [ 'data', 'label' ] )

for data in dataset:
        print( data )
~~~

**loader、source = loader**

也可以通过列表或元组的形式

~~~python
loader = [ np.array( 0 ), np.array( 1 ), np.array( 2 ) ]
dataset = GeneratorDataset( source = loader, column_names = [ 'data' ] )
~~~

#### 6.可迭代数据集

使不了

#### 7.生成器

使不了



### （三）数据变换

#### 1.rescale

对图像像素进行调整 $output_i = input_i * rescale + shift$

`rescale`：缩放因子

`shift`：平移因子

~~~python
random_np = np.random.randint( 0, 255, ( 48, 48 ), np.uint8 )
random_image = Image.fromarray( random_np )

rescale = vision.Rescale( 1.0 / 255.0, 0 )
rescale_image = rescale( random_image )
~~~

#### 2.Normalize

- `mean`：图像每个通道的均值
- `std`：图像每个通道的标准差
- `is_hwc`：bool值，输入图像的格式 -> True为(height, width, channel)，False为(channel, height, width)

$output_c = \frac{input_c - mean_c}{std_c}$  -> 图像的每个通道根据 `mean` 和 `std` 进行调整

~~~
normalize = vision.Normalize( mean = ( 0.1307, ), std = ( 0.3081, ) )
normalize_image = normalize( rescale_image )
~~~

#### 3.HWC2CHW

`(height, width, channel)` & `(channel, height, width)`

~~~pythoon
hwc_image = np.expand_dims( normalized_image, -1 )
hwc2chw = vision.HWC2CHW()
chw_image = hwc2chw( hwc_image )

hwc_image.shape, chw_image.shape
~~~

#### 4.Text Transforms

~~~python
texts = ['Welcome to Beijing']
test_dataset = GeneratorDataset(texts, 'text')

def my_tokenizer(content):
    return content.split()

test_dataset = test_dataset.map(text.PythonTokenizer(my_tokenizer))
print(next(test_dataset.create_tuple_iterator()))

vocab = text.Vocab.from_dataset(test_dataset)
# {'to': 2, 'Beijing': 0, 'Welcome': 1}

# map方法进行词表映射变换，将Token转为Index
test_dataset = test_dataset.map(text.Lookup(vocab))
print(next(test_dataset.create_tuple_iterator()))

~~~



### （四）网络构建

https://www.mindspore.cn/tutorials/zh-CN/r2.3.0rc2/beginner/model.html



## 二. 应用实例

### MNIST

1.必要的库

~~~python
import mindspore
from mindspore import nn
from mindspore.dataset import vision, transforms
from mindspore.dataset import MnistDataset
~~~

2.下载导入数据

- 基于Pipeline 的数据引擎，通过`mindspore.dataset`进行数据的预处理

~~~python
from download import download

url = 'https://,,,,'

'''
以Mnist数据集为例
url = "https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/" \
      "notebook/datasets/MNIST_Data.zip"
'''

path = download( url, '../data', kind = 'zip', replace = True )
~~~

3.数据处理

- 使用数据处理流水线，指定map, batch, shuffle 等操作

~~~python
def datapip( dataset, batch_size ):
	image_transforms = [
		vision.Rescale( 1.0 / 225.0, 0 ),		# 将输入的图像缩放为1/225
        vision.Normalize( mean = ( 0.1307, ), std = ( 0.3081, ) ),		# 均值和标准差值的归一化处理
        vision.HWC2CHW()
	]
    label_transform = transforms.TypeCast( mindspore.int32 )
    
    dataset = dataset.map( image_transforms, 'image' )
    dataset = dataset.map( label_transforms, 'label' )
    dataset = dataset.batch( batch_size )
    
    return dataset
~~~

