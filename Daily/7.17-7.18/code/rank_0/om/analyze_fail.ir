# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
For primitive[Conv2D], the x shape size must be equal to 4, but got 5.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\utils\check_convert_utils.cc:590 mindspore::CheckAndConvertUtils::CheckInteger

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417
        if not self.sense_flag:
# 1 In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418
            return self._no_sens_impl(*inputs)
                   ^
# 2 In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437
        if self.return_grad:
# 3 In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433
        loss = self.network(*inputs)
               ^
# 4 In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:121
        out = self._backbone(data)
              ^
# 5 In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:43
# 6 In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294
        for cell in self.cell_list:
# 7 In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:295
            input_data = cell(input_data)
                         ^
# 8 In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362
        if self.has_bias:
# 9 In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361
        output = self.conv2d(x, self.weight)
                 ^

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037
# Total subgraphs: 275

# Attrs:
training : 1

# Total params: 207
# Params:
%para1_inputs0 : <null>
%para2_inputs1 : <null>
%para3_features.0.weight : <Ref[Tensor[Float32]], (96, 3, 11, 11), ref_key=:features.0.weight>  :  has_default
%para4_features.3.conv1.weight : <Ref[Tensor[Float32]], (32, 96, 1, 1), ref_key=:features.3.conv1.weight>  :  has_default
%para5_features.3.bn1.gamma : <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn1.gamma>  :  has_default
%para6_features.3.bn1.beta : <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn1.beta>  :  has_default
%para7_features.3.conv2.weight : <Ref[Tensor[Float32]], (32, 32, 3, 3), ref_key=:features.3.conv2.weight>  :  has_default
%para8_features.3.bn2.gamma : <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn2.gamma>  :  has_default
%para9_features.3.bn2.beta : <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn2.beta>  :  has_default
%para10_features.3.conv3.weight : <Ref[Tensor[Float32]], (128, 32, 1, 1), ref_key=:features.3.conv3.weight>  :  has_default
%para11_features.3.bn3.gamma : <Ref[Tensor[Float32]], (128), ref_key=:features.3.bn3.gamma>  :  has_default
%para12_features.3.bn3.beta : <Ref[Tensor[Float32]], (128), ref_key=:features.3.bn3.beta>  :  has_default
%para13_features.3.downsample.0.weight : <Ref[Tensor[Float32]], (128, 96, 1, 1), ref_key=:features.3.downsample.0.weight>  :  has_default
%para14_features.3.downsample.1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:features.3.downsample.1.gamma>  :  has_default
%para15_features.3.downsample.1.beta : <Ref[Tensor[Float32]], (128), ref_key=:features.3.downsample.1.beta>  :  has_default
%para16_features.4.weight : <Ref[Tensor[Float32]], (256, 128, 5, 5), ref_key=:features.4.weight>  :  has_default
%para17_features.7.conv1.weight : <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:features.7.conv1.weight>  :  has_default
%para18_features.7.bn1.gamma : <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn1.gamma>  :  has_default
%para19_features.7.bn1.beta : <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn1.beta>  :  has_default
%para20_features.7.conv2.weight : <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:features.7.conv2.weight>  :  has_default
%para21_features.7.bn2.gamma : <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn2.gamma>  :  has_default
%para22_features.7.bn2.beta : <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn2.beta>  :  has_default
%para23_features.7.conv3.weight : <Ref[Tensor[Float32]], (512, 64, 1, 1), ref_key=:features.7.conv3.weight>  :  has_default
%para24_features.7.bn3.gamma : <Ref[Tensor[Float32]], (512), ref_key=:features.7.bn3.gamma>  :  has_default
%para25_features.7.bn3.beta : <Ref[Tensor[Float32]], (512), ref_key=:features.7.bn3.beta>  :  has_default
%para26_features.7.downsample.0.weight : <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:features.7.downsample.0.weight>  :  has_default
%para27_features.7.downsample.1.gamma : <Ref[Tensor[Float32]], (512), ref_key=:features.7.downsample.1.gamma>  :  has_default
%para28_features.7.downsample.1.beta : <Ref[Tensor[Float32]], (512), ref_key=:features.7.downsample.1.beta>  :  has_default
%para29_features.8.weight : <Ref[Tensor[Float32]], (768, 512, 3, 3), ref_key=:features.8.weight>  :  has_default
%para30_features.10.conv1.weight : <Ref[Tensor[Float32]], (128, 768, 1, 1), ref_key=:features.10.conv1.weight>  :  has_default
%para31_features.10.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn1.gamma>  :  has_default
%para32_features.10.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn1.beta>  :  has_default
%para33_features.10.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:features.10.conv2.weight>  :  has_default
%para34_features.10.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn2.gamma>  :  has_default
%para35_features.10.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn2.beta>  :  has_default
%para36_features.10.conv3.weight : <Ref[Tensor[Float32]], (768, 128, 1, 1), ref_key=:features.10.conv3.weight>  :  has_default
%para37_features.10.bn3.gamma : <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.gamma>  :  has_default
%para38_features.10.bn3.beta : <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.beta>  :  has_default
%para39_features.11.weight : <Ref[Tensor[Float32]], (1024, 768, 3, 3), ref_key=:features.11.weight>  :  has_default
%para40_features.13.conv1.weight : <Ref[Tensor[Float32]], (128, 1024, 1, 1), ref_key=:features.13.conv1.weight>  :  has_default
%para41_features.13.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn1.gamma>  :  has_default
%para42_features.13.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn1.beta>  :  has_default
%para43_features.13.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:features.13.conv2.weight>  :  has_default
%para44_features.13.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn2.gamma>  :  has_default
%para45_features.13.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn2.beta>  :  has_default
%para46_features.13.conv3.weight : <Ref[Tensor[Float32]], (1280, 128, 1, 1), ref_key=:features.13.conv3.weight>  :  has_default
%para47_features.13.bn3.gamma : <Ref[Tensor[Float32]], (1280), ref_key=:features.13.bn3.gamma>  :  has_default
%para48_features.13.bn3.beta : <Ref[Tensor[Float32]], (1280), ref_key=:features.13.bn3.beta>  :  has_default
%para49_features.13.downsample.0.weight : <Ref[Tensor[Float32]], (1280, 1024, 1, 1), ref_key=:features.13.downsample.0.weight>  :  has_default
%para50_features.13.downsample.1.gamma : <Ref[Tensor[Float32]], (1280), ref_key=:features.13.downsample.1.gamma>  :  has_default
%para51_features.13.downsample.1.beta : <Ref[Tensor[Float32]], (1280), ref_key=:features.13.downsample.1.beta>  :  has_default
%para52_classifier.1.weight : <Ref[Tensor[Float32]], (1024, 1280), ref_key=:classifier.1.weight>  :  has_default
%para53_classifier.1.bias : <Ref[Tensor[Float32]], (1024), ref_key=:classifier.1.bias>  :  has_default
%para54_classifier.4.weight : <Ref[Tensor[Float32]], (512, 1024), ref_key=:classifier.4.weight>  :  has_default
%para55_classifier.4.bias : <Ref[Tensor[Float32]], (512), ref_key=:classifier.4.bias>  :  has_default
%para56_classifier.6.weight : <Ref[Tensor[Float32]], (256, 512), ref_key=:classifier.6.weight>  :  has_default
%para57_classifier.6.bias : <Ref[Tensor[Float32]], (256), ref_key=:classifier.6.bias>  :  has_default
%para58_classifier.8.weight : <Ref[Tensor[Float32]], (5, 256), ref_key=:classifier.8.weight>  :  has_default
%para59_classifier.8.bias : <Ref[Tensor[Float32]], (5), ref_key=:classifier.8.bias>  :  has_default
%para60_moment1.features.0.weight : <Ref[Tensor[Float32]], (96, 3, 11, 11), ref_key=:moment1.features.0.weight>  :  has_default
%para61_moment1.features.3.conv1.weight : <Ref[Tensor[Float32]], (32, 96, 1, 1), ref_key=:moment1.features.3.conv1.weight>  :  has_default
%para62_moment1.features.3.bn1.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment1.features.3.bn1.gamma>  :  has_default
%para63_moment1.features.3.bn1.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment1.features.3.bn1.beta>  :  has_default
%para64_moment1.features.3.conv2.weight : <Ref[Tensor[Float32]], (32, 32, 3, 3), ref_key=:moment1.features.3.conv2.weight>  :  has_default
%para65_moment1.features.3.bn2.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment1.features.3.bn2.gamma>  :  has_default
%para66_moment1.features.3.bn2.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment1.features.3.bn2.beta>  :  has_default
%para67_moment1.features.3.conv3.weight : <Ref[Tensor[Float32]], (128, 32, 1, 1), ref_key=:moment1.features.3.conv3.weight>  :  has_default
%para68_moment1.features.3.bn3.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.3.bn3.gamma>  :  has_default
%para69_moment1.features.3.bn3.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.3.bn3.beta>  :  has_default
%para70_moment1.features.3.downsample.0.weight : <Ref[Tensor[Float32]], (128, 96, 1, 1), ref_key=:moment1.features.3.downsample.0.weight>  :  has_default
%para71_moment1.features.3.downsample.1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.3.downsample.1.gamma>  :  has_default
%para72_moment1.features.3.downsample.1.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.3.downsample.1.beta>  :  has_default
%para73_moment1.features.4.weight : <Ref[Tensor[Float32]], (256, 128, 5, 5), ref_key=:moment1.features.4.weight>  :  has_default
%para74_moment1.features.7.conv1.weight : <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:moment1.features.7.conv1.weight>  :  has_default
%para75_moment1.features.7.bn1.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moment1.features.7.bn1.gamma>  :  has_default
%para76_moment1.features.7.bn1.beta : <Ref[Tensor[Float32]], (64), ref_key=:moment1.features.7.bn1.beta>  :  has_default
%para77_moment1.features.7.conv2.weight : <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moment1.features.7.conv2.weight>  :  has_default
%para78_moment1.features.7.bn2.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moment1.features.7.bn2.gamma>  :  has_default
%para79_moment1.features.7.bn2.beta : <Ref[Tensor[Float32]], (64), ref_key=:moment1.features.7.bn2.beta>  :  has_default
%para80_moment1.features.7.conv3.weight : <Ref[Tensor[Float32]], (512, 64, 1, 1), ref_key=:moment1.features.7.conv3.weight>  :  has_default
%para81_moment1.features.7.bn3.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moment1.features.7.bn3.gamma>  :  has_default
%para82_moment1.features.7.bn3.beta : <Ref[Tensor[Float32]], (512), ref_key=:moment1.features.7.bn3.beta>  :  has_default
%para83_moment1.features.7.downsample.0.weight : <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:moment1.features.7.downsample.0.weight>  :  has_default
%para84_moment1.features.7.downsample.1.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moment1.features.7.downsample.1.gamma>  :  has_default
%para85_moment1.features.7.downsample.1.beta : <Ref[Tensor[Float32]], (512), ref_key=:moment1.features.7.downsample.1.beta>  :  has_default
%para86_moment1.features.8.weight : <Ref[Tensor[Float32]], (768, 512, 3, 3), ref_key=:moment1.features.8.weight>  :  has_default
%para87_moment1.features.10.conv1.weight : <Ref[Tensor[Float32]], (128, 768, 1, 1), ref_key=:moment1.features.10.conv1.weight>  :  has_default
%para88_moment1.features.10.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.10.bn1.gamma>  :  has_default
%para89_moment1.features.10.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.10.bn1.beta>  :  has_default
%para90_moment1.features.10.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moment1.features.10.conv2.weight>  :  has_default
%para91_moment1.features.10.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.10.bn2.gamma>  :  has_default
%para92_moment1.features.10.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.10.bn2.beta>  :  has_default
%para93_moment1.features.10.conv3.weight : <Ref[Tensor[Float32]], (768, 128, 1, 1), ref_key=:moment1.features.10.conv3.weight>  :  has_default
%para94_moment1.features.10.bn3.gamma : <Ref[Tensor[Float32]], (768), ref_key=:moment1.features.10.bn3.gamma>  :  has_default
%para95_moment1.features.10.bn3.beta : <Ref[Tensor[Float32]], (768), ref_key=:moment1.features.10.bn3.beta>  :  has_default
%para96_moment1.features.11.weight : <Ref[Tensor[Float32]], (1024, 768, 3, 3), ref_key=:moment1.features.11.weight>  :  has_default
%para97_moment1.features.13.conv1.weight : <Ref[Tensor[Float32]], (128, 1024, 1, 1), ref_key=:moment1.features.13.conv1.weight>  :  has_default
%para98_moment1.features.13.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.13.bn1.gamma>  :  has_default
%para99_moment1.features.13.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.13.bn1.beta>  :  has_default
%para100_moment1.features.13.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moment1.features.13.conv2.weight>  :  has_default
%para101_moment1.features.13.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.13.bn2.gamma>  :  has_default
%para102_moment1.features.13.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.13.bn2.beta>  :  has_default
%para103_moment1.features.13.conv3.weight : <Ref[Tensor[Float32]], (1280, 128, 1, 1), ref_key=:moment1.features.13.conv3.weight>  :  has_default
%para104_moment1.features.13.bn3.gamma : <Ref[Tensor[Float32]], (1280), ref_key=:moment1.features.13.bn3.gamma>  :  has_default
%para105_moment1.features.13.bn3.beta : <Ref[Tensor[Float32]], (1280), ref_key=:moment1.features.13.bn3.beta>  :  has_default
%para106_moment1.features.13.downsample.0.weight : <Ref[Tensor[Float32]], (1280, 1024, 1, 1), ref_key=:moment1.features.13.downsample.0.weight>  :  has_default
%para107_moment1.features.13.downsample.1.gamma : <Ref[Tensor[Float32]], (1280), ref_key=:moment1.features.13.downsample.1.gamma>  :  has_default
%para108_moment1.features.13.downsample.1.beta : <Ref[Tensor[Float32]], (1280), ref_key=:moment1.features.13.downsample.1.beta>  :  has_default
%para109_moment1.classifier.1.weight : <Ref[Tensor[Float32]], (1024, 1280), ref_key=:moment1.classifier.1.weight>  :  has_default
%para110_moment1.classifier.1.bias : <Ref[Tensor[Float32]], (1024), ref_key=:moment1.classifier.1.bias>  :  has_default
%para111_moment1.classifier.4.weight : <Ref[Tensor[Float32]], (512, 1024), ref_key=:moment1.classifier.4.weight>  :  has_default
%para112_moment1.classifier.4.bias : <Ref[Tensor[Float32]], (512), ref_key=:moment1.classifier.4.bias>  :  has_default
%para113_moment1.classifier.6.weight : <Ref[Tensor[Float32]], (256, 512), ref_key=:moment1.classifier.6.weight>  :  has_default
%para114_moment1.classifier.6.bias : <Ref[Tensor[Float32]], (256), ref_key=:moment1.classifier.6.bias>  :  has_default
%para115_moment1.classifier.8.weight : <Ref[Tensor[Float32]], (5, 256), ref_key=:moment1.classifier.8.weight>  :  has_default
%para116_moment1.classifier.8.bias : <Ref[Tensor[Float32]], (5), ref_key=:moment1.classifier.8.bias>  :  has_default
%para117_moment2.features.0.weight : <Ref[Tensor[Float32]], (96, 3, 11, 11), ref_key=:moment2.features.0.weight>  :  has_default
%para118_moment2.features.3.conv1.weight : <Ref[Tensor[Float32]], (32, 96, 1, 1), ref_key=:moment2.features.3.conv1.weight>  :  has_default
%para119_moment2.features.3.bn1.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment2.features.3.bn1.gamma>  :  has_default
%para120_moment2.features.3.bn1.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment2.features.3.bn1.beta>  :  has_default
%para121_moment2.features.3.conv2.weight : <Ref[Tensor[Float32]], (32, 32, 3, 3), ref_key=:moment2.features.3.conv2.weight>  :  has_default
%para122_moment2.features.3.bn2.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment2.features.3.bn2.gamma>  :  has_default
%para123_moment2.features.3.bn2.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment2.features.3.bn2.beta>  :  has_default
%para124_moment2.features.3.conv3.weight : <Ref[Tensor[Float32]], (128, 32, 1, 1), ref_key=:moment2.features.3.conv3.weight>  :  has_default
%para125_moment2.features.3.bn3.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.3.bn3.gamma>  :  has_default
%para126_moment2.features.3.bn3.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.3.bn3.beta>  :  has_default
%para127_moment2.features.3.downsample.0.weight : <Ref[Tensor[Float32]], (128, 96, 1, 1), ref_key=:moment2.features.3.downsample.0.weight>  :  has_default
%para128_moment2.features.3.downsample.1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.3.downsample.1.gamma>  :  has_default
%para129_moment2.features.3.downsample.1.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.3.downsample.1.beta>  :  has_default
%para130_moment2.features.4.weight : <Ref[Tensor[Float32]], (256, 128, 5, 5), ref_key=:moment2.features.4.weight>  :  has_default
%para131_moment2.features.7.conv1.weight : <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:moment2.features.7.conv1.weight>  :  has_default
%para132_moment2.features.7.bn1.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moment2.features.7.bn1.gamma>  :  has_default
%para133_moment2.features.7.bn1.beta : <Ref[Tensor[Float32]], (64), ref_key=:moment2.features.7.bn1.beta>  :  has_default
%para134_moment2.features.7.conv2.weight : <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moment2.features.7.conv2.weight>  :  has_default
%para135_moment2.features.7.bn2.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moment2.features.7.bn2.gamma>  :  has_default
%para136_moment2.features.7.bn2.beta : <Ref[Tensor[Float32]], (64), ref_key=:moment2.features.7.bn2.beta>  :  has_default
%para137_moment2.features.7.conv3.weight : <Ref[Tensor[Float32]], (512, 64, 1, 1), ref_key=:moment2.features.7.conv3.weight>  :  has_default
%para138_moment2.features.7.bn3.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moment2.features.7.bn3.gamma>  :  has_default
%para139_moment2.features.7.bn3.beta : <Ref[Tensor[Float32]], (512), ref_key=:moment2.features.7.bn3.beta>  :  has_default
%para140_moment2.features.7.downsample.0.weight : <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:moment2.features.7.downsample.0.weight>  :  has_default
%para141_moment2.features.7.downsample.1.gamma : <Ref[Tensor[Float32]], (512), ref_key=:moment2.features.7.downsample.1.gamma>  :  has_default
%para142_moment2.features.7.downsample.1.beta : <Ref[Tensor[Float32]], (512), ref_key=:moment2.features.7.downsample.1.beta>  :  has_default
%para143_moment2.features.8.weight : <Ref[Tensor[Float32]], (768, 512, 3, 3), ref_key=:moment2.features.8.weight>  :  has_default
%para144_moment2.features.10.conv1.weight : <Ref[Tensor[Float32]], (128, 768, 1, 1), ref_key=:moment2.features.10.conv1.weight>  :  has_default
%para145_moment2.features.10.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.10.bn1.gamma>  :  has_default
%para146_moment2.features.10.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.10.bn1.beta>  :  has_default
%para147_moment2.features.10.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moment2.features.10.conv2.weight>  :  has_default
%para148_moment2.features.10.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.10.bn2.gamma>  :  has_default
%para149_moment2.features.10.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.10.bn2.beta>  :  has_default
%para150_moment2.features.10.conv3.weight : <Ref[Tensor[Float32]], (768, 128, 1, 1), ref_key=:moment2.features.10.conv3.weight>  :  has_default
%para151_moment2.features.10.bn3.gamma : <Ref[Tensor[Float32]], (768), ref_key=:moment2.features.10.bn3.gamma>  :  has_default
%para152_moment2.features.10.bn3.beta : <Ref[Tensor[Float32]], (768), ref_key=:moment2.features.10.bn3.beta>  :  has_default
%para153_moment2.features.11.weight : <Ref[Tensor[Float32]], (1024, 768, 3, 3), ref_key=:moment2.features.11.weight>  :  has_default
%para154_moment2.features.13.conv1.weight : <Ref[Tensor[Float32]], (128, 1024, 1, 1), ref_key=:moment2.features.13.conv1.weight>  :  has_default
%para155_moment2.features.13.bn1.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.13.bn1.gamma>  :  has_default
%para156_moment2.features.13.bn1.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.13.bn1.beta>  :  has_default
%para157_moment2.features.13.conv2.weight : <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moment2.features.13.conv2.weight>  :  has_default
%para158_moment2.features.13.bn2.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.13.bn2.gamma>  :  has_default
%para159_moment2.features.13.bn2.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.13.bn2.beta>  :  has_default
%para160_moment2.features.13.conv3.weight : <Ref[Tensor[Float32]], (1280, 128, 1, 1), ref_key=:moment2.features.13.conv3.weight>  :  has_default
%para161_moment2.features.13.bn3.gamma : <Ref[Tensor[Float32]], (1280), ref_key=:moment2.features.13.bn3.gamma>  :  has_default
%para162_moment2.features.13.bn3.beta : <Ref[Tensor[Float32]], (1280), ref_key=:moment2.features.13.bn3.beta>  :  has_default
%para163_moment2.features.13.downsample.0.weight : <Ref[Tensor[Float32]], (1280, 1024, 1, 1), ref_key=:moment2.features.13.downsample.0.weight>  :  has_default
%para164_moment2.features.13.downsample.1.gamma : <Ref[Tensor[Float32]], (1280), ref_key=:moment2.features.13.downsample.1.gamma>  :  has_default
%para165_moment2.features.13.downsample.1.beta : <Ref[Tensor[Float32]], (1280), ref_key=:moment2.features.13.downsample.1.beta>  :  has_default
%para166_moment2.classifier.1.weight : <Ref[Tensor[Float32]], (1024, 1280), ref_key=:moment2.classifier.1.weight>  :  has_default
%para167_moment2.classifier.1.bias : <Ref[Tensor[Float32]], (1024), ref_key=:moment2.classifier.1.bias>  :  has_default
%para168_moment2.classifier.4.weight : <Ref[Tensor[Float32]], (512, 1024), ref_key=:moment2.classifier.4.weight>  :  has_default
%para169_moment2.classifier.4.bias : <Ref[Tensor[Float32]], (512), ref_key=:moment2.classifier.4.bias>  :  has_default
%para170_moment2.classifier.6.weight : <Ref[Tensor[Float32]], (256, 512), ref_key=:moment2.classifier.6.weight>  :  has_default
%para171_moment2.classifier.6.bias : <Ref[Tensor[Float32]], (256), ref_key=:moment2.classifier.6.bias>  :  has_default
%para172_moment2.classifier.8.weight : <Ref[Tensor[Float32]], (5, 256), ref_key=:moment2.classifier.8.weight>  :  has_default
%para173_moment2.classifier.8.bias : <Ref[Tensor[Float32]], (5), ref_key=:moment2.classifier.8.bias>  :  has_default
%para174_beta1_power : <Ref[Tensor[Float32]], (), ref_key=:beta1_power>  :  has_default
%para175_beta2_power : <Ref[Tensor[Float32]], (), ref_key=:beta2_power>  :  has_default
%para176_global_step : <Ref[Tensor[Int32]], (1), ref_key=:global_step>  :  has_default
%para177_learning_rate : <Ref[Tensor[Float32]], (), ref_key=:learning_rate>  :  has_default
%para178_features.3.bn3.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:features.3.bn3.moving_mean>  :  has_default
%para179_features.3.bn3.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:features.3.bn3.moving_variance>  :  has_default
%para180_features.7.bn3.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:features.7.bn3.moving_mean>  :  has_default
%para181_features.7.bn3.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:features.7.bn3.moving_variance>  :  has_default
%para182_features.10.bn3.moving_mean : <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.moving_mean>  :  has_default
%para183_features.10.bn3.moving_variance : <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.moving_variance>  :  has_default
%para184_features.13.bn3.moving_mean : <Ref[Tensor[Float32]], (1280), ref_key=:features.13.bn3.moving_mean>  :  has_default
%para185_features.13.bn3.moving_variance : <Ref[Tensor[Float32]], (1280), ref_key=:features.13.bn3.moving_variance>  :  has_default
%para186_features.3.bn2.moving_mean : <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn2.moving_mean>  :  has_default
%para187_features.3.bn2.moving_variance : <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn2.moving_variance>  :  has_default
%para188_features.7.bn2.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn2.moving_mean>  :  has_default
%para189_features.7.bn2.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn2.moving_variance>  :  has_default
%para190_features.10.bn2.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn2.moving_mean>  :  has_default
%para191_features.10.bn2.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn2.moving_variance>  :  has_default
%para192_features.13.bn2.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn2.moving_mean>  :  has_default
%para193_features.13.bn2.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn2.moving_variance>  :  has_default
%para194_features.3.bn1.moving_mean : <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn1.moving_mean>  :  has_default
%para195_features.3.bn1.moving_variance : <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn1.moving_variance>  :  has_default
%para196_features.7.bn1.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn1.moving_mean>  :  has_default
%para197_features.7.bn1.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn1.moving_variance>  :  has_default
%para198_features.10.bn1.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn1.moving_mean>  :  has_default
%para199_features.10.bn1.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn1.moving_variance>  :  has_default
%para200_features.13.bn1.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn1.moving_mean>  :  has_default
%para201_features.13.bn1.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn1.moving_variance>  :  has_default
%para202_features.3.downsample.1.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:features.3.downsample.1.moving_mean>  :  has_default
%para203_features.3.downsample.1.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:features.3.downsample.1.moving_variance>  :  has_default
%para204_features.7.downsample.1.moving_mean : <Ref[Tensor[Float32]], (512), ref_key=:features.7.downsample.1.moving_mean>  :  has_default
%para205_features.7.downsample.1.moving_variance : <Ref[Tensor[Float32]], (512), ref_key=:features.7.downsample.1.moving_variance>  :  has_default
%para206_features.13.downsample.1.moving_mean : <Ref[Tensor[Float32]], (1280), ref_key=:features.13.downsample.1.moving_mean>  :  has_default
%para207_features.13.downsample.1.moving_variance : <Ref[Tensor[Float32]], (1280), ref_key=:features.13.downsample.1.moving_variance>  :  has_default

subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037 : 000001D997C03160
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037(%para1_inputs0, %para2_inputs1, %para3_features.0.weight, %para4_features.3.conv1.weight, %para5_features.3.bn1.gamma, %para6_features.3.bn1.beta, %para7_features.3.conv2.weight, %para8_features.3.bn2.gamma, %para9_features.3.bn2.beta, %para10_features.3.conv3.weight, %para11_features.3.bn3.gamma, %para12_features.3.bn3.beta, %para13_features.3.downsample.0.weight, %para14_features.3.downsample.1.gamma, %para15_features.3.downsample.1.beta, %para16_features.4.weight, %para17_features.7.conv1.weight, %para18_features.7.bn1.gamma, %para19_features.7.bn1.beta, %para20_features.7.conv2.weight, %para21_features.7.bn2.gamma, %para22_features.7.bn2.beta, %para23_features.7.conv3.weight, %para24_features.7.bn3.gamma, %para25_features.7.bn3.beta, %para26_features.7.downsample.0.weight, %para27_features.7.downsample.1.gamma, %para28_features.7.downsample.1.beta, %para29_features.8.weight, %para30_features.10.conv1.weight, %para31_features.10.bn1.gamma, %para32_features.10.bn1.beta, %para33_features.10.conv2.weight, %para34_features.10.bn2.gamma, %para35_features.10.bn2.beta, %para36_features.10.conv3.weight, %para37_features.10.bn3.gamma, %para38_features.10.bn3.beta, %para39_features.11.weight, %para40_features.13.conv1.weight, %para41_features.13.bn1.gamma, %para42_features.13.bn1.beta, %para43_features.13.conv2.weight, %para44_features.13.bn2.gamma, %para45_features.13.bn2.beta, %para46_features.13.conv3.weight, %para47_features.13.bn3.gamma, %para48_features.13.bn3.beta, %para49_features.13.downsample.0.weight, %para50_features.13.downsample.1.gamma, %para51_features.13.downsample.1.beta, %para52_classifier.1.weight, %para53_classifier.1.bias, %para54_classifier.4.weight, %para55_classifier.4.bias, %para56_classifier.6.weight, %para57_classifier.6.bias, %para58_classifier.8.weight, %para59_classifier.8.bias, %para60_moment1.features.0.weight, %para61_moment1.features.3.conv1.weight, %para62_moment1.features.3.bn1.gamma, %para63_moment1.features.3.bn1.beta, %para64_moment1.features.3.conv2.weight, %para65_moment1.features.3.bn2.gamma, %para66_moment1.features.3.bn2.beta, %para67_moment1.features.3.conv3.weight, %para68_moment1.features.3.bn3.gamma, %para69_moment1.features.3.bn3.beta, %para70_moment1.features.3.downsample.0.weight, %para71_moment1.features.3.downsample.1.gamma, %para72_moment1.features.3.downsample.1.beta, %para73_moment1.features.4.weight, %para74_moment1.features.7.conv1.weight, %para75_moment1.features.7.bn1.gamma, %para76_moment1.features.7.bn1.beta, %para77_moment1.features.7.conv2.weight, %para78_moment1.features.7.bn2.gamma, %para79_moment1.features.7.bn2.beta, %para80_moment1.features.7.conv3.weight, %para81_moment1.features.7.bn3.gamma, %para82_moment1.features.7.bn3.beta, %para83_moment1.features.7.downsample.0.weight, %para84_moment1.features.7.downsample.1.gamma, %para85_moment1.features.7.downsample.1.beta, %para86_moment1.features.8.weight, %para87_moment1.features.10.conv1.weight, %para88_moment1.features.10.bn1.gamma, %para89_moment1.features.10.bn1.beta, %para90_moment1.features.10.conv2.weight, %para91_moment1.features.10.bn2.gamma, %para92_moment1.features.10.bn2.beta, %para93_moment1.features.10.conv3.weight, %para94_moment1.features.10.bn3.gamma, %para95_moment1.features.10.bn3.beta, %para96_moment1.features.11.weight, %para97_moment1.features.13.conv1.weight, %para98_moment1.features.13.bn1.gamma, %para99_moment1.features.13.bn1.beta, %para100_moment1.features.13.conv2.weight, %para101_moment1.features.13.bn2.gamma, %para102_moment1.features.13.bn2.beta, %para103_moment1.features.13.conv3.weight, %para104_moment1.features.13.bn3.gamma, %para105_moment1.features.13.bn3.beta, %para106_moment1.features.13.downsample.0.weight, %para107_moment1.features.13.downsample.1.gamma, %para108_moment1.features.13.downsample.1.beta, %para109_moment1.classifier.1.weight, %para110_moment1.classifier.1.bias, %para111_moment1.classifier.4.weight, %para112_moment1.classifier.4.bias, %para113_moment1.classifier.6.weight, %para114_moment1.classifier.6.bias, %para115_moment1.classifier.8.weight, %para116_moment1.classifier.8.bias, %para117_moment2.features.0.weight, %para118_moment2.features.3.conv1.weight, %para119_moment2.features.3.bn1.gamma, %para120_moment2.features.3.bn1.beta, %para121_moment2.features.3.conv2.weight, %para122_moment2.features.3.bn2.gamma, %para123_moment2.features.3.bn2.beta, %para124_moment2.features.3.conv3.weight, %para125_moment2.features.3.bn3.gamma, %para126_moment2.features.3.bn3.beta, %para127_moment2.features.3.downsample.0.weight, %para128_moment2.features.3.downsample.1.gamma, %para129_moment2.features.3.downsample.1.beta, %para130_moment2.features.4.weight, %para131_moment2.features.7.conv1.weight, %para132_moment2.features.7.bn1.gamma, %para133_moment2.features.7.bn1.beta, %para134_moment2.features.7.conv2.weight, %para135_moment2.features.7.bn2.gamma, %para136_moment2.features.7.bn2.beta, %para137_moment2.features.7.conv3.weight, %para138_moment2.features.7.bn3.gamma, %para139_moment2.features.7.bn3.beta, %para140_moment2.features.7.downsample.0.weight, %para141_moment2.features.7.downsample.1.gamma, %para142_moment2.features.7.downsample.1.beta, %para143_moment2.features.8.weight, %para144_moment2.features.10.conv1.weight, %para145_moment2.features.10.bn1.gamma, %para146_moment2.features.10.bn1.beta, %para147_moment2.features.10.conv2.weight, %para148_moment2.features.10.bn2.gamma, %para149_moment2.features.10.bn2.beta, %para150_moment2.features.10.conv3.weight, %para151_moment2.features.10.bn3.gamma, %para152_moment2.features.10.bn3.beta, %para153_moment2.features.11.weight, %para154_moment2.features.13.conv1.weight, %para155_moment2.features.13.bn1.gamma, %para156_moment2.features.13.bn1.beta, %para157_moment2.features.13.conv2.weight, %para158_moment2.features.13.bn2.gamma, %para159_moment2.features.13.bn2.beta, %para160_moment2.features.13.conv3.weight, %para161_moment2.features.13.bn3.gamma, %para162_moment2.features.13.bn3.beta, %para163_moment2.features.13.downsample.0.weight, %para164_moment2.features.13.downsample.1.gamma, %para165_moment2.features.13.downsample.1.beta, %para166_moment2.classifier.1.weight, %para167_moment2.classifier.1.bias, %para168_moment2.classifier.4.weight, %para169_moment2.classifier.4.bias, %para170_moment2.classifier.6.weight, %para171_moment2.classifier.6.bias, %para172_moment2.classifier.8.weight, %para173_moment2.classifier.8.bias, %para174_beta1_power, %para175_beta2_power, %para176_global_step, %para177_learning_rate, %para178_features.3.bn3.moving_mean, %para179_features.3.bn3.moving_variance, %para180_features.7.bn3.moving_mean, %para181_features.7.bn3.moving_variance, %para182_features.10.bn3.moving_mean, %para183_features.10.bn3.moving_variance, %para184_features.13.bn3.moving_mean, %para185_features.13.bn3.moving_variance, %para186_features.3.bn2.moving_mean, %para187_features.3.bn2.moving_variance, %para188_features.7.bn2.moving_mean, %para189_features.7.bn2.moving_variance, %para190_features.10.bn2.moving_mean, %para191_features.10.bn2.moving_variance, %para192_features.13.bn2.moving_mean, %para193_features.13.bn2.moving_variance, %para194_features.3.bn1.moving_mean, %para195_features.3.bn1.moving_variance, %para196_features.7.bn1.moving_mean, %para197_features.7.bn1.moving_variance, %para198_features.10.bn1.moving_mean, %para199_features.10.bn1.moving_variance, %para200_features.13.bn1.moving_mean, %para201_features.13.bn1.moving_variance, %para202_features.3.downsample.1.moving_mean, %para203_features.3.downsample.1.moving_variance, %para204_features.7.downsample.1.moving_mean, %para205_features.7.downsample.1.moving_variance, %para206_features.13.downsample.1.moving_mean, %para207_features.13.downsample.1.moving_variance) {

#------------------------> 0
  %1(CNode_1057) = call @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1038()
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417/        if not self.sense_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417/        if not self.sense_flag:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037:CNode_1057{[0]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1038}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037:CNode_1058{[0]: ValueNode<Primitive> Return, [1]: CNode_1057}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1038 : 000001D997BFFC40
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1038 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037]() {
  %1(CNode_1059) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037):MakeTuple(%para1_inputs0, %para2_inputs1)
      : (<Tensor[Float32], (32, 1, 3, 256, 256)>, <Tensor[Int32], (32)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 3, 256, 256), (32))>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/

#------------------------> 1
  %2(CNode_1060) = UnpackCall_unpack_call(@_no_sens_impl_1061, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 3, 256, 256), (32))>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%2)
      : (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1038:CNode_1060{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.1062, [1]: ValueNode<FuncGraph> _no_sens_impl_1061, [2]: CNode_1059}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1038:CNode_1063{[0]: ValueNode<Primitive> Return, [1]: CNode_1060}


subgraph attr:
core : 1
subgraph instance: UnpackCall_1039 : 000001D8B245D210
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
subgraph @UnpackCall_1039(%para208_, %para209_) {
  %1(CNode_1060) = TupleGetItem(%para209_1041, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 3, 256, 256), (32))>, <Int64, NoShape>) -> (<Tensor[Float32], (32, 1, 3, 256, 256)>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  %2(CNode_1060) = TupleGetItem(%para209_1041, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 3, 256, 256), (32))>, <Int64, NoShape>) -> (<Tensor[Int32], (32)>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/

#------------------------> 2
  %3(CNode_1060) = %para208_1040(%1, %2)
      : (<Tensor[Float32], (32, 1, 3, 256, 256)>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @UnpackCall_1039:CNode_1060{[0]: param_1040, [1]: CNode_1060, [2]: CNode_1060}
#   2: @UnpackCall_1039:CNode_1060{[0]: ValueNode<Primitive> Return, [1]: CNode_1060}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_1042 : 000001D8B2459250
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_1042 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para210_inputs0, %para211_inputs1) {

#------------------------> 3
  %1(CNode_1064) = call @_no_sens_impl_1043()
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_1042:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.1065, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1048, [2]: CNode_1066}
#   2: @_no_sens_impl_1042:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1048, [2]: CNode_1066}
#   3: @_no_sens_impl_1042:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_1067}
#   4: @_no_sens_impl_1042:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.1068, [1]: grads, [2]: CNode_1066}
#   5: @_no_sens_impl_1042:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_1069, [1]: grads}
#   6: @_no_sens_impl_1042:CNode_1070{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_1071, [1]: grads}
#   7: @_no_sens_impl_1042:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_1070}
#   8: @_no_sens_impl_1042:CNode_1064{[0]: ValueNode<FuncGraph> _no_sens_impl_1043}
#   9: @_no_sens_impl_1042:CNode_1072{[0]: ValueNode<Primitive> Return, [1]: CNode_1064}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_1043 : 000001D8B2459CF0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_1043 parent: [subgraph @_no_sens_impl_1042]() {

#------------------------> 4
  %1(CNode_1073) = call @_no_sens_impl_1044()
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_1043:CNode_1073{[0]: ValueNode<FuncGraph> _no_sens_impl_1044}
#   2: @_no_sens_impl_1043:CNode_1074{[0]: ValueNode<Primitive> Return, [1]: CNode_1073}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_1044 : 000001D8B245B780
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_1044 parent: [subgraph @_no_sens_impl_1042]() {
  %1(CNode_1066) = $(_no_sens_impl_1042):MakeTuple(%para210_inputs0, %para211_inputs1)
      : (<Tensor[Float32], (32, 1, 3, 256, 256)>, <Tensor[Int32], (32)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 3, 256, 256), (32))>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/

#------------------------> 5
  %2(loss) = $(_no_sens_impl_1042):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1048, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 3, 256, 256), (32))>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  %3(grads) = $(_no_sens_impl_1042):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1048, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 3, 256, 256), (32))>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(CNode_1067) = $(_no_sens_impl_1042):MakeTuple(%para3_features.0.weight, %para4_features.3.conv1.weight, %para5_features.3.bn1.gamma, %para6_features.3.bn1.beta, %para7_features.3.conv2.weight, %para8_features.3.bn2.gamma, %para9_features.3.bn2.beta, %para10_features.3.conv3.weight, %para11_features.3.bn3.gamma, %para12_features.3.bn3.beta, %para13_features.3.downsample.0.weight, %para14_features.3.downsample.1.gamma, %para15_features.3.downsample.1.beta, %para16_features.4.weight, %para17_features.7.conv1.weight, %para18_features.7.bn1.gamma, %para19_features.7.bn1.beta, %para20_features.7.conv2.weight, %para21_features.7.bn2.gamma, %para22_features.7.bn2.beta, %para23_features.7.conv3.weight, %para24_features.7.bn3.gamma, %para25_features.7.bn3.beta, %para26_features.7.downsample.0.weight, %para27_features.7.downsample.1.gamma, %para28_features.7.downsample.1.beta, %para29_features.8.weight, %para30_features.10.conv1.weight, %para31_features.10.bn1.gamma, %para32_features.10.bn1.beta, %para33_features.10.conv2.weight, %para34_features.10.bn2.gamma, %para35_features.10.bn2.beta, %para36_features.10.conv3.weight, %para37_features.10.bn3.gamma, %para38_features.10.bn3.beta, %para39_features.11.weight, %para40_features.13.conv1.weight, %para41_features.13.bn1.gamma, %para42_features.13.bn1.beta, %para43_features.13.conv2.weight, %para44_features.13.bn2.gamma, %para45_features.13.bn2.beta, %para46_features.13.conv3.weight, %para47_features.13.bn3.gamma, %para48_features.13.bn3.beta, %para49_features.13.downsample.0.weight, %para50_features.13.downsample.1.gamma, %para51_features.13.downsample.1.beta, %para52_classifier.1.weight, %para53_classifier.1.bias, %para54_classifier.4.weight, %para55_classifier.4.bias, %para56_classifier.6.weight, %para57_classifier.6.bias, %para58_classifier.8.weight, %para59_classifier.8.bias)
      : (<Ref[Tensor[Float32]], (96, 3, 11, 11)>, <Ref[Tensor[Float32]], (32, 96, 1, 1)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (32, 32, 3, 3)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (128, 32, 1, 1)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128, 96, 1, 1)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (256, 128, 5, 5)>, <Ref[Tensor[Float32]], (64, 256, 1, 1)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64, 64, 3, 3)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (512, 64, 1, 1)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512, 256, 1, 1)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (768, 512, 3, 3)>, <Ref[Tensor[Float32]], (128, 768, 1, 1)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128, 128, 3, 3)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (768, 128, 1, 1)>, <Ref[Tensor[Float32]], (768)>, <Ref[Tensor[Float32]], (768)>, <Ref[Tensor[Float32]], (1024, 768, 3, 3)>, <Ref[Tensor[Float32]], (128, 1024, 1, 1)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128, 128, 3, 3)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (1280, 128, 1, 1)>, <Ref[Tensor[Float32]], (1280)>, <Ref[Tensor[Float32]], (1280)>, <Ref[Tensor[Float32]], (1280, 1024, 1, 1)>, <Ref[Tensor[Float32]], (1280)>, <Ref[Tensor[Float32]], (1280)>, <Ref[Tensor[Float32]], (1024, 1280)>, <Ref[Tensor[Float32]], (1024)>, <Ref[Tensor[Float32]], (512, 1024)>, <Ref[Tensor[Float32]], (512)>, <Ref[Tensor[Float32]], (256, 512)>, <Ref[Tensor[Float32]], (256)>, <Ref[Tensor[Float32]], (5, 256)>, <Ref[Tensor[Float32]], (5)>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_1042):S_Prim_grad(%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_1042):UnpackCall_unpack_call(%5, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 3, 256, 256), (32))>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %7(grads) = $(_no_sens_impl_1042):call @mindspore_nn_layer_basic_Identity_construct_1069(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %8(CNode_1070) = $(_no_sens_impl_1042):call @mindspore_nn_optim_adam_Adam_construct_1071(%7)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %9(loss) = $(_no_sens_impl_1042):S_Prim_Depend[side_effect_propagate: I64(1)](%2, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%9)
      : (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @_no_sens_impl_1044:CNode_1075{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
core : 1
subgraph instance: UnpackCall_1045 : 000001D8A170C170
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
subgraph @UnpackCall_1045(%para212_, %para213_) {
  %1(loss) = TupleGetItem(%para213_1047, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 3, 256, 256), (32))>, <Int64, NoShape>) -> (<Tensor[Float32], (32, 1, 3, 256, 256)>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(loss) = TupleGetItem(%para213_1047, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 1, 3, 256, 256), (32))>, <Int64, NoShape>) -> (<Tensor[Int32], (32)>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/

#------------------------> 6
  %3(loss) = %para212_1046(%1, %2)
      : (<Tensor[Float32], (32, 1, 3, 256, 256)>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
}
# Order:
#   1: @UnpackCall_1045:loss{[0]: param_1046, [1]: loss, [2]: loss}
#   2: @UnpackCall_1045:loss{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1048 : 000001D8B2457270
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1048 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para214_data, %para215_label) {

#------------------------> 7
  %1(out) = call @__main___Net_construct_1049(%para214_data)
      : (<Tensor[Float32], (32, 1, 3, 256, 256)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_1077) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1076(%1, %para215_label)
      : (<null>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1048:out{[0]: ValueNode<FuncGraph> __main___Net_construct_1049, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1048:CNode_1077{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1076, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1048:CNode_1078{[0]: ValueNode<Primitive> Return, [1]: CNode_1077}


subgraph attr:
training : 1
subgraph instance: __main___Net_construct_1049 : 000001D8B24567D0
# In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:42/
subgraph @__main___Net_construct_1049 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para216_x) {

#------------------------> 8
  %1(x) = call @mindspore_nn_layer_container_SequentialCell_construct_1050(%para216_x)
      : (<Tensor[Float32], (32, 1, 3, 256, 256)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:43/
  %2(CNode_1079) = getattr(%1, "view")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:44/
  %3(CNode_1080) = getattr(%1, "shape")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:44/
  %4(CNode_1081) = S_Prim_getitem(%3, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:44/
  %5(CNode_1082) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:44/
  %6(x) = %2(%4, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:44/
  %7(x) = call @mindspore_nn_layer_container_SequentialCell_construct_1083(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:45/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:46/
}
# Order:
#   1: @__main___Net_construct_1049:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1050, [1]: param_x}
#   2: @__main___Net_construct_1049:CNode_1079{[0]: ValueNode<Primitive> getattr, [1]: x, [2]: ValueNode<StringImm> view}
#   3: @__main___Net_construct_1049:CNode_1080{[0]: ValueNode<Primitive> getattr, [1]: x, [2]: ValueNode<StringImm> shape}
#   4: @__main___Net_construct_1049:CNode_1081{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1080, [2]: ValueNode<Int64Imm> 0}
#   5: @__main___Net_construct_1049:CNode_1082{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   6: @__main___Net_construct_1049:x{[0]: CNode_1079, [1]: CNode_1081, [2]: CNode_1082}
#   7: @__main___Net_construct_1049:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1083, [1]: x}
#   8: @__main___Net_construct_1049:CNode_1084{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1050 : 000001D8B2458260
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1050 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para217_input_data) {

#------------------------> 9
  %1(CNode_1085) = call @mindspore_nn_layer_container_SequentialCell_construct_1051(I64(0), %para217_input_data)
      : (<Int64, NoShape>, <Tensor[Float32], (32, 1, 3, 256, 256)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1050:CNode_1086{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_1087}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1050:CNode_1085{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1051, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1050:CNode_1088{[0]: ValueNode<Primitive> Return, [1]: CNode_1085}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1051 : 000001D8B2456280
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1051 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1050](%para218_, %para219_) {
  %1(CNode_1087) = $(mindspore_nn_layer_container_SequentialCell_construct_1050):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1054, @mindspore_nn_layer_activation_ReLU_construct_1089, @mindspore_nn_layer_pooling_MaxPool2d_construct_1090, @__main___Bottleneck_construct_1091, @mindspore_nn_layer_conv_Conv2d_construct_1092, @mindspore_nn_layer_activation_ReLU_construct_1093, @mindspore_nn_layer_pooling_MaxPool2d_construct_1094, @__main___Bottleneck_construct_1095, @mindspore_nn_layer_conv_Conv2d_construct_1096, @mindspore_nn_layer_activation_ReLU_construct_1097, @__main___Bottleneck_construct_1098, @mindspore_nn_layer_conv_Conv2d_construct_1099, @mindspore_nn_layer_activation_ReLU_construct_1100, @__main___Bottleneck_construct_1101, @mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_1102)
      : (<Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Tuple[Func*15], TupleShape(NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1086) = $(mindspore_nn_layer_container_SequentialCell_construct_1050):S_Prim_inner_len(%1)
      : (<Tuple[Func*15], TupleShape(NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape)>) -> (<Int64, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_1103) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para218_@CNode_1052, %2)
      : (<Int64, NoShape>, <Int64, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1104) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_1053, @mindspore_nn_layer_container_SequentialCell_construct_1105)
      : (<Bool, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/

#------------------------> 10
  %5(CNode_1106) = %4()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1051:CNode_1103{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.73, [1]: param_@CNode_1052, [2]: CNode_1086}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1051:CNode_1104{[0]: ValueNode<Primitive> Switch, [1]: CNode_1103, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1053, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1105}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1051:CNode_1106{[0]: CNode_1104}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1051:CNode_1107{[0]: ValueNode<Primitive> Return, [1]: CNode_1106}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1053 : 000001D8B245A240
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1053 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1051]() {
  %1(CNode_1052) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para218_@CNode_1052, I64(1))
      : (<Int64, NoShape>, <Int64, NoShape>) -> (<Int64, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1108) = StopGradient(%1)
      : (<Int64, NoShape>) -> (<Int64, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_1087) = $(mindspore_nn_layer_container_SequentialCell_construct_1050):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1054, @mindspore_nn_layer_activation_ReLU_construct_1089, @mindspore_nn_layer_pooling_MaxPool2d_construct_1090, @__main___Bottleneck_construct_1091, @mindspore_nn_layer_conv_Conv2d_construct_1092, @mindspore_nn_layer_activation_ReLU_construct_1093, @mindspore_nn_layer_pooling_MaxPool2d_construct_1094, @__main___Bottleneck_construct_1095, @mindspore_nn_layer_conv_Conv2d_construct_1096, @mindspore_nn_layer_activation_ReLU_construct_1097, @__main___Bottleneck_construct_1098, @mindspore_nn_layer_conv_Conv2d_construct_1099, @mindspore_nn_layer_activation_ReLU_construct_1100, @__main___Bottleneck_construct_1101, @mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_1102)
      : (<Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Tuple[Func*15], TupleShape(NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1110) = call @ms_iter_1109(%3)
      : (<Tuple[Func*15], TupleShape(NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape)>) -> (<Tuple[Func*15], TupleShape(NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para218_@CNode_1052)
      : (<Tuple[Func*15], TupleShape(NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape, NoShape)>, <Int64, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/

#------------------------> 11
  %6(input_data) = %5(%para219_phi_input_data)
      : (<Tensor[Float32], (32, 1, 3, 256, 256)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_1111) = call @mindspore_nn_layer_container_SequentialCell_construct_1051(%1, %6)
      : (<Int64, NoShape>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_1112) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <Int64, NoShape>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1053:CNode_1110{[0]: ValueNode<FuncGraph> ms_iter_1109, [1]: CNode_1087}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1053:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1110, [2]: param_@CNode_1052}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1053:CNode_1052{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.80, [1]: param_@CNode_1052, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1053:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_1053:CNode_1111{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1051, [1]: CNode_1052, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_1053:CNode_1113{[0]: ValueNode<Primitive> Return, [1]: CNode_1112}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1054 : 000001D8A1732F00
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1054 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para220_x) {

#------------------------> 12
  %1(CNode_1114) = call @mindspore_nn_layer_conv_Conv2d_construct_1055()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1054:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.0.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1054:CNode_1114{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1055}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1054:CNode_1115{[0]: ValueNode<Primitive> Return, [1]: CNode_1114}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1055 : 000001D8A172DF50
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1055 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1054]() {

#------------------------> 13
  %1(CNode_1116) = call @mindspore_nn_layer_conv_Conv2d_construct_1056()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1055:CNode_1116{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1056}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1055:CNode_1117{[0]: ValueNode<Primitive> Return, [1]: CNode_1116}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1056 : 000001D8A172F9E0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1056 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1054]() {

#------------------------> 14
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1054):S_Prim_Conv2D[out_channel: I64(96), kernel_size: (I64(11), I64(11)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(2), I64(2), I64(2), I64(2)), pad_mode: I64(0), stride: (I64(1), I64(1), I64(4), I64(4)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para220_x, %para3_features.0.weight)
      : (<Tensor[Float32], (32, 1, 3, 256, 256)>, <Ref[Tensor[Float32]], (96, 3, 11, 11)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1056:CNode_1118{[0]: ValueNode<Primitive> Return, [1]: output}


# ===============================================================================================
# The total of function graphs in evaluation stack: 15/16 (Ignored 1 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
subgraph attr:
training : 1
subgraph instance: _no_sens_impl_1061 : 000001D997BFF6F0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_1061 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para221_inputs) {
  %1(CNode_1064) = call @_no_sens_impl_1119()
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_1061:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.1065, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1120, [2]: param_inputs}
#   2: @_no_sens_impl_1061:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1120, [2]: param_inputs}
#   3: @_no_sens_impl_1061:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_1067}
#   4: @_no_sens_impl_1061:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.1068, [1]: grads, [2]: param_inputs}
#   5: @_no_sens_impl_1061:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_1121, [1]: grads}
#   6: @_no_sens_impl_1061:CNode_1070{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_1122, [1]: grads}
#   7: @_no_sens_impl_1061:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_1070}
#   8: @_no_sens_impl_1061:CNode_1064{[0]: ValueNode<FuncGraph> _no_sens_impl_1119}
#   9: @_no_sens_impl_1061:CNode_1072{[0]: ValueNode<Primitive> Return, [1]: CNode_1064}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_1122 : 000001D997C00190
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:910/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_adam_Adam_construct_1122 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para222_gradients) {
  %1(CNode_1124) = call @mindspore_nn_optim_adam_Adam_construct_1123()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:916/        if not self.use_offload:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:916/        if not self.use_offload:/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_1122:gradients{[0]: ValueNode<FuncGraph> flatten_gradients_1125, [1]: param_gradients}
#   2: @mindspore_nn_optim_adam_Adam_construct_1122:gradients{[0]: ValueNode<FuncGraph> decay_weight_1126, [1]: gradients}
#   3: @mindspore_nn_optim_adam_Adam_construct_1122:CNode_1124{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_1123}
#   4: @mindspore_nn_optim_adam_Adam_construct_1122:CNode_1127{[0]: ValueNode<Primitive> Return, [1]: CNode_1124}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Identity_construct_1121 : 000001D997BFD1C0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:505/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Identity_construct_1121(%para223_x) {
  Return(%para223_x)
      : (<null>)
      #scope: (Default/grad_reducer-Identity)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:506/        return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Identity_construct_1121:CNode_1128{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1120 : 000001D997C0FB40
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1120 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para224_data, %para225_label) {
  %1(out) = call @__main___Net_construct_1129(%para224_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_1077) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1130(%1, %para225_label)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1120:out{[0]: ValueNode<FuncGraph> __main___Net_construct_1129, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1120:CNode_1077{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1130, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1120:CNode_1078{[0]: ValueNode<Primitive> Return, [1]: CNode_1077}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_1119 : 000001D8B2455290
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_1119 parent: [subgraph @_no_sens_impl_1061]() {
  %1(CNode_1073) = call @_no_sens_impl_1131()
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_1119:CNode_1073{[0]: ValueNode<FuncGraph> _no_sens_impl_1131}
#   2: @_no_sens_impl_1119:CNode_1074{[0]: ValueNode<Primitive> Return, [1]: CNode_1073}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_1126 : 000001D997BFD710
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_1126(%para226_gradients) {
  %1(CNode_1133) = call @decay_weight_1132()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_1126:CNode_1133{[0]: ValueNode<FuncGraph> decay_weight_1132}
#   2: @decay_weight_1126:CNode_1134{[0]: ValueNode<Primitive> Return, [1]: CNode_1133}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_1125 : 000001D997C05140
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_1125(%para227_gradients) {
  %1(CNode_1136) = call @flatten_gradients_1135()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_1125:CNode_1136{[0]: ValueNode<FuncGraph> flatten_gradients_1135}
#   2: @flatten_gradients_1125:CNode_1137{[0]: ValueNode<Primitive> Return, [1]: CNode_1136}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_1123 : 000001D997C06130
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:910/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_adam_Adam_construct_1123 parent: [subgraph @mindspore_nn_optim_adam_Adam_construct_1122]() {
  %1(CNode_1139) = call @mindspore_nn_optim_adam_Adam_construct_1138()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:917/            gradients = self.gradients_centralization(gradients)/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:917/            gradients = self.gradients_centralization(gradients)/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_1123:gradients{[0]: ValueNode<FuncGraph> gradients_centralization_1140, [1]: gradients}
#   2: @mindspore_nn_optim_adam_Adam_construct_1123:CNode_1139{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_1138}
#   3: @mindspore_nn_optim_adam_Adam_construct_1123:CNode_1141{[0]: ValueNode<Primitive> Return, [1]: CNode_1139}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1130 : 000001D8B24512D0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1130(%para228_logits, %para229_labels) {
  %1(CNode_1142) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("logits", %para228_logits, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:778/        _check_is_tensor('logits', logits, self.cls_name)/
  %2(CNode_1143) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("labels", %para229_labels, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:779/        _check_is_tensor('labels', labels, self.cls_name)/
  %3(CNode_1144) = MakeTuple(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
  %4(CNode_1145) = StopGradient(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
  %5(CNode_1147) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1146()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  %6(CNode_1148) = Depend[side_effect_propagate: I64(1)](%5, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1130:CNode_1142{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> logits, [2]: param_logits, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1130:CNode_1143{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> labels, [2]: param_labels, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1130:CNode_1147{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1146}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1130:CNode_1149{[0]: ValueNode<Primitive> Return, [1]: CNode_1148}


subgraph attr:
training : 1
subgraph instance: __main___Net_construct_1129 : 000001D997C10090
# In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:42/
subgraph @__main___Net_construct_1129 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para230_x) {
  %1(x) = call @mindspore_nn_layer_container_SequentialCell_construct_1150(%para230_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:43/
  %2(CNode_1079) = getattr(%1, "view")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:44/
  %3(CNode_1080) = getattr(%1, "shape")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:44/
  %4(CNode_1081) = S_Prim_getitem(%3, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:44/
  %5(CNode_1082) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:44/
  %6(x) = %2(%4, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:44/
  %7(x) = call @mindspore_nn_layer_container_SequentialCell_construct_1151(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:45/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\1212914247.py:46/
}
# Order:
#   1: @__main___Net_construct_1129:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1150, [1]: param_x}
#   2: @__main___Net_construct_1129:CNode_1079{[0]: ValueNode<Primitive> getattr, [1]: x, [2]: ValueNode<StringImm> view}
#   3: @__main___Net_construct_1129:CNode_1080{[0]: ValueNode<Primitive> getattr, [1]: x, [2]: ValueNode<StringImm> shape}
#   4: @__main___Net_construct_1129:CNode_1081{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1080, [2]: ValueNode<Int64Imm> 0}
#   5: @__main___Net_construct_1129:CNode_1082{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   6: @__main___Net_construct_1129:x{[0]: CNode_1079, [1]: CNode_1081, [2]: CNode_1082}
#   7: @__main___Net_construct_1129:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1151, [1]: x}
#   8: @__main___Net_construct_1129:CNode_1084{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_1131 : 000001D8B244D860
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_1131 parent: [subgraph @_no_sens_impl_1061]() {
  %1(loss) = $(_no_sens_impl_1061):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1120, %para221_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(grads) = $(_no_sens_impl_1061):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_1120, %para221_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %3(CNode_1067) = $(_no_sens_impl_1061):MakeTuple(%para3_features.0.weight, %para4_features.3.conv1.weight, %para5_features.3.bn1.gamma, %para6_features.3.bn1.beta, %para7_features.3.conv2.weight, %para8_features.3.bn2.gamma, %para9_features.3.bn2.beta, %para10_features.3.conv3.weight, %para11_features.3.bn3.gamma, %para12_features.3.bn3.beta, %para13_features.3.downsample.0.weight, %para14_features.3.downsample.1.gamma, %para15_features.3.downsample.1.beta, %para16_features.4.weight, %para17_features.7.conv1.weight, %para18_features.7.bn1.gamma, %para19_features.7.bn1.beta, %para20_features.7.conv2.weight, %para21_features.7.bn2.gamma, %para22_features.7.bn2.beta, %para23_features.7.conv3.weight, %para24_features.7.bn3.gamma, %para25_features.7.bn3.beta, %para26_features.7.downsample.0.weight, %para27_features.7.downsample.1.gamma, %para28_features.7.downsample.1.beta, %para29_features.8.weight, %para30_features.10.conv1.weight, %para31_features.10.bn1.gamma, %para32_features.10.bn1.beta, %para33_features.10.conv2.weight, %para34_features.10.bn2.gamma, %para35_features.10.bn2.beta, %para36_features.10.conv3.weight, %para37_features.10.bn3.gamma, %para38_features.10.bn3.beta, %para39_features.11.weight, %para40_features.13.conv1.weight, %para41_features.13.bn1.gamma, %para42_features.13.bn1.beta, %para43_features.13.conv2.weight, %para44_features.13.bn2.gamma, %para45_features.13.bn2.beta, %para46_features.13.conv3.weight, %para47_features.13.bn3.gamma, %para48_features.13.bn3.beta, %para49_features.13.downsample.0.weight, %para50_features.13.downsample.1.gamma, %para51_features.13.downsample.1.beta, %para52_classifier.1.weight, %para53_classifier.1.bias, %para54_classifier.4.weight, %para55_classifier.4.bias, %para56_classifier.6.weight, %para57_classifier.6.bias, %para58_classifier.8.weight, %para59_classifier.8.bias)
      : (<Ref[Tensor[Float32]], (96, 3, 11, 11), ref_key=:features.0.weight>, <Ref[Tensor[Float32]], (32, 96, 1, 1), ref_key=:features.3.conv1.weight>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn1.beta>, <Ref[Tensor[Float32]], (32, 32, 3, 3), ref_key=:features.3.conv2.weight>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn2.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn2.beta>, <Ref[Tensor[Float32]], (128, 32, 1, 1), ref_key=:features.3.conv3.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.bn3.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.bn3.beta>, <Ref[Tensor[Float32]], (128, 96, 1, 1), ref_key=:features.3.downsample.0.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.downsample.1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.downsample.1.beta>, <Ref[Tensor[Float32]], (256, 128, 5, 5), ref_key=:features.4.weight>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:features.7.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:features.7.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn2.beta>, <Ref[Tensor[Float32]], (512, 64, 1, 1), ref_key=:features.7.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.bn3.beta>, <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:features.7.downsample.0.weight>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.downsample.1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.downsample.1.beta>, <Ref[Tensor[Float32]], (768, 512, 3, 3), ref_key=:features.8.weight>, <Ref[Tensor[Float32]], (128, 768, 1, 1), ref_key=:features.10.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:features.10.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn2.beta>, <Ref[Tensor[Float32]], (768, 128, 1, 1), ref_key=:features.10.conv3.weight>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.gamma>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.beta>, <Ref[Tensor[Float32]], (1024, 768, 3, 3), ref_key=:features.11.weight>, <Ref[Tensor[Float32]], (128, 1024, 1, 1), ref_key=:features.13.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:features.13.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn2.beta>, <Ref[Tensor[Float32]], (1280, 128, 1, 1), ref_key=:features.13.conv3.weight>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.bn3.gamma>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.bn3.beta>, <Ref[Tensor[Float32]], (1280, 1024, 1, 1), ref_key=:features.13.downsample.0.weight>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.downsample.1.gamma>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.downsample.1.beta>, <Ref[Tensor[Float32]], (1024, 1280), ref_key=:classifier.1.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:classifier.1.bias>, <Ref[Tensor[Float32]], (512, 1024), ref_key=:classifier.4.weight>, <Ref[Tensor[Float32]], (512), ref_key=:classifier.4.bias>, <Ref[Tensor[Float32]], (256, 512), ref_key=:classifier.6.weight>, <Ref[Tensor[Float32]], (256), ref_key=:classifier.6.bias>, <Ref[Tensor[Float32]], (5, 256), ref_key=:classifier.8.weight>, <Ref[Tensor[Float32]], (5), ref_key=:classifier.8.bias>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(grads) = $(_no_sens_impl_1061):S_Prim_grad(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_1061):UnpackCall_unpack_call(%4, %para221_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_1061):call @mindspore_nn_layer_basic_Identity_construct_1121(%5)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %7(CNode_1070) = $(_no_sens_impl_1061):call @mindspore_nn_optim_adam_Adam_construct_1122(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %8(loss) = $(_no_sens_impl_1061):S_Prim_Depend[side_effect_propagate: I64(1)](%1, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%8)
      : (<null>)
      #scope: (Default)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @_no_sens_impl_1131:CNode_1075{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_1132 : 000001D997C05690
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_1132 parent: [subgraph @decay_weight_1126]() {
  %1(CNode_1153) = call @decay_weight_1152()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_1132:CNode_1153{[0]: ValueNode<FuncGraph> decay_weight_1152}
#   2: @decay_weight_1132:CNode_1154{[0]: ValueNode<Primitive> Return, [1]: CNode_1153}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_1135 : 000001D997C006E0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_1135 parent: [subgraph @flatten_gradients_1125]() {
  %1(CNode_1156) = call @flatten_gradients_1155()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_1135:CNode_1156{[0]: ValueNode<FuncGraph> flatten_gradients_1155}
#   2: @flatten_gradients_1135:CNode_1157{[0]: ValueNode<Primitive> Return, [1]: CNode_1156}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_1140 : 000001D997C05BE0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_1140(%para231_gradients) {
  %1(CNode_1159) = call @gradients_centralization_1158()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_1140:CNode_1159{[0]: ValueNode<FuncGraph> gradients_centralization_1158}
#   2: @gradients_centralization_1140:CNode_1160{[0]: ValueNode<Primitive> Return, [1]: CNode_1159}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_1138 : 000001D997BFEC50
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:910/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_adam_Adam_construct_1138 parent: [subgraph @mindspore_nn_optim_adam_Adam_construct_1123]() {
  %1(CNode_1161) = S_Prim_AssignAdd[input_names: ["ref", "value"], output_names: ["ref"], side_effect_mem: Bool(1)](%para176_global_step, Tensor(shape=[1], dtype=Int32, value=[1]))
      : (<Ref[Tensor[Int32]], (1), ref_key=:global_step>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:921/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %2(beta1_power) = S_Prim_mul(%para174_beta1_power, Tensor(shape=[], dtype=Float32, value=0.9))
      : (<Ref[Tensor[Float32]], (), ref_key=:beta1_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:923/        beta1_power = self.beta1_power * self.beta1/
  %3(CNode_1163) = call @assign_1162(%para174_beta1_power, %2)
      : (<Ref[Tensor[Float32]], (), ref_key=:beta1_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:924/        self.beta1_power = beta1_power/
  %4(beta2_power) = S_Prim_mul(%para175_beta2_power, Tensor(shape=[], dtype=Float32, value=0.999))
      : (<Ref[Tensor[Float32]], (), ref_key=:beta2_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:925/        beta2_power = self.beta2_power * self.beta2/
  %5(CNode_1164) = call @assign_1162(%para175_beta2_power, %4)
      : (<Ref[Tensor[Float32]], (), ref_key=:beta2_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:926/        self.beta2_power = beta2_power/
  %6(CNode_1165) = MakeTuple(%1, %3, %5)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:909/    @jit/
  %7(CNode_1166) = StopGradient(%6)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:909/    @jit/
  %8(CNode_1167) = $(mindspore_nn_optim_adam_Adam_construct_1122):MakeTuple(%para3_features.0.weight, %para4_features.3.conv1.weight, %para5_features.3.bn1.gamma, %para6_features.3.bn1.beta, %para7_features.3.conv2.weight, %para8_features.3.bn2.gamma, %para9_features.3.bn2.beta, %para10_features.3.conv3.weight, %para11_features.3.bn3.gamma, %para12_features.3.bn3.beta, %para13_features.3.downsample.0.weight, %para14_features.3.downsample.1.gamma, %para15_features.3.downsample.1.beta, %para16_features.4.weight, %para17_features.7.conv1.weight, %para18_features.7.bn1.gamma, %para19_features.7.bn1.beta, %para20_features.7.conv2.weight, %para21_features.7.bn2.gamma, %para22_features.7.bn2.beta, %para23_features.7.conv3.weight, %para24_features.7.bn3.gamma, %para25_features.7.bn3.beta, %para26_features.7.downsample.0.weight, %para27_features.7.downsample.1.gamma, %para28_features.7.downsample.1.beta, %para29_features.8.weight, %para30_features.10.conv1.weight, %para31_features.10.bn1.gamma, %para32_features.10.bn1.beta, %para33_features.10.conv2.weight, %para34_features.10.bn2.gamma, %para35_features.10.bn2.beta, %para36_features.10.conv3.weight, %para37_features.10.bn3.gamma, %para38_features.10.bn3.beta, %para39_features.11.weight, %para40_features.13.conv1.weight, %para41_features.13.bn1.gamma, %para42_features.13.bn1.beta, %para43_features.13.conv2.weight, %para44_features.13.bn2.gamma, %para45_features.13.bn2.beta, %para46_features.13.conv3.weight, %para47_features.13.bn3.gamma, %para48_features.13.bn3.beta, %para49_features.13.downsample.0.weight, %para50_features.13.downsample.1.gamma, %para51_features.13.downsample.1.beta, %para52_classifier.1.weight, %para53_classifier.1.bias, %para54_classifier.4.weight, %para55_classifier.4.bias, %para56_classifier.6.weight, %para57_classifier.6.bias, %para58_classifier.8.weight, %para59_classifier.8.bias)
      : (<Ref[Tensor[Float32]], (96, 3, 11, 11), ref_key=:features.0.weight>, <Ref[Tensor[Float32]], (32, 96, 1, 1), ref_key=:features.3.conv1.weight>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn1.beta>, <Ref[Tensor[Float32]], (32, 32, 3, 3), ref_key=:features.3.conv2.weight>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn2.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn2.beta>, <Ref[Tensor[Float32]], (128, 32, 1, 1), ref_key=:features.3.conv3.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.bn3.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.bn3.beta>, <Ref[Tensor[Float32]], (128, 96, 1, 1), ref_key=:features.3.downsample.0.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.downsample.1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.downsample.1.beta>, <Ref[Tensor[Float32]], (256, 128, 5, 5), ref_key=:features.4.weight>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:features.7.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:features.7.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn2.beta>, <Ref[Tensor[Float32]], (512, 64, 1, 1), ref_key=:features.7.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.bn3.beta>, <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:features.7.downsample.0.weight>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.downsample.1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.downsample.1.beta>, <Ref[Tensor[Float32]], (768, 512, 3, 3), ref_key=:features.8.weight>, <Ref[Tensor[Float32]], (128, 768, 1, 1), ref_key=:features.10.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:features.10.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn2.beta>, <Ref[Tensor[Float32]], (768, 128, 1, 1), ref_key=:features.10.conv3.weight>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.gamma>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.beta>, <Ref[Tensor[Float32]], (1024, 768, 3, 3), ref_key=:features.11.weight>, <Ref[Tensor[Float32]], (128, 1024, 1, 1), ref_key=:features.13.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:features.13.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn2.beta>, <Ref[Tensor[Float32]], (1280, 128, 1, 1), ref_key=:features.13.conv3.weight>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.bn3.gamma>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.bn3.beta>, <Ref[Tensor[Float32]], (1280, 1024, 1, 1), ref_key=:features.13.downsample.0.weight>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.downsample.1.gamma>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.downsample.1.beta>, <Ref[Tensor[Float32]], (1024, 1280), ref_key=:classifier.1.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:classifier.1.bias>, <Ref[Tensor[Float32]], (512, 1024), ref_key=:classifier.4.weight>, <Ref[Tensor[Float32]], (512), ref_key=:classifier.4.bias>, <Ref[Tensor[Float32]], (256, 512), ref_key=:classifier.6.weight>, <Ref[Tensor[Float32]], (256), ref_key=:classifier.6.bias>, <Ref[Tensor[Float32]], (5, 256), ref_key=:classifier.8.weight>, <Ref[Tensor[Float32]], (5), ref_key=:classifier.8.bias>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:911/        params = self._parameters/
  %9(CNode_1168) = $(mindspore_nn_optim_adam_Adam_construct_1122):MakeTuple(%para60_moment1.features.0.weight, %para61_moment1.features.3.conv1.weight, %para62_moment1.features.3.bn1.gamma, %para63_moment1.features.3.bn1.beta, %para64_moment1.features.3.conv2.weight, %para65_moment1.features.3.bn2.gamma, %para66_moment1.features.3.bn2.beta, %para67_moment1.features.3.conv3.weight, %para68_moment1.features.3.bn3.gamma, %para69_moment1.features.3.bn3.beta, %para70_moment1.features.3.downsample.0.weight, %para71_moment1.features.3.downsample.1.gamma, %para72_moment1.features.3.downsample.1.beta, %para73_moment1.features.4.weight, %para74_moment1.features.7.conv1.weight, %para75_moment1.features.7.bn1.gamma, %para76_moment1.features.7.bn1.beta, %para77_moment1.features.7.conv2.weight, %para78_moment1.features.7.bn2.gamma, %para79_moment1.features.7.bn2.beta, %para80_moment1.features.7.conv3.weight, %para81_moment1.features.7.bn3.gamma, %para82_moment1.features.7.bn3.beta, %para83_moment1.features.7.downsample.0.weight, %para84_moment1.features.7.downsample.1.gamma, %para85_moment1.features.7.downsample.1.beta, %para86_moment1.features.8.weight, %para87_moment1.features.10.conv1.weight, %para88_moment1.features.10.bn1.gamma, %para89_moment1.features.10.bn1.beta, %para90_moment1.features.10.conv2.weight, %para91_moment1.features.10.bn2.gamma, %para92_moment1.features.10.bn2.beta, %para93_moment1.features.10.conv3.weight, %para94_moment1.features.10.bn3.gamma, %para95_moment1.features.10.bn3.beta, %para96_moment1.features.11.weight, %para97_moment1.features.13.conv1.weight, %para98_moment1.features.13.bn1.gamma, %para99_moment1.features.13.bn1.beta, %para100_moment1.features.13.conv2.weight, %para101_moment1.features.13.bn2.gamma, %para102_moment1.features.13.bn2.beta, %para103_moment1.features.13.conv3.weight, %para104_moment1.features.13.bn3.gamma, %para105_moment1.features.13.bn3.beta, %para106_moment1.features.13.downsample.0.weight, %para107_moment1.features.13.downsample.1.gamma, %para108_moment1.features.13.downsample.1.beta, %para109_moment1.classifier.1.weight, %para110_moment1.classifier.1.bias, %para111_moment1.classifier.4.weight, %para112_moment1.classifier.4.bias, %para113_moment1.classifier.6.weight, %para114_moment1.classifier.6.bias, %para115_moment1.classifier.8.weight, %para116_moment1.classifier.8.bias)
      : (<Ref[Tensor[Float32]], (96, 3, 11, 11), ref_key=:moment1.features.0.weight>, <Ref[Tensor[Float32]], (32, 96, 1, 1), ref_key=:moment1.features.3.conv1.weight>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.features.3.bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.features.3.bn1.beta>, <Ref[Tensor[Float32]], (32, 32, 3, 3), ref_key=:moment1.features.3.conv2.weight>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.features.3.bn2.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.features.3.bn2.beta>, <Ref[Tensor[Float32]], (128, 32, 1, 1), ref_key=:moment1.features.3.conv3.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.3.bn3.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.3.bn3.beta>, <Ref[Tensor[Float32]], (128, 96, 1, 1), ref_key=:moment1.features.3.downsample.0.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.3.downsample.1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.3.downsample.1.beta>, <Ref[Tensor[Float32]], (256, 128, 5, 5), ref_key=:moment1.features.4.weight>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:moment1.features.7.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moment1.features.7.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moment1.features.7.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moment1.features.7.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moment1.features.7.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moment1.features.7.bn2.beta>, <Ref[Tensor[Float32]], (512, 64, 1, 1), ref_key=:moment1.features.7.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moment1.features.7.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moment1.features.7.bn3.beta>, <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:moment1.features.7.downsample.0.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moment1.features.7.downsample.1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moment1.features.7.downsample.1.beta>, <Ref[Tensor[Float32]], (768, 512, 3, 3), ref_key=:moment1.features.8.weight>, <Ref[Tensor[Float32]], (128, 768, 1, 1), ref_key=:moment1.features.10.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.10.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.10.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moment1.features.10.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.10.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.10.bn2.beta>, <Ref[Tensor[Float32]], (768, 128, 1, 1), ref_key=:moment1.features.10.conv3.weight>, <Ref[Tensor[Float32]], (768), ref_key=:moment1.features.10.bn3.gamma>, <Ref[Tensor[Float32]], (768), ref_key=:moment1.features.10.bn3.beta>, <Ref[Tensor[Float32]], (1024, 768, 3, 3), ref_key=:moment1.features.11.weight>, <Ref[Tensor[Float32]], (128, 1024, 1, 1), ref_key=:moment1.features.13.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.13.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.13.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moment1.features.13.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.13.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.13.bn2.beta>, <Ref[Tensor[Float32]], (1280, 128, 1, 1), ref_key=:moment1.features.13.conv3.weight>, <Ref[Tensor[Float32]], (1280), ref_key=:moment1.features.13.bn3.gamma>, <Ref[Tensor[Float32]], (1280), ref_key=:moment1.features.13.bn3.beta>, <Ref[Tensor[Float32]], (1280, 1024, 1, 1), ref_key=:moment1.features.13.downsample.0.weight>, <Ref[Tensor[Float32]], (1280), ref_key=:moment1.features.13.downsample.1.gamma>, <Ref[Tensor[Float32]], (1280), ref_key=:moment1.features.13.downsample.1.beta>, <Ref[Tensor[Float32]], (1024, 1280), ref_key=:moment1.classifier.1.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:moment1.classifier.1.bias>, <Ref[Tensor[Float32]], (512, 1024), ref_key=:moment1.classifier.4.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moment1.classifier.4.bias>, <Ref[Tensor[Float32]], (256, 512), ref_key=:moment1.classifier.6.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moment1.classifier.6.bias>, <Ref[Tensor[Float32]], (5, 256), ref_key=:moment1.classifier.8.weight>, <Ref[Tensor[Float32]], (5), ref_key=:moment1.classifier.8.bias>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:912/        moment1 = self.moment1/
  %10(CNode_1169) = $(mindspore_nn_optim_adam_Adam_construct_1122):MakeTuple(%para117_moment2.features.0.weight, %para118_moment2.features.3.conv1.weight, %para119_moment2.features.3.bn1.gamma, %para120_moment2.features.3.bn1.beta, %para121_moment2.features.3.conv2.weight, %para122_moment2.features.3.bn2.gamma, %para123_moment2.features.3.bn2.beta, %para124_moment2.features.3.conv3.weight, %para125_moment2.features.3.bn3.gamma, %para126_moment2.features.3.bn3.beta, %para127_moment2.features.3.downsample.0.weight, %para128_moment2.features.3.downsample.1.gamma, %para129_moment2.features.3.downsample.1.beta, %para130_moment2.features.4.weight, %para131_moment2.features.7.conv1.weight, %para132_moment2.features.7.bn1.gamma, %para133_moment2.features.7.bn1.beta, %para134_moment2.features.7.conv2.weight, %para135_moment2.features.7.bn2.gamma, %para136_moment2.features.7.bn2.beta, %para137_moment2.features.7.conv3.weight, %para138_moment2.features.7.bn3.gamma, %para139_moment2.features.7.bn3.beta, %para140_moment2.features.7.downsample.0.weight, %para141_moment2.features.7.downsample.1.gamma, %para142_moment2.features.7.downsample.1.beta, %para143_moment2.features.8.weight, %para144_moment2.features.10.conv1.weight, %para145_moment2.features.10.bn1.gamma, %para146_moment2.features.10.bn1.beta, %para147_moment2.features.10.conv2.weight, %para148_moment2.features.10.bn2.gamma, %para149_moment2.features.10.bn2.beta, %para150_moment2.features.10.conv3.weight, %para151_moment2.features.10.bn3.gamma, %para152_moment2.features.10.bn3.beta, %para153_moment2.features.11.weight, %para154_moment2.features.13.conv1.weight, %para155_moment2.features.13.bn1.gamma, %para156_moment2.features.13.bn1.beta, %para157_moment2.features.13.conv2.weight, %para158_moment2.features.13.bn2.gamma, %para159_moment2.features.13.bn2.beta, %para160_moment2.features.13.conv3.weight, %para161_moment2.features.13.bn3.gamma, %para162_moment2.features.13.bn3.beta, %para163_moment2.features.13.downsample.0.weight, %para164_moment2.features.13.downsample.1.gamma, %para165_moment2.features.13.downsample.1.beta, %para166_moment2.classifier.1.weight, %para167_moment2.classifier.1.bias, %para168_moment2.classifier.4.weight, %para169_moment2.classifier.4.bias, %para170_moment2.classifier.6.weight, %para171_moment2.classifier.6.bias, %para172_moment2.classifier.8.weight, %para173_moment2.classifier.8.bias)
      : (<Ref[Tensor[Float32]], (96, 3, 11, 11), ref_key=:moment2.features.0.weight>, <Ref[Tensor[Float32]], (32, 96, 1, 1), ref_key=:moment2.features.3.conv1.weight>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.features.3.bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.features.3.bn1.beta>, <Ref[Tensor[Float32]], (32, 32, 3, 3), ref_key=:moment2.features.3.conv2.weight>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.features.3.bn2.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.features.3.bn2.beta>, <Ref[Tensor[Float32]], (128, 32, 1, 1), ref_key=:moment2.features.3.conv3.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.3.bn3.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.3.bn3.beta>, <Ref[Tensor[Float32]], (128, 96, 1, 1), ref_key=:moment2.features.3.downsample.0.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.3.downsample.1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.3.downsample.1.beta>, <Ref[Tensor[Float32]], (256, 128, 5, 5), ref_key=:moment2.features.4.weight>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:moment2.features.7.conv1.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moment2.features.7.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moment2.features.7.bn1.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moment2.features.7.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moment2.features.7.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moment2.features.7.bn2.beta>, <Ref[Tensor[Float32]], (512, 64, 1, 1), ref_key=:moment2.features.7.conv3.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moment2.features.7.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moment2.features.7.bn3.beta>, <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:moment2.features.7.downsample.0.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moment2.features.7.downsample.1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:moment2.features.7.downsample.1.beta>, <Ref[Tensor[Float32]], (768, 512, 3, 3), ref_key=:moment2.features.8.weight>, <Ref[Tensor[Float32]], (128, 768, 1, 1), ref_key=:moment2.features.10.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.10.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.10.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moment2.features.10.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.10.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.10.bn2.beta>, <Ref[Tensor[Float32]], (768, 128, 1, 1), ref_key=:moment2.features.10.conv3.weight>, <Ref[Tensor[Float32]], (768), ref_key=:moment2.features.10.bn3.gamma>, <Ref[Tensor[Float32]], (768), ref_key=:moment2.features.10.bn3.beta>, <Ref[Tensor[Float32]], (1024, 768, 3, 3), ref_key=:moment2.features.11.weight>, <Ref[Tensor[Float32]], (128, 1024, 1, 1), ref_key=:moment2.features.13.conv1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.13.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.13.bn1.beta>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:moment2.features.13.conv2.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.13.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.13.bn2.beta>, <Ref[Tensor[Float32]], (1280, 128, 1, 1), ref_key=:moment2.features.13.conv3.weight>, <Ref[Tensor[Float32]], (1280), ref_key=:moment2.features.13.bn3.gamma>, <Ref[Tensor[Float32]], (1280), ref_key=:moment2.features.13.bn3.beta>, <Ref[Tensor[Float32]], (1280, 1024, 1, 1), ref_key=:moment2.features.13.downsample.0.weight>, <Ref[Tensor[Float32]], (1280), ref_key=:moment2.features.13.downsample.1.gamma>, <Ref[Tensor[Float32]], (1280), ref_key=:moment2.features.13.downsample.1.beta>, <Ref[Tensor[Float32]], (1024, 1280), ref_key=:moment2.classifier.1.weight>, <Ref[Tensor[Float32]], (1024), ref_key=:moment2.classifier.1.bias>, <Ref[Tensor[Float32]], (512, 1024), ref_key=:moment2.classifier.4.weight>, <Ref[Tensor[Float32]], (512), ref_key=:moment2.classifier.4.bias>, <Ref[Tensor[Float32]], (256, 512), ref_key=:moment2.classifier.6.weight>, <Ref[Tensor[Float32]], (256), ref_key=:moment2.classifier.6.bias>, <Ref[Tensor[Float32]], (5, 256), ref_key=:moment2.classifier.8.weight>, <Ref[Tensor[Float32]], (5), ref_key=:moment2.classifier.8.bias>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:913/        moment2 = self.moment2/
  %11(lr) = call @get_lr_1170()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:920/        lr = self.get_lr()/
  %12(gradients) = $(mindspore_nn_optim_adam_Adam_construct_1122):call @flatten_gradients_1125(%para222_gradients)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:914/        gradients = self.flatten_gradients(gradients)/
  %13(gradients) = $(mindspore_nn_optim_adam_Adam_construct_1122):call @decay_weight_1126(%12)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:915/        gradients = self.decay_weight(gradients)/
  %14(gradients) = $(mindspore_nn_optim_adam_Adam_construct_1123):call @gradients_centralization_1140(%13)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:917/            gradients = self.gradients_centralization(gradients)/
  %15(gradients) = call @scale_grad_1171(%14)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:918/        gradients = self.scale_grad(gradients)/
  %16(gradients) = call @_grad_sparse_indices_deduplicate_1172(%15)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:919/        gradients = self._grad_sparse_indices_deduplicate(gradients)/
  %17(CNode_1174) = call @_apply_adam_1173(%8, %2, %4, %9, %10, %11, %16)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:928/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  %18(CNode_1175) = Depend[side_effect_propagate: I64(1)](%17, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:928/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  Return(%18)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:928/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_1138:gradients{[0]: ValueNode<FuncGraph> scale_grad_1171, [1]: gradients}
#   2: @mindspore_nn_optim_adam_Adam_construct_1138:gradients{[0]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_1172, [1]: gradients}
#   3: @mindspore_nn_optim_adam_Adam_construct_1138:lr{[0]: ValueNode<FuncGraph> get_lr_1170}
#   4: @mindspore_nn_optim_adam_Adam_construct_1138:CNode_1161{[0]: ValueNode<DoSignaturePrimitive> S_Prim_AssignAdd, [1]: param_global_step, [2]: ValueNode<Tensor> Tensor(shape=[1], dtype=Int32, value=[1])}
#   5: @mindspore_nn_optim_adam_Adam_construct_1138:beta1_power{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_beta1_power, [2]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.9)}
#   6: @mindspore_nn_optim_adam_Adam_construct_1138:CNode_1163{[0]: ValueNode<FuncGraph> assign_1162, [1]: param_beta1_power, [2]: beta1_power}
#   7: @mindspore_nn_optim_adam_Adam_construct_1138:beta2_power{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_beta2_power, [2]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.999)}
#   8: @mindspore_nn_optim_adam_Adam_construct_1138:CNode_1164{[0]: ValueNode<FuncGraph> assign_1162, [1]: param_beta2_power, [2]: beta2_power}
#   9: @mindspore_nn_optim_adam_Adam_construct_1138:CNode_1174{[0]: ValueNode<FuncGraph> _apply_adam_1173, [1]: CNode_1167, [2]: beta1_power, [3]: beta2_power, [4]: CNode_1168, [5]: CNode_1169, [6]: lr, [7]: gradients}
#  10: @mindspore_nn_optim_adam_Adam_construct_1138:CNode_1176{[0]: ValueNode<Primitive> Return, [1]: CNode_1175}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1146 : 000001D8B244CDC0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1146 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1130]() {
  %1(CNode_1177) = S_Prim_equal("mean", "mean")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %2(CNode_1178) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %3(CNode_1179) = Switch(%2, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1180, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1181)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %4(CNode_1182) = %3()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1146:CNode_1177{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<StringImm> mean, [2]: ValueNode<StringImm> mean}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1146:CNode_1178{[0]: ValueNode<Primitive> Cond, [1]: CNode_1177, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1146:CNode_1179{[0]: ValueNode<Primitive> Switch, [1]: CNode_1178, [2]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1180, [3]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1181}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1146:CNode_1182{[0]: CNode_1179}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1146:CNode_1183{[0]: ValueNode<Primitive> Return, [1]: CNode_1182}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1151 : 000001D8B24458E0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1151 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para232_input_data) {
  %1(CNode_1185) = call @mindspore_nn_layer_container_SequentialCell_construct_1184(I64(0), %para232_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1151:CNode_1186{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_1187}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1151:CNode_1185{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1184, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1151:CNode_1188{[0]: ValueNode<Primitive> Return, [1]: CNode_1185}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1150 : 000001D997C08660
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1150 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para233_input_data) {
  %1(CNode_1085) = call @mindspore_nn_layer_container_SequentialCell_construct_1189(I64(0), %para233_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1150:CNode_1086{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_1087}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1150:CNode_1085{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1189, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1150:CNode_1088{[0]: ValueNode<Primitive> Return, [1]: CNode_1085}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_1152 : 000001D997BFDC60
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_1152 parent: [subgraph @decay_weight_1126]() {
  Return(%para226_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:450/        return gradients/
}
# Order:
#   1: @decay_weight_1152:CNode_1190{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_1155 : 000001D997C00C30
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_1155 parent: [subgraph @flatten_gradients_1125]() {
  Return(%para227_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:427/        return gradients/
}
# Order:
#   1: @flatten_gradients_1155:CNode_1191{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_1158 : 000001D997C01180
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_1158 parent: [subgraph @gradients_centralization_1140]() {
  %1(CNode_1193) = call @gradients_centralization_1192()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_1158:CNode_1193{[0]: ValueNode<FuncGraph> gradients_centralization_1192}
#   2: @gradients_centralization_1158:CNode_1194{[0]: ValueNode<Primitive> Return, [1]: CNode_1193}


subgraph attr:
subgraph instance: assign_1162 : 000001D997C0CB70
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\ops\function\parameter_func.py:24/def assign(variable, value):/
subgraph @assign_1162(%para234_variable, %para235_value) {
  %1(CNode_1195) = S_Prim_Assign[input_names: ["ref", "value"], output_names: ["output"], side_effect_mem: Bool(1)](%para234_variable, %para235_value)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\ops\function\parameter_func.py:58/    return assign_(variable, value)/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\ops\function\parameter_func.py:58/    return assign_(variable, value)/
}
# Order:
#   1: @assign_1162:CNode_1195{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Assign, [1]: param_variable, [2]: param_value}
#   2: @assign_1162:CNode_1196{[0]: ValueNode<Primitive> Return, [1]: CNode_1195}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_1173 : 000001D997C07BC0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_1173(%para236_params, %para237_beta1_power, %para238_beta2_power, %para239_moment1, %para240_moment2, %para241_lr, %para242_gradients) {
  %1(CNode_1198) = call @_apply_adam_1197()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:817/        if self.use_offload:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:817/        if self.use_offload:/
}
# Order:
#   1: @_apply_adam_1173:CNode_1198{[0]: ValueNode<FuncGraph> _apply_adam_1197}
#   2: @_apply_adam_1173:CNode_1199{[0]: ValueNode<Primitive> Return, [1]: CNode_1198}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_1172 : 000001D997C0DB60
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_1172(%para243_gradients) {
  %1(CNode_1200) = S_Prim_not_equal("CPU", "CPU")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %2(CNode_1201) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %3(CNode_1202) = Switch(%2, @_grad_sparse_indices_deduplicate_1203, @_grad_sparse_indices_deduplicate_1204)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %4(CNode_1205) = %3()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %5(CNode_1206) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %6(CNode_1207) = Switch(%5, @_grad_sparse_indices_deduplicate_1208, @_grad_sparse_indices_deduplicate_1209)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %7(CNode_1210) = %6()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %8(CNode_1212) = call @_grad_sparse_indices_deduplicate_1211(%7)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:919/        gradients = self._grad_sparse_indices_deduplicate(gradients)/
  Return(%8)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_1172:CNode_1200{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: ValueNode<StringImm> CPU, [2]: ValueNode<StringImm> CPU}
#   2: @_grad_sparse_indices_deduplicate_1172:CNode_1201{[0]: ValueNode<Primitive> Cond, [1]: CNode_1200, [2]: ValueNode<BoolImm> false}
#   3: @_grad_sparse_indices_deduplicate_1172:CNode_1202{[0]: ValueNode<Primitive> Switch, [1]: CNode_1201, [2]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_1203, [3]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_1204}
#   4: @_grad_sparse_indices_deduplicate_1172:CNode_1205{[0]: CNode_1202}
#   5: @_grad_sparse_indices_deduplicate_1172:CNode_1206{[0]: ValueNode<Primitive> Cond, [1]: CNode_1205, [2]: ValueNode<BoolImm> false}
#   6: @_grad_sparse_indices_deduplicate_1172:CNode_1207{[0]: ValueNode<Primitive> Switch, [1]: CNode_1206, [2]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_1208, [3]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_1209}
#   7: @_grad_sparse_indices_deduplicate_1172:CNode_1210{[0]: CNode_1207}
#   8: @_grad_sparse_indices_deduplicate_1172:CNode_1212{[0]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_1211, [1]: CNode_1210}
#   9: @_grad_sparse_indices_deduplicate_1172:CNode_1213{[0]: ValueNode<Primitive> Return, [1]: CNode_1212}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_1171 : 000001D997C0D610
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_1171(%para244_gradients) {
  %1(CNode_1215) = call @scale_grad_1214()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_1171:CNode_1215{[0]: ValueNode<FuncGraph> scale_grad_1214}
#   2: @scale_grad_1171:CNode_1216{[0]: ValueNode<Primitive> Return, [1]: CNode_1215}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_1170 : 000001D997C0A0F0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_1170 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037]() {
  %1(CNode_1218) = call @get_lr_1217()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_1170:CNode_1218{[0]: ValueNode<FuncGraph> get_lr_1217}
#   2: @get_lr_1170:CNode_1219{[0]: ValueNode<Primitive> Return, [1]: CNode_1218}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1180 : 000001D8B24547F0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1180 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1130]() {
  %1(x) = S_Prim_SparseSoftmaxCrossEntropyWithLogits[is_grad: Bool(0), sens: F32(1), input_names: ["features", "labels"], output_names: ["output"]](%para228_logits, %para229_labels)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:782/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:783/                return x/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1180:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SparseSoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: param_labels}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1180:CNode_1220{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1181 : 000001D8B24522C0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1181 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1130]() {
  %1(CNode_1222) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1181:CNode_1222{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1181:CNode_1223{[0]: ValueNode<Primitive> Return, [1]: CNode_1222}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1224 : 000001D8B244F2F0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1224(%para245_x) {
  %1(CNode_1225) = S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_1226) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_1227) = Switch(%2, @mindspore_nn_layer_basic_Dropout_construct_1228, @mindspore_nn_layer_basic_Dropout_construct_1229)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_1230) = %3()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %5(CNode_1231) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %6(CNode_1232) = Switch(%5, @mindspore_nn_layer_basic_Dropout_construct_1233, @mindspore_nn_layer_basic_Dropout_construct_1234)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %7(CNode_1235) = %6()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1224:CNode_1225{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: ValueNode<BoolImm> true}
#   2: @mindspore_nn_layer_basic_Dropout_construct_1224:CNode_1226{[0]: ValueNode<Primitive> Cond, [1]: CNode_1225, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_1224:CNode_1227{[0]: ValueNode<Primitive> Switch, [1]: CNode_1226, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1228, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1229}
#   4: @mindspore_nn_layer_basic_Dropout_construct_1224:CNode_1230{[0]: CNode_1227}
#   5: @mindspore_nn_layer_basic_Dropout_construct_1224:CNode_1231{[0]: ValueNode<Primitive> Cond, [1]: CNode_1230, [2]: ValueNode<BoolImm> false}
#   6: @mindspore_nn_layer_basic_Dropout_construct_1224:CNode_1232{[0]: ValueNode<Primitive> Switch, [1]: CNode_1231, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1233, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1234}
#   7: @mindspore_nn_layer_basic_Dropout_construct_1224:CNode_1235{[0]: CNode_1232}
#   8: @mindspore_nn_layer_basic_Dropout_construct_1224:CNode_1236{[0]: ValueNode<Primitive> Return, [1]: CNode_1235}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_1237 : 000001D8B244BDD0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_1237 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para246_x) {
  %1(CNode_1239) = call @L_mindspore_nn_layer_basic_Dense_construct_1238(%para246_x, %para53_classifier.1.bias, %para52_classifier.1.weight)
      : (<null>, <Ref[Tensor[Float32]], (1024), ref_key=:classifier.1.bias>, <Ref[Tensor[Float32]], (1024, 1280), ref_key=:classifier.1.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_1237:CNode_1239{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1238, [1]: param_x, [2]: param_classifier.1.bias, [3]: param_classifier.1.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_1237:CNode_1240{[0]: ValueNode<Primitive> Return, [1]: CNode_1239}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_1241 : 000001D8B244D310
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_1241(%para247_x) {
  %1(CNode_1242) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para247_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/2-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/2-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_1241:CNode_1242{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_1241:CNode_1243{[0]: ValueNode<Primitive> Return, [1]: CNode_1242}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1244 : 000001D8B2449DF0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1244(%para248_x) {
  %1(CNode_1245) = S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_1246) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_1247) = Switch(%2, @mindspore_nn_layer_basic_Dropout_construct_1248, @mindspore_nn_layer_basic_Dropout_construct_1249)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_1250) = %3()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %5(CNode_1251) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %6(CNode_1252) = Switch(%5, @mindspore_nn_layer_basic_Dropout_construct_1253, @mindspore_nn_layer_basic_Dropout_construct_1254)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %7(CNode_1255) = %6()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1244:CNode_1245{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: ValueNode<BoolImm> true}
#   2: @mindspore_nn_layer_basic_Dropout_construct_1244:CNode_1246{[0]: ValueNode<Primitive> Cond, [1]: CNode_1245, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_1244:CNode_1247{[0]: ValueNode<Primitive> Switch, [1]: CNode_1246, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1248, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1249}
#   4: @mindspore_nn_layer_basic_Dropout_construct_1244:CNode_1250{[0]: CNode_1247}
#   5: @mindspore_nn_layer_basic_Dropout_construct_1244:CNode_1251{[0]: ValueNode<Primitive> Cond, [1]: CNode_1250, [2]: ValueNode<BoolImm> false}
#   6: @mindspore_nn_layer_basic_Dropout_construct_1244:CNode_1252{[0]: ValueNode<Primitive> Switch, [1]: CNode_1251, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1253, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1254}
#   7: @mindspore_nn_layer_basic_Dropout_construct_1244:CNode_1255{[0]: CNode_1252}
#   8: @mindspore_nn_layer_basic_Dropout_construct_1244:CNode_1256{[0]: ValueNode<Primitive> Return, [1]: CNode_1255}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_1257 : 000001D8B2449350
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_1257 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para249_x) {
  %1(CNode_1258) = call @L_mindspore_nn_layer_basic_Dense_construct_1238(%para249_x, %para55_classifier.4.bias, %para54_classifier.4.weight)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:classifier.4.bias>, <Ref[Tensor[Float32]], (512, 1024), ref_key=:classifier.4.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/4-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_1257:CNode_1258{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1238, [1]: param_x, [2]: param_classifier.4.bias, [3]: param_classifier.4.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_1257:CNode_1259{[0]: ValueNode<Primitive> Return, [1]: CNode_1258}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_1260 : 000001D8B2448E00
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_1260(%para250_x) {
  %1(CNode_1261) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para250_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/5-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/5-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_1260:CNode_1261{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_1260:CNode_1262{[0]: ValueNode<Primitive> Return, [1]: CNode_1261}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_1263 : 000001D8B24498A0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_1263 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para251_x) {
  %1(CNode_1264) = call @L_mindspore_nn_layer_basic_Dense_construct_1238(%para251_x, %para57_classifier.6.bias, %para56_classifier.6.weight)
      : (<null>, <Ref[Tensor[Float32]], (256), ref_key=:classifier.6.bias>, <Ref[Tensor[Float32]], (256, 512), ref_key=:classifier.6.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/6-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_1263:CNode_1264{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1238, [1]: param_x, [2]: param_classifier.6.bias, [3]: param_classifier.6.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_1263:CNode_1265{[0]: ValueNode<Primitive> Return, [1]: CNode_1264}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_1266 : 000001D8B24488B0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_1266(%para252_x) {
  %1(CNode_1267) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para252_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/7-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/7-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_1266:CNode_1267{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_1266:CNode_1268{[0]: ValueNode<Primitive> Return, [1]: CNode_1267}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_1269 : 000001D8B2442910
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_1269 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para253_x) {
  %1(CNode_1270) = call @L_mindspore_nn_layer_basic_Dense_construct_1238(%para253_x, %para59_classifier.8.bias, %para58_classifier.8.weight)
      : (<null>, <Ref[Tensor[Float32]], (5), ref_key=:classifier.8.bias>, <Ref[Tensor[Float32]], (5, 256), ref_key=:classifier.8.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/8-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_1269:CNode_1270{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1238, [1]: param_x, [2]: param_classifier.8.bias, [3]: param_classifier.8.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_1269:CNode_1271{[0]: ValueNode<Primitive> Return, [1]: CNode_1270}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1184 : 000001D8B244F840
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1184 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1151](%para254_, %para255_) {
  %1(CNode_1187) = $(mindspore_nn_layer_container_SequentialCell_construct_1151):MakeTuple(@mindspore_nn_layer_basic_Dropout_construct_1224, @mindspore_nn_layer_basic_Dense_construct_1237, @mindspore_nn_layer_activation_ReLU_construct_1241, @mindspore_nn_layer_basic_Dropout_construct_1244, @mindspore_nn_layer_basic_Dense_construct_1257, @mindspore_nn_layer_activation_ReLU_construct_1260, @mindspore_nn_layer_basic_Dense_construct_1263, @mindspore_nn_layer_activation_ReLU_construct_1266, @mindspore_nn_layer_basic_Dense_construct_1269)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1186) = $(mindspore_nn_layer_container_SequentialCell_construct_1151):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_1272) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para254_@CNode_1273, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1274) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_1275, @mindspore_nn_layer_container_SequentialCell_construct_1276)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_1277) = %4()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1184:CNode_1272{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.73, [1]: param_@CNode_1273, [2]: CNode_1186}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1184:CNode_1274{[0]: ValueNode<Primitive> Switch, [1]: CNode_1272, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1275, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1276}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1184:CNode_1277{[0]: CNode_1274}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1184:CNode_1278{[0]: ValueNode<Primitive> Return, [1]: CNode_1277}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1279 : 000001D8B2443E50
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1279 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para256_x) {
  %1(CNode_1114) = call @mindspore_nn_layer_conv_Conv2d_construct_1280()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1279:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.0.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1279:CNode_1114{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1280}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1279:CNode_1115{[0]: ValueNode<Primitive> Return, [1]: CNode_1114}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_1281 : 000001D8B24403E0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_1281(%para257_x) {
  %1(CNode_1282) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para257_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/1-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/1-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_1281:CNode_1282{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_1281:CNode_1283{[0]: ValueNode<Primitive> Return, [1]: CNode_1282}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1284 : 000001D8B243C420
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1284(%para258_x) {
  %1(CNode_1285) = getattr(%para258_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %2(CNode_1286) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %3(CNode_1287) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %4(CNode_1288) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_1289, @mindspore_nn_layer_pooling_MaxPool2d_construct_1290)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %5(CNode_1291) = %4()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1284:CNode_1285{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1284:CNode_1286{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_1285, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1284:CNode_1287{[0]: ValueNode<Primitive> Cond, [1]: CNode_1286, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1284:CNode_1288{[0]: ValueNode<Primitive> Switch, [1]: CNode_1287, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1289, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1290}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_1284:CNode_1291{[0]: CNode_1288}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_1284:CNode_1292{[0]: ValueNode<Primitive> Return, [1]: CNode_1291}


subgraph attr:
training : 1
subgraph instance: __main___Bottleneck_construct_1293 : 000001D997C388B0
# In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:19/
subgraph @__main___Bottleneck_construct_1293 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para259_x) {
  %1(out) = call @mindspore_nn_layer_conv_Conv2d_construct_1294(%para259_residual)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:21/
  %2(out) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1295(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:22/
  %3(out) = call @mindspore_nn_layer_activation_ReLU_construct_1296(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:23/
  %4(out) = call @mindspore_nn_layer_conv_Conv2d_construct_1297(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:24/
  %5(out) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1298(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:25/
  %6(out) = call @mindspore_nn_layer_activation_ReLU_construct_1296(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:26/
  %7(out) = call @mindspore_nn_layer_conv_Conv2d_construct_1299(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:27/
  %8(out) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1300(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:28/
  %9(residual) = call @mindspore_nn_layer_container_SequentialCell_construct_1301(%para259_residual)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:29/
  %10(out) = S_Prim_add(%8, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:30/
  %11(out) = call @mindspore_nn_layer_activation_ReLU_construct_1296(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:31/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:32/
}
# Order:
#   1: @__main___Bottleneck_construct_1293:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1294, [1]: param_residual}
#   2: @__main___Bottleneck_construct_1293:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1295, [1]: out}
#   3: @__main___Bottleneck_construct_1293:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_1296, [1]: out}
#   4: @__main___Bottleneck_construct_1293:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1297, [1]: out}
#   5: @__main___Bottleneck_construct_1293:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1298, [1]: out}
#   6: @__main___Bottleneck_construct_1293:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_1296, [1]: out}
#   7: @__main___Bottleneck_construct_1293:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1299, [1]: out}
#   8: @__main___Bottleneck_construct_1293:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1300, [1]: out}
#   9: @__main___Bottleneck_construct_1293:residual{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1301, [1]: param_residual}
#  10: @__main___Bottleneck_construct_1293:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: out, [2]: residual}
#  11: @__main___Bottleneck_construct_1293:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_1296, [1]: out}
#  12: @__main___Bottleneck_construct_1293:CNode_1302{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1303 : 000001D997C398A0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1303 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para260_x) {
  %1(CNode_1305) = call @mindspore_nn_layer_conv_Conv2d_construct_1304()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/4-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/4-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1303:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.4.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1303:CNode_1305{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1304}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1303:CNode_1306{[0]: ValueNode<Primitive> Return, [1]: CNode_1305}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_1307 : 000001D997C35E30
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_1307(%para261_x) {
  %1(CNode_1308) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para261_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/5-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/5-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_1307:CNode_1308{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_1307:CNode_1309{[0]: ValueNode<Primitive> Return, [1]: CNode_1308}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1310 : 000001D997C358E0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1310(%para262_x) {
  %1(CNode_1311) = getattr(%para262_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %2(CNode_1312) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %3(CNode_1313) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %4(CNode_1314) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_1315, @mindspore_nn_layer_pooling_MaxPool2d_construct_1316)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %5(CNode_1317) = %4()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1310:CNode_1311{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1310:CNode_1312{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_1311, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1310:CNode_1313{[0]: ValueNode<Primitive> Cond, [1]: CNode_1312, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1310:CNode_1314{[0]: ValueNode<Primitive> Switch, [1]: CNode_1313, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1315, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1316}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_1310:CNode_1317{[0]: CNode_1314}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_1310:CNode_1318{[0]: ValueNode<Primitive> Return, [1]: CNode_1317}


subgraph attr:
training : 1
subgraph instance: __main___Bottleneck_construct_1319 : 000001D997C2D960
# In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:19/
subgraph @__main___Bottleneck_construct_1319 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para263_x) {
  %1(out) = call @mindspore_nn_layer_conv_Conv2d_construct_1320(%para263_residual)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:21/
  %2(out) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1321(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:22/
  %3(out) = call @mindspore_nn_layer_activation_ReLU_construct_1322(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:23/
  %4(out) = call @mindspore_nn_layer_conv_Conv2d_construct_1323(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:24/
  %5(out) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1324(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:25/
  %6(out) = call @mindspore_nn_layer_activation_ReLU_construct_1322(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:26/
  %7(out) = call @mindspore_nn_layer_conv_Conv2d_construct_1325(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:27/
  %8(out) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1326(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:28/
  %9(residual) = call @mindspore_nn_layer_container_SequentialCell_construct_1327(%para263_residual)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:29/
  %10(out) = S_Prim_add(%8, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:30/
  %11(out) = call @mindspore_nn_layer_activation_ReLU_construct_1322(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:31/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:32/
}
# Order:
#   1: @__main___Bottleneck_construct_1319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1320, [1]: param_residual}
#   2: @__main___Bottleneck_construct_1319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1321, [1]: out}
#   3: @__main___Bottleneck_construct_1319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_1322, [1]: out}
#   4: @__main___Bottleneck_construct_1319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1323, [1]: out}
#   5: @__main___Bottleneck_construct_1319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1324, [1]: out}
#   6: @__main___Bottleneck_construct_1319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_1322, [1]: out}
#   7: @__main___Bottleneck_construct_1319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1325, [1]: out}
#   8: @__main___Bottleneck_construct_1319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1326, [1]: out}
#   9: @__main___Bottleneck_construct_1319:residual{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1327, [1]: param_residual}
#  10: @__main___Bottleneck_construct_1319:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: out, [2]: residual}
#  11: @__main___Bottleneck_construct_1319:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_1322, [1]: out}
#  12: @__main___Bottleneck_construct_1319:CNode_1328{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1329 : 000001D997C269D0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1329 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para264_x) {
  %1(CNode_1331) = call @mindspore_nn_layer_conv_Conv2d_construct_1330()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/8-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/8-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1329:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.8.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1329:CNode_1331{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1330}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1329:CNode_1332{[0]: ValueNode<Primitive> Return, [1]: CNode_1331}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_1333 : 000001D997C259E0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_1333(%para265_x) {
  %1(CNode_1334) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para265_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/9-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/9-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_1333:CNode_1334{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_1333:CNode_1335{[0]: ValueNode<Primitive> Return, [1]: CNode_1334}


subgraph attr:
training : 1
subgraph instance: __main___Bottleneck_construct_1336 : 000001D997C22F60
# In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:19/
subgraph @__main___Bottleneck_construct_1336 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para266_x) {
  %1(out) = call @mindspore_nn_layer_conv_Conv2d_construct_1337(%para266_residual)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:21/
  %2(out) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1338(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:22/
  %3(out) = call @mindspore_nn_layer_activation_ReLU_construct_1339(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:23/
  %4(out) = call @mindspore_nn_layer_conv_Conv2d_construct_1340(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:24/
  %5(out) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1341(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:25/
  %6(out) = call @mindspore_nn_layer_activation_ReLU_construct_1339(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:26/
  %7(out) = call @mindspore_nn_layer_conv_Conv2d_construct_1342(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:27/
  %8(out) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:28/
  %9(residual) = call @mindspore_nn_layer_container_SequentialCell_construct_1344(%para266_residual)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:29/
  %10(out) = S_Prim_add(%8, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:30/
  %11(out) = call @mindspore_nn_layer_activation_ReLU_construct_1339(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:31/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:32/
}
# Order:
#   1: @__main___Bottleneck_construct_1336:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1337, [1]: param_residual}
#   2: @__main___Bottleneck_construct_1336:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1338, [1]: out}
#   3: @__main___Bottleneck_construct_1336:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_1339, [1]: out}
#   4: @__main___Bottleneck_construct_1336:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1340, [1]: out}
#   5: @__main___Bottleneck_construct_1336:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1341, [1]: out}
#   6: @__main___Bottleneck_construct_1336:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_1339, [1]: out}
#   7: @__main___Bottleneck_construct_1336:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1342, [1]: out}
#   8: @__main___Bottleneck_construct_1336:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1343, [1]: out}
#   9: @__main___Bottleneck_construct_1336:residual{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1344, [1]: param_residual}
#  10: @__main___Bottleneck_construct_1336:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: out, [2]: residual}
#  11: @__main___Bottleneck_construct_1336:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_1339, [1]: out}
#  12: @__main___Bottleneck_construct_1336:CNode_1345{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1346 : 000001D997C1DA60
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1346 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para267_x) {
  %1(CNode_1348) = call @mindspore_nn_layer_conv_Conv2d_construct_1347()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/11-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/11-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1346:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.11.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1346:CNode_1348{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1347}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1346:CNode_1349{[0]: ValueNode<Primitive> Return, [1]: CNode_1348}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_1350 : 000001D997C1CFC0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_1350(%para268_x) {
  %1(CNode_1351) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para268_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/12-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/12-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_1350:CNode_1351{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_1350:CNode_1352{[0]: ValueNode<Primitive> Return, [1]: CNode_1351}


subgraph attr:
training : 1
subgraph instance: __main___Bottleneck_construct_1353 : 000001D997C105E0
# In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:19/
subgraph @__main___Bottleneck_construct_1353 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para269_x) {
  %1(out) = call @mindspore_nn_layer_conv_Conv2d_construct_1354(%para269_residual)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:21/
  %2(out) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1355(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:22/
  %3(out) = call @mindspore_nn_layer_activation_ReLU_construct_1356(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:23/
  %4(out) = call @mindspore_nn_layer_conv_Conv2d_construct_1357(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:24/
  %5(out) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1358(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:25/
  %6(out) = call @mindspore_nn_layer_activation_ReLU_construct_1356(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:26/
  %7(out) = call @mindspore_nn_layer_conv_Conv2d_construct_1359(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:27/
  %8(out) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1360(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:28/
  %9(residual) = call @mindspore_nn_layer_container_SequentialCell_construct_1361(%para269_residual)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:29/
  %10(out) = S_Prim_add(%8, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:30/
  %11(out) = call @mindspore_nn_layer_activation_ReLU_construct_1356(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:31/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck)
      # In file C:\Users\EC319\AppData\Local\Temp\ipykernel_10904\831123025.py:32/
}
# Order:
#   1: @__main___Bottleneck_construct_1353:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1354, [1]: param_residual}
#   2: @__main___Bottleneck_construct_1353:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1355, [1]: out}
#   3: @__main___Bottleneck_construct_1353:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_1356, [1]: out}
#   4: @__main___Bottleneck_construct_1353:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1357, [1]: out}
#   5: @__main___Bottleneck_construct_1353:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1358, [1]: out}
#   6: @__main___Bottleneck_construct_1353:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_1356, [1]: out}
#   7: @__main___Bottleneck_construct_1353:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1359, [1]: out}
#   8: @__main___Bottleneck_construct_1353:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1360, [1]: out}
#   9: @__main___Bottleneck_construct_1353:residual{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1361, [1]: param_residual}
#  10: @__main___Bottleneck_construct_1353:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: out, [2]: residual}
#  11: @__main___Bottleneck_construct_1353:out{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_1356, [1]: out}
#  12: @__main___Bottleneck_construct_1353:CNode_1362{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_1363 : 000001D997C0B0E0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:1356/    def construct(self, input):/
subgraph @mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_1363(%para270_input) {
  %1(CNode_1364) = S_Prim_AdaptiveAvgPool2D[output_size: (I64(1), I64(1)), input_names: ["x"], output_names: ["y"]](%para270_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/14-AdaptiveAvgPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:1357/        return self.adaptive_avgpool2d(input)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/14-AdaptiveAvgPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:1357/        return self.adaptive_avgpool2d(input)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_1363:CNode_1364{[0]: ValueNode<DoSignaturePrimitive> S_Prim_AdaptiveAvgPool2D, [1]: param_input}
#   2: @mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_1363:CNode_1365{[0]: ValueNode<Primitive> Return, [1]: CNode_1364}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1189 : 000001D8B2444E40
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1189 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1150](%para271_, %para272_) {
  %1(CNode_1087) = $(mindspore_nn_layer_container_SequentialCell_construct_1150):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1279, @mindspore_nn_layer_activation_ReLU_construct_1281, @mindspore_nn_layer_pooling_MaxPool2d_construct_1284, @__main___Bottleneck_construct_1293, @mindspore_nn_layer_conv_Conv2d_construct_1303, @mindspore_nn_layer_activation_ReLU_construct_1307, @mindspore_nn_layer_pooling_MaxPool2d_construct_1310, @__main___Bottleneck_construct_1319, @mindspore_nn_layer_conv_Conv2d_construct_1329, @mindspore_nn_layer_activation_ReLU_construct_1333, @__main___Bottleneck_construct_1336, @mindspore_nn_layer_conv_Conv2d_construct_1346, @mindspore_nn_layer_activation_ReLU_construct_1350, @__main___Bottleneck_construct_1353, @mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_1363)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1086) = $(mindspore_nn_layer_container_SequentialCell_construct_1150):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_1103) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para271_@CNode_1052, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1104) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_1366, @mindspore_nn_layer_container_SequentialCell_construct_1367)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_1106) = %4()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1189:CNode_1103{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.73, [1]: param_@CNode_1052, [2]: CNode_1086}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1189:CNode_1104{[0]: ValueNode<Primitive> Switch, [1]: CNode_1103, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1366, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1367}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1189:CNode_1106{[0]: CNode_1104}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1189:CNode_1107{[0]: ValueNode<Primitive> Return, [1]: CNode_1106}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_1192 : 000001D997C06680
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_1192 parent: [subgraph @gradients_centralization_1140]() {
  Return(%para231_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:469/        return gradients/
}
# Order:
#   1: @gradients_centralization_1192:CNode_1368{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_1197 : 000001D997C0C0D0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_1197 parent: [subgraph @_apply_adam_1173]() {
  %1(CNode_1370) = call @_apply_adam_1369()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:826/            if self.use_dist_optimizer:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:826/            if self.use_dist_optimizer:/
}
# Order:
#   1: @_apply_adam_1197:CNode_1370{[0]: ValueNode<FuncGraph> _apply_adam_1369}
#   2: @_apply_adam_1197:CNode_1371{[0]: ValueNode<Primitive> Return, [1]: CNode_1370}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
after_block : 1
subgraph instance: _grad_sparse_indices_deduplicate_1211 : 000001D997C0E0B0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_1211(%para273_) {
  Return(%para273_phi_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:521/        return gradients/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_1211:CNode_1372{[0]: ValueNode<Primitive> Return, [1]: param_phi_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_1208 : 000001D997C0B630
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_1208 parent: [subgraph @_grad_sparse_indices_deduplicate_1172]() {
  %1(CNode_1373) = S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_indices_deduplicate)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:520/            gradients = self.map_(F.partial(_indices_deduplicate), gradients)/
  %2(gradients) = S_Prim_map(%1, %para243_gradients)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:520/            gradients = self.map_(F.partial(_indices_deduplicate), gradients)/
  Return(%2)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:520/            gradients = self.map_(F.partial(_indices_deduplicate), gradients)/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_1208:CNode_1373{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_indices_deduplicate}
#   2: @_grad_sparse_indices_deduplicate_1208:gradients{[0]: ValueNode<DoSignaturePrimitive> S_Prim_map, [1]: CNode_1373, [2]: param_gradients}
#   3: @_grad_sparse_indices_deduplicate_1208:CNode_1374{[0]: ValueNode<Primitive> Return, [1]: gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_1209 : 000001D997C07670
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_1209 parent: [subgraph @_grad_sparse_indices_deduplicate_1172]() {
  Return(%para243_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_1209:CNode_1375{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_1203 : 000001D997C09650
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_1203() {
  Return(Bool(1))
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_1203:CNode_1376{[0]: ValueNode<Primitive> Return, [1]: ValueNode<BoolImm> true}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_1204 : 000001D997C09100
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_1204 parent: [subgraph @_grad_sparse_indices_deduplicate_1172]() {
  %1(CNode_1200) = $(_grad_sparse_indices_deduplicate_1172):S_Prim_not_equal("CPU", "CPU")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_1204:CNode_1377{[0]: ValueNode<Primitive> Return, [1]: CNode_1200}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_1214 : 000001D997C07120
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_1214 parent: [subgraph @scale_grad_1171]() {
  %1(CNode_1379) = call @scale_grad_1378()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_1214:CNode_1379{[0]: ValueNode<FuncGraph> scale_grad_1378}
#   2: @scale_grad_1214:CNode_1380{[0]: ValueNode<Primitive> Return, [1]: CNode_1379}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_1217 : 000001D997C0A640
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_1217 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037]() {
  %1(CNode_1382) = call @get_lr_1381()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_1217:CNode_1382{[0]: ValueNode<FuncGraph> get_lr_1381}
#   2: @get_lr_1217:CNode_1383{[0]: ValueNode<Primitive> Return, [1]: CNode_1382}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221 : 000001D8B24532B0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1130]() {
  %1(CNode_1385) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1384()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221:CNode_1386{[0]: ValueNode<FuncGraph> shape_1387, [1]: param_logits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221:CNode_1388{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221:CNode_1389{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1386, [2]: CNode_1388}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221:labels{[0]: ValueNode<DoSignaturePrimitive> S_Prim_OneHot, [1]: param_labels, [2]: CNode_1389, [3]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1), [4]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0)}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221:CNode_1385{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1384}
#   6: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221:CNode_1390{[0]: ValueNode<Primitive> Return, [1]: CNode_1385}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1233 : 000001D8B244DDB0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1233 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_1224]() {
  Return(%para245_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:192/            return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1233:CNode_1391{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1234 : 000001D8B244E850
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1234 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_1224]() {
  %1(CNode_1393) = call @mindspore_nn_layer_basic_Dropout_construct_1392()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1234:CNode_1393{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1392}
#   2: @mindspore_nn_layer_basic_Dropout_construct_1234:CNode_1394{[0]: ValueNode<Primitive> Return, [1]: CNode_1393}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1228 : 000001D8B2450D80
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1228 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_1224]() {
  %1(CNode_1225) = $(mindspore_nn_layer_basic_Dropout_construct_1224):S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1228:CNode_1395{[0]: ValueNode<Primitive> Return, [1]: CNode_1225}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1229 : 000001D8B2451D70
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1229() {
  %1(CNode_1396) = S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_1397) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_1398) = Switch(%2, @mindspore_nn_layer_basic_Dropout_construct_1399, @mindspore_nn_layer_basic_Dropout_construct_1400)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_1401) = %3()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1229:CNode_1396{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 1}
#   2: @mindspore_nn_layer_basic_Dropout_construct_1229:CNode_1397{[0]: ValueNode<Primitive> Cond, [1]: CNode_1396, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_1229:CNode_1398{[0]: ValueNode<Primitive> Switch, [1]: CNode_1397, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1399, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1400}
#   4: @mindspore_nn_layer_basic_Dropout_construct_1229:CNode_1401{[0]: CNode_1398}
#   5: @mindspore_nn_layer_basic_Dropout_construct_1229:CNode_1402{[0]: ValueNode<Primitive> Return, [1]: CNode_1401}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_1238 : 000001D8B2446380
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_1238(%para274_x, %para275_, %para276_) {
  %1(x_shape) = S_Prim_Shape(%para274_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_1403) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_1404) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
  %4(CNode_1405) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_1406) = S_Prim_not_equal(%4, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_1407) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_1408) = Switch(%6, @L_mindspore_nn_layer_basic_Dense_construct_1409, @L_mindspore_nn_layer_basic_Dense_construct_1410)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_1411) = %7()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %9(CNode_1413) = call @L_mindspore_nn_layer_basic_Dense_construct_1412(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %10(CNode_1414) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_1238:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_1238:CNode_1403{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_1238:CNode_1405{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_1238:CNode_1406{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_1405, [2]: ValueNode<Int64Imm> 2}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_1238:CNode_1407{[0]: ValueNode<Primitive> Cond, [1]: CNode_1406, [2]: ValueNode<BoolImm> false}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_1238:CNode_1408{[0]: ValueNode<Primitive> Switch, [1]: CNode_1407, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1409, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1410}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_1238:CNode_1411{[0]: CNode_1408}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_1238:CNode_1413{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1412, [1]: CNode_1411}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_1238:CNode_1414{[0]: ValueNode<Primitive> Depend, [1]: CNode_1413, [2]: CNode_1404}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_1238:CNode_1240{[0]: ValueNode<Primitive> Return, [1]: CNode_1414}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1253 : 000001D8B244E300
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1253 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_1244]() {
  Return(%para248_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:192/            return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1253:CNode_1415{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1254 : 000001D8B2443900
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1254 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_1244]() {
  %1(CNode_1417) = call @mindspore_nn_layer_basic_Dropout_construct_1416()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1254:CNode_1417{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1416}
#   2: @mindspore_nn_layer_basic_Dropout_construct_1254:CNode_1418{[0]: ValueNode<Primitive> Return, [1]: CNode_1417}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1248 : 000001D8B244ADE0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1248 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_1244]() {
  %1(CNode_1245) = $(mindspore_nn_layer_basic_Dropout_construct_1244):S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1248:CNode_1419{[0]: ValueNode<Primitive> Return, [1]: CNode_1245}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1249 : 000001D8B244A340
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1249() {
  %1(CNode_1420) = S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_1421) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_1422) = Switch(%2, @mindspore_nn_layer_basic_Dropout_construct_1423, @mindspore_nn_layer_basic_Dropout_construct_1424)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_1425) = %3()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1249:CNode_1420{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 1}
#   2: @mindspore_nn_layer_basic_Dropout_construct_1249:CNode_1421{[0]: ValueNode<Primitive> Cond, [1]: CNode_1420, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_1249:CNode_1422{[0]: ValueNode<Primitive> Switch, [1]: CNode_1421, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1423, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_1424}
#   4: @mindspore_nn_layer_basic_Dropout_construct_1249:CNode_1425{[0]: CNode_1422}
#   5: @mindspore_nn_layer_basic_Dropout_construct_1249:CNode_1426{[0]: ValueNode<Primitive> Return, [1]: CNode_1425}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1275 : 000001D8B244C870
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1275 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1184]() {
  %1(CNode_1273) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para254_@CNode_1273, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1427) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_1187) = $(mindspore_nn_layer_container_SequentialCell_construct_1151):MakeTuple(@mindspore_nn_layer_basic_Dropout_construct_1224, @mindspore_nn_layer_basic_Dense_construct_1237, @mindspore_nn_layer_activation_ReLU_construct_1241, @mindspore_nn_layer_basic_Dropout_construct_1244, @mindspore_nn_layer_basic_Dense_construct_1257, @mindspore_nn_layer_activation_ReLU_construct_1260, @mindspore_nn_layer_basic_Dense_construct_1263, @mindspore_nn_layer_activation_ReLU_construct_1266, @mindspore_nn_layer_basic_Dense_construct_1269)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1429) = call @ms_iter_1428(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para254_@CNode_1273)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para255_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_1430) = call @mindspore_nn_layer_container_SequentialCell_construct_1184(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_1431) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1275:CNode_1429{[0]: ValueNode<FuncGraph> ms_iter_1428, [1]: CNode_1187}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1275:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1429, [2]: param_@CNode_1273}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1275:CNode_1273{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.80, [1]: param_@CNode_1273, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1275:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_1275:CNode_1430{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1184, [1]: CNode_1273, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_1275:CNode_1432{[0]: ValueNode<Primitive> Return, [1]: CNode_1431}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1276 : 000001D8B2450830
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1276 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1184]() {
  Return(%para255_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1276:CNode_1433{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1280 : 000001D8B24423C0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1280 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1279]() {
  %1(CNode_1116) = call @mindspore_nn_layer_conv_Conv2d_construct_1434()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1280:CNode_1116{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1434}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1280:CNode_1117{[0]: ValueNode<Primitive> Return, [1]: CNode_1116}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1289 : 000001D8B2440E80
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1289 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1284]() {
  %1(CNode_1435) = getattr(%para258_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_1437) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1436(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1289:CNode_1435{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1289:x{[0]: CNode_1435, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1289:CNode_1438{[0]: ValueNode<Primitive> Return, [1]: CNode_1437}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1289:CNode_1437{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1436, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1290 : 000001D8B243DEB0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1290 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1284]() {
  %1(CNode_1439) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1436(%para258_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1290:CNode_1440{[0]: ValueNode<Primitive> Return, [1]: CNode_1439}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1290:CNode_1439{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1436, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_1296 : 000001D8B243BED0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_1296(%para277_x) {
  %1(CNode_1441) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para277_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/relu-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/relu-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_1296:CNode_1441{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_1296:CNode_1442{[0]: ValueNode<Primitive> Return, [1]: CNode_1441}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1301 : 000001D8B243AEE0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1301 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para278_input_data) {
  %1(CNode_1444) = call @mindspore_nn_layer_container_SequentialCell_construct_1443(I64(0), %para278_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1301:CNode_1445{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_1446}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1301:CNode_1444{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1443, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1301:CNode_1447{[0]: ValueNode<Primitive> Return, [1]: CNode_1444}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1300 : 000001D8B243B980
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1300 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para279_x) {
  %1(CNode_1449) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448(%para279_x, %para11_features.3.bn3.gamma, %para12_features.3.bn3.beta, %para178_features.3.bn3.moving_mean, %para179_features.3.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.bn3.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.bn3.beta>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.bn3.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1300:CNode_1449{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448, [1]: param_x, [2]: param_features.3.bn3.gamma, [3]: param_features.3.bn3.beta, [4]: param_features.3.bn3.moving_mean, [5]: param_features.3.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1300:CNode_1450{[0]: ValueNode<Primitive> Return, [1]: CNode_1449}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1299 : 000001D8B2437F10
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1299 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para280_x) {
  %1(CNode_1452) = call @mindspore_nn_layer_conv_Conv2d_construct_1451()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1299:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.3.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1299:CNode_1452{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1451}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1299:CNode_1453{[0]: ValueNode<Primitive> Return, [1]: CNode_1452}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1298 : 000001D8B2439450
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1298 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para281_x) {
  %1(CNode_1455) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454(%para281_x, %para8_features.3.bn2.gamma, %para9_features.3.bn2.beta, %para186_features.3.bn2.moving_mean, %para187_features.3.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn2.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn2.beta>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn2.moving_mean>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1298:CNode_1455{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454, [1]: param_x, [2]: param_features.3.bn2.gamma, [3]: param_features.3.bn2.beta, [4]: param_features.3.bn2.moving_mean, [5]: param_features.3.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1298:CNode_1456{[0]: ValueNode<Primitive> Return, [1]: CNode_1455}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1297 : 000001D8B24389B0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1297 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para282_x) {
  %1(CNode_1458) = call @mindspore_nn_layer_conv_Conv2d_construct_1457()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1297:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.3.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1297:CNode_1458{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1457}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1297:CNode_1459{[0]: ValueNode<Primitive> Return, [1]: CNode_1458}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1295 : 000001D997C3B880
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1295 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para283_x) {
  %1(CNode_1460) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454(%para283_x, %para5_features.3.bn1.gamma, %para6_features.3.bn1.beta, %para194_features.3.bn1.moving_mean, %para195_features.3.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn1.beta>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn1.moving_mean>, <Ref[Tensor[Float32]], (32), ref_key=:features.3.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn1-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1295:CNode_1460{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454, [1]: param_x, [2]: param_features.3.bn1.gamma, [3]: param_features.3.bn1.beta, [4]: param_features.3.bn1.moving_mean, [5]: param_features.3.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1295:CNode_1461{[0]: ValueNode<Primitive> Return, [1]: CNode_1460}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1294 : 000001D997C34E40
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1294 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para284_x) {
  %1(CNode_1463) = call @mindspore_nn_layer_conv_Conv2d_construct_1462()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1294:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.3.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1294:CNode_1463{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1462}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1294:CNode_1464{[0]: ValueNode<Primitive> Return, [1]: CNode_1463}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1304 : 000001D997C3A340
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1304 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1303]() {
  %1(CNode_1466) = call @mindspore_nn_layer_conv_Conv2d_construct_1465()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/4-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/4-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1304:CNode_1466{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1465}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1304:CNode_1467{[0]: ValueNode<Primitive> Return, [1]: CNode_1466}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1315 : 000001D997C39350
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1315 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1310]() {
  %1(CNode_1468) = getattr(%para262_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_1470) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1469(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1315:CNode_1468{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1315:x{[0]: CNode_1468, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1315:CNode_1471{[0]: ValueNode<Primitive> Return, [1]: CNode_1470}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1315:CNode_1470{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1469, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1316 : 000001D997C348F0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1316 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1310]() {
  %1(CNode_1472) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1469(%para262_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1316:CNode_1473{[0]: ValueNode<Primitive> Return, [1]: CNode_1472}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1316:CNode_1472{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1469, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_1322 : 000001D997C2C420
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_1322(%para285_x) {
  %1(CNode_1474) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para285_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/relu-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/relu-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_1322:CNode_1474{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_1322:CNode_1475{[0]: ValueNode<Primitive> Return, [1]: CNode_1474}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1327 : 000001D997C31920
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1327 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para286_input_data) {
  %1(CNode_1477) = call @mindspore_nn_layer_container_SequentialCell_construct_1476(I64(0), %para286_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1327:CNode_1478{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_1479}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1327:CNode_1477{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1476, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1327:CNode_1480{[0]: ValueNode<Primitive> Return, [1]: CNode_1477}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1326 : 000001D997C33E50
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1326 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para287_x) {
  %1(CNode_1482) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481(%para287_x, %para24_features.7.bn3.gamma, %para25_features.7.bn3.beta, %para180_features.7.bn3.moving_mean, %para181_features.7.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.bn3.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.bn3.beta>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.bn3.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1326:CNode_1482{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481, [1]: param_x, [2]: param_features.7.bn3.gamma, [3]: param_features.7.bn3.beta, [4]: param_features.7.bn3.moving_mean, [5]: param_features.7.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1326:CNode_1483{[0]: ValueNode<Primitive> Return, [1]: CNode_1482}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1325 : 000001D997C323C0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1325 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para288_x) {
  %1(CNode_1485) = call @mindspore_nn_layer_conv_Conv2d_construct_1484()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1325:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.7.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1325:CNode_1485{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1484}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1325:CNode_1486{[0]: ValueNode<Primitive> Return, [1]: CNode_1485}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1324 : 000001D997C30E80
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1324 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para289_x) {
  %1(CNode_1488) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487(%para289_x, %para21_features.7.bn2.gamma, %para22_features.7.bn2.beta, %para188_features.7.bn2.moving_mean, %para189_features.7.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn2.beta>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn2.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1324:CNode_1488{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487, [1]: param_x, [2]: param_features.7.bn2.gamma, [3]: param_features.7.bn2.beta, [4]: param_features.7.bn2.moving_mean, [5]: param_features.7.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1324:CNode_1489{[0]: ValueNode<Primitive> Return, [1]: CNode_1488}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1323 : 000001D997C2FE90
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1323 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para290_x) {
  %1(CNode_1491) = call @mindspore_nn_layer_conv_Conv2d_construct_1490()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1323:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.7.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1323:CNode_1491{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1490}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1323:CNode_1492{[0]: ValueNode<Primitive> Return, [1]: CNode_1491}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1321 : 000001D997C28F00
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1321 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para291_x) {
  %1(CNode_1493) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487(%para291_x, %para18_features.7.bn1.gamma, %para19_features.7.bn1.beta, %para196_features.7.bn1.moving_mean, %para197_features.7.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn1.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn1.beta>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn1.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:features.7.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn1-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1321:CNode_1493{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487, [1]: param_x, [2]: param_features.7.bn1.gamma, [3]: param_features.7.bn1.beta, [4]: param_features.7.bn1.moving_mean, [5]: param_features.7.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1321:CNode_1494{[0]: ValueNode<Primitive> Return, [1]: CNode_1493}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1320 : 000001D997C289B0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1320 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para292_x) {
  %1(CNode_1496) = call @mindspore_nn_layer_conv_Conv2d_construct_1495()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1320:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.7.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1320:CNode_1496{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1495}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1320:CNode_1497{[0]: ValueNode<Primitive> Return, [1]: CNode_1496}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1330 : 000001D997C26F20
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1330 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1329]() {
  %1(CNode_1499) = call @mindspore_nn_layer_conv_Conv2d_construct_1498()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/8-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/8-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1330:CNode_1499{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1498}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1330:CNode_1500{[0]: ValueNode<Primitive> Return, [1]: CNode_1499}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_1339 : 000001D997C25490
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_1339(%para293_x) {
  %1(CNode_1501) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para293_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/relu-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/relu-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_1339:CNode_1501{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_1339:CNode_1502{[0]: ValueNode<Primitive> Return, [1]: CNode_1501}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1344 : 000001D997C21F70
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1344(%para294_input_data) {
  %1(CNode_1504) = call @mindspore_nn_layer_container_SequentialCell_construct_1503(I64(0), %para294_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1344:CNode_1505{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: ValueNode<ValueList> []}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1344:CNode_1504{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1503, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1344:CNode_1506{[0]: ValueNode<Primitive> Return, [1]: CNode_1504}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1343 : 000001D997C23A00
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para295_x) {
  %1(CNode_1507) = S_Prim_Shape(%para295_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_1508) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_1509) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_1510) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_1511) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_1512) = Switch(%5, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1513, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1514)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_1515) = %6()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_1516) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343:CNode_1507{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343:CNode_1508{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_1507, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343:CNode_1510{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343:CNode_1511{[0]: ValueNode<Primitive> Cond, [1]: CNode_1510, [2]: ValueNode<BoolImm> false}
#   5: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343:CNode_1512{[0]: ValueNode<Primitive> Switch, [1]: CNode_1511, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1513, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1514}
#   6: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343:CNode_1515{[0]: CNode_1512}
#   7: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343:CNode_1517{[0]: ValueNode<Primitive> Return, [1]: CNode_1516}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1342 : 000001D997C1E500
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1342 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para296_x) {
  %1(CNode_1519) = call @mindspore_nn_layer_conv_Conv2d_construct_1518()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1342:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.10.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1342:CNode_1519{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1518}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1342:CNode_1520{[0]: ValueNode<Primitive> Return, [1]: CNode_1519}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1341 : 000001D997C1FF90
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1341 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para297_x) {
  %1(CNode_1521) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448(%para297_x, %para34_features.10.bn2.gamma, %para35_features.10.bn2.beta, %para190_features.10.bn2.moving_mean, %para191_features.10.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn2.beta>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn2.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1341:CNode_1521{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448, [1]: param_x, [2]: param_features.10.bn2.gamma, [3]: param_features.10.bn2.beta, [4]: param_features.10.bn2.moving_mean, [5]: param_features.10.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1341:CNode_1522{[0]: ValueNode<Primitive> Return, [1]: CNode_1521}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1340 : 000001D997C1BFD0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1340 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para298_x) {
  %1(CNode_1524) = call @L_mindspore_nn_layer_conv_Conv2d_construct_1523(%para298_x, %para33_features.10.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:features.10.conv2.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1340:CNode_1524{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_conv_Conv2d_construct_1523, [1]: param_x, [2]: param_features.10.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1340:CNode_1525{[0]: ValueNode<Primitive> Return, [1]: CNode_1524}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1338 : 000001D997C1FA40
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1338 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para299_x) {
  %1(CNode_1526) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448(%para299_x, %para31_features.10.bn1.gamma, %para32_features.10.bn1.beta, %para198_features.10.bn1.moving_mean, %para199_features.10.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn1.beta>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn1.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:features.10.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn1-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1338:CNode_1526{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448, [1]: param_x, [2]: param_features.10.bn1.gamma, [3]: param_features.10.bn1.beta, [4]: param_features.10.bn1.moving_mean, [5]: param_features.10.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1338:CNode_1527{[0]: ValueNode<Primitive> Return, [1]: CNode_1526}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1337 : 000001D997C20A30
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1337 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para300_x) {
  %1(CNode_1529) = call @mindspore_nn_layer_conv_Conv2d_construct_1528()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1337:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.10.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1337:CNode_1529{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1528}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1337:CNode_1530{[0]: ValueNode<Primitive> Return, [1]: CNode_1529}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1347 : 000001D997C1F4F0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1347 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1346]() {
  %1(CNode_1532) = call @mindspore_nn_layer_conv_Conv2d_construct_1531()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/11-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/11-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1347:CNode_1532{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1531}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1347:CNode_1533{[0]: ValueNode<Primitive> Return, [1]: CNode_1532}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_1356 : 000001D997C1EFA0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_1356(%para301_x) {
  %1(CNode_1534) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para301_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/relu-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/relu-ReLU)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_1356:CNode_1534{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_1356:CNode_1535{[0]: ValueNode<Primitive> Return, [1]: CNode_1534}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1361 : 000001D997C18560
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1361 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para302_input_data) {
  %1(CNode_1537) = call @mindspore_nn_layer_container_SequentialCell_construct_1536(I64(0), %para302_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1361:CNode_1538{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_1539}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1361:CNode_1537{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1536, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1361:CNode_1540{[0]: ValueNode<Primitive> Return, [1]: CNode_1537}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1360 : 000001D997C1EA50
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1360 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para303_x) {
  %1(CNode_1542) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541(%para303_x, %para47_features.13.bn3.gamma, %para48_features.13.bn3.beta, %para184_features.13.bn3.moving_mean, %para185_features.13.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.bn3.gamma>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.bn3.beta>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.bn3.moving_mean>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1360:CNode_1542{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541, [1]: param_x, [2]: param_features.13.bn3.gamma, [3]: param_features.13.bn3.beta, [4]: param_features.13.bn3.moving_mean, [5]: param_features.13.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1360:CNode_1543{[0]: ValueNode<Primitive> Return, [1]: CNode_1542}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1359 : 000001D997C16030
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1359 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para304_x) {
  %1(CNode_1545) = call @mindspore_nn_layer_conv_Conv2d_construct_1544()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1359:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.13.conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1359:CNode_1545{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1544}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1359:CNode_1546{[0]: ValueNode<Primitive> Return, [1]: CNode_1545}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1358 : 000001D997C15AE0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1358 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para305_x) {
  %1(CNode_1547) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448(%para305_x, %para44_features.13.bn2.gamma, %para45_features.13.bn2.beta, %para192_features.13.bn2.moving_mean, %para193_features.13.bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn2.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn2.beta>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn2.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1358:CNode_1547{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448, [1]: param_x, [2]: param_features.13.bn2.gamma, [3]: param_features.13.bn2.beta, [4]: param_features.13.bn2.moving_mean, [5]: param_features.13.bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1358:CNode_1548{[0]: ValueNode<Primitive> Return, [1]: CNode_1547}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1357 : 000001D997C14AF0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1357 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para306_x) {
  %1(CNode_1549) = call @L_mindspore_nn_layer_conv_Conv2d_construct_1523(%para306_x, %para43_features.13.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 128, 3, 3), ref_key=:features.13.conv2.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1357:CNode_1549{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_conv_Conv2d_construct_1523, [1]: param_x, [2]: param_features.13.conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1357:CNode_1550{[0]: ValueNode<Primitive> Return, [1]: CNode_1549}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1355 : 000001D997C11080
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1355 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para307_x) {
  %1(CNode_1551) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448(%para307_x, %para41_features.13.bn1.gamma, %para42_features.13.bn1.beta, %para200_features.13.bn1.moving_mean, %para201_features.13.bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn1.beta>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn1.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:features.13.bn1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn1-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1355:CNode_1551{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448, [1]: param_x, [2]: param_features.13.bn1.gamma, [3]: param_features.13.bn1.beta, [4]: param_features.13.bn1.moving_mean, [5]: param_features.13.bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1355:CNode_1552{[0]: ValueNode<Primitive> Return, [1]: CNode_1551}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1354 : 000001D997C08BB0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1354 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para308_x) {
  %1(CNode_1554) = call @mindspore_nn_layer_conv_Conv2d_construct_1553()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1354:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.13.conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1354:CNode_1554{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1553}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1354:CNode_1555{[0]: ValueNode<Primitive> Return, [1]: CNode_1554}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1366 : 000001D8B24413D0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1366 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1189]() {
  %1(CNode_1052) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para271_@CNode_1052, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1108) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_1087) = $(mindspore_nn_layer_container_SequentialCell_construct_1150):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1279, @mindspore_nn_layer_activation_ReLU_construct_1281, @mindspore_nn_layer_pooling_MaxPool2d_construct_1284, @__main___Bottleneck_construct_1293, @mindspore_nn_layer_conv_Conv2d_construct_1303, @mindspore_nn_layer_activation_ReLU_construct_1307, @mindspore_nn_layer_pooling_MaxPool2d_construct_1310, @__main___Bottleneck_construct_1319, @mindspore_nn_layer_conv_Conv2d_construct_1329, @mindspore_nn_layer_activation_ReLU_construct_1333, @__main___Bottleneck_construct_1336, @mindspore_nn_layer_conv_Conv2d_construct_1346, @mindspore_nn_layer_activation_ReLU_construct_1350, @__main___Bottleneck_construct_1353, @mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_1363)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1110) = call @ms_iter_1428(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para271_@CNode_1052)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para272_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_1111) = call @mindspore_nn_layer_container_SequentialCell_construct_1189(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_1112) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1366:CNode_1110{[0]: ValueNode<FuncGraph> ms_iter_1428, [1]: CNode_1087}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1366:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1110, [2]: param_@CNode_1052}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1366:CNode_1052{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.80, [1]: param_@CNode_1052, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1366:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_1366:CNode_1111{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1189, [1]: CNode_1052, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_1366:CNode_1113{[0]: ValueNode<Primitive> Return, [1]: CNode_1112}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1367 : 000001D8B2447370
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1367 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1189]() {
  Return(%para272_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1367:CNode_1556{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_1369 : 000001D997C08110
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_1369 parent: [subgraph @_apply_adam_1173]() {
  %1(CNode_1558) = call @_apply_adam_1557()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:866/                if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:866/                if self.is_group_lr:/
}
# Order:
#   1: @_apply_adam_1369:CNode_1558{[0]: ValueNode<FuncGraph> _apply_adam_1557}
#   2: @_apply_adam_1369:CNode_1559{[0]: ValueNode<Primitive> Return, [1]: CNode_1558}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_1378 : 000001D997C0E600
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_1378 parent: [subgraph @scale_grad_1171]() {
  Return(%para244_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:488/        return gradients/
}
# Order:
#   1: @scale_grad_1378:CNode_1560{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_1381 : 000001D997C0BB80
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_1381 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037]() {
  Return(%para177_learning_rate)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:756/        return lr/
}
# Order:
#   1: @get_lr_1381:CNode_1561{[0]: ValueNode<Primitive> Return, [1]: param_learning_rate}


subgraph attr:
subgraph instance: shape_1387 : 000001D8B2452810
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\ops\function\array_func.py:1483/def shape(input_x):/
subgraph @shape_1387(%para309_input_x) {
  %1(CNode_1562) = S_Prim_Shape(%para309_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\ops\function\array_func.py:1509/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\ops\function\array_func.py:1509/    return shape_(input_x)/
}
# Order:
#   1: @shape_1387:CNode_1562{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @shape_1387:CNode_1563{[0]: ValueNode<Primitive> Return, [1]: CNode_1562}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1384 : 000001D8B244B880
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1384 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221]() {
  %1(CNode_1386) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221):call @shape_1387(%para228_logits)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %2(CNode_1388) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221):S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %3(CNode_1389) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221):S_Prim_getitem(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %4(labels) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1221):S_Prim_OneHot[axis: I64(-1), input_names: ["indices", "depth", "on_value", "off_value"], output_names: ["output"]](%para229_labels, %3, Tensor(shape=[], dtype=Float32, value=1), Tensor(shape=[], dtype=Float32, value=0))
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %5(CNode_1564) = S_Prim_SoftmaxCrossEntropyWithLogits(%para228_logits, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %6(x) = S_Prim_getitem(%5, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %7(CNode_1566) = call @get_loss_1565(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:786/        return self.get_loss(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:786/        return self.get_loss(x)/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1384:CNode_1564{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: labels}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1384:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1564, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1384:CNode_1566{[0]: ValueNode<FuncGraph> get_loss_1565, [1]: x}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_1384:CNode_1567{[0]: ValueNode<Primitive> Return, [1]: CNode_1566}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1392 : 000001D8B244EDA0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1392 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_1224]() {
  %1(CNode_1568) = S_Prim_Dropout[keep_prob: F32(0.5), Seed0: I64(0), Seed1: I64(0), side_effect_hidden: Bool(1)](%para245_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:194/        out, _ = self.dropout(x)/
  %2(out) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:194/        out, _ = self.dropout(x)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:195/        return out/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1392:CNode_1568{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Dropout, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Dropout_construct_1392:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1568, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Dropout_construct_1392:CNode_1569{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1399 : 000001D8B244FD90
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1399 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_1229]() {
  %1(CNode_1396) = $(mindspore_nn_layer_basic_Dropout_construct_1229):S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1399:CNode_1570{[0]: ValueNode<Primitive> Return, [1]: CNode_1396}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1400 : 000001D8B2451820
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1400() {
  %1(CNode_1571) = S_Prim_equal(F32(0.5), I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/0-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1400:CNode_1571{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_basic_Dropout_construct_1400:CNode_1572{[0]: ValueNode<Primitive> Return, [1]: CNode_1571}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_1412 : 000001D8B2441E70
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_1412 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_1238](%para310_) {
  %1(CNode_1574) = call @L_mindspore_nn_layer_basic_Dense_construct_1573()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_1412:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_phi_x, [2]: param_L_classifier.1.weight}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_1412:CNode_1574{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1573}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_1412:CNode_1575{[0]: ValueNode<Primitive> Return, [1]: CNode_1574}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_1409 : 000001D8B2441920
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_1409 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_1238]() {
  %1(CNode_1576) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %2(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_1238):S_Prim_Shape(%para274_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %3(CNode_1577) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %4(CNode_1578) = S_Prim_getitem(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %5(CNode_1579) = S_Prim_MakeTuple(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %6(x) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para274_x, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_1409:CNode_1576{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_1409:CNode_1577{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_1409:CNode_1578{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_1577}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_1409:CNode_1579{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_1576, [2]: CNode_1578}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_1409:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_x, [2]: CNode_1579}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_1409:CNode_1580{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_1410 : 000001D8B24448F0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_1410 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_1238]() {
  Return(%para274_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_1410:CNode_1581{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1416 : 000001D8B24502E0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1416 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_1244]() {
  %1(CNode_1582) = S_Prim_Dropout[keep_prob: F32(0.5), Seed0: I64(0), Seed1: I64(0), side_effect_hidden: Bool(1)](%para248_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:194/        out, _ = self.dropout(x)/
  %2(out) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:194/        out, _ = self.dropout(x)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:195/        return out/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1416:CNode_1582{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Dropout, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Dropout_construct_1416:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1582, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Dropout_construct_1416:CNode_1583{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1423 : 000001D8B244B330
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1423 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_1249]() {
  %1(CNode_1420) = $(mindspore_nn_layer_basic_Dropout_construct_1249):S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1423:CNode_1584{[0]: ValueNode<Primitive> Return, [1]: CNode_1420}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_1424 : 000001D8B244A890
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_1424() {
  %1(CNode_1585) = S_Prim_equal(F32(0.5), I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/3-Dropout)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_1424:CNode_1585{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_basic_Dropout_construct_1424:CNode_1586{[0]: ValueNode<Primitive> Return, [1]: CNode_1585}


subgraph attr:
subgraph instance: ms_iter_1428 : 000001D997C1CA70
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\_extends\parse\standard_method.py:2345/def ms_iter(xs):/
subgraph @ms_iter_1428(%para311_xs) {
  %1(CNode_1587) = getattr(%para311_xs, "__ms_iter__")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\_extends\parse\standard_method.py:2347/    return xs.__ms_iter__/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\_extends\parse\standard_method.py:2347/    return xs.__ms_iter__/
}
# Order:
#   1: @ms_iter_1428:CNode_1587{[0]: ValueNode<Primitive> getattr, [1]: param_xs, [2]: ValueNode<StringImm> __ms_iter__}
#   2: @ms_iter_1428:CNode_1588{[0]: ValueNode<Primitive> Return, [1]: CNode_1587}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1434 : 000001D8B2445E30
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1434 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1279]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1279):S_Prim_Conv2D[out_channel: I64(96), kernel_size: (I64(11), I64(11)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(2), I64(2), I64(2), I64(2)), pad_mode: I64(0), stride: (I64(1), I64(1), I64(4), I64(4)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para256_x, %para3_features.0.weight)
      : (<null>, <Ref[Tensor[Float32]], (96, 3, 11, 11), ref_key=:features.0.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1434:CNode_1118{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1436 : 000001D8B243E400
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1436(%para312_, %para313_) {
  %1(CNode_1590) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1589()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1436:CNode_1590{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1589}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1436:CNode_1591{[0]: ValueNode<Primitive> Return, [1]: CNode_1590}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1592 : 000001D8B2439EF0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1592 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para314_x) {
  %1(CNode_1594) = call @mindspore_nn_layer_conv_Conv2d_construct_1593()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1592:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.3.downsample.0.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1592:CNode_1594{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1593}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1592:CNode_1595{[0]: ValueNode<Primitive> Return, [1]: CNode_1594}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1596 : 000001D8B2438460
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1596 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para315_x) {
  %1(CNode_1597) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448(%para315_x, %para14_features.3.downsample.1.gamma, %para15_features.3.downsample.1.beta, %para202_features.3.downsample.1.moving_mean, %para203_features.3.downsample.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.downsample.1.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.downsample.1.beta>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.downsample.1.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:features.3.downsample.1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell/1-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1596:CNode_1597{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448, [1]: param_x, [2]: param_features.3.downsample.1.gamma, [3]: param_features.3.downsample.1.beta, [4]: param_features.3.downsample.1.moving_mean, [5]: param_features.3.downsample.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1596:CNode_1598{[0]: ValueNode<Primitive> Return, [1]: CNode_1597}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1443 : 000001D8B243CEC0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1443 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1301](%para316_, %para317_) {
  %1(CNode_1446) = $(mindspore_nn_layer_container_SequentialCell_construct_1301):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1592, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1596)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1445) = $(mindspore_nn_layer_container_SequentialCell_construct_1301):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_1599) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para316_@CNode_1600, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1601) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_1602, @mindspore_nn_layer_container_SequentialCell_construct_1603)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_1604) = %4()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1443:CNode_1599{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.73, [1]: param_@CNode_1600, [2]: CNode_1445}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1443:CNode_1601{[0]: ValueNode<Primitive> Switch, [1]: CNode_1599, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1602, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1603}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1443:CNode_1604{[0]: CNode_1601}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1443:CNode_1605{[0]: ValueNode<Primitive> Return, [1]: CNode_1604}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448 : 000001D997C115D0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448(%para318_x, %para319_, %para320_, %para321_, %para322_) {
  %1(CNode_1606) = S_Prim_Shape(%para318_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_1607) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_1608) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_1609) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_1610) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_1611) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1612, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1613)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_1614) = %6()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_1615) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448:CNode_1606{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448:CNode_1607{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_1606, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448:CNode_1609{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448:CNode_1610{[0]: ValueNode<Primitive> Cond, [1]: CNode_1609, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448:CNode_1611{[0]: ValueNode<Primitive> Switch, [1]: CNode_1610, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1612, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1613}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448:CNode_1614{[0]: CNode_1611}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448:CNode_1450{[0]: ValueNode<Primitive> Return, [1]: CNode_1615}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1451 : 000001D8B24399A0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1451 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1299]() {
  %1(CNode_1617) = call @mindspore_nn_layer_conv_Conv2d_construct_1616()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1451:CNode_1617{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1616}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1451:CNode_1618{[0]: ValueNode<Primitive> Return, [1]: CNode_1617}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454 : 000001D997C3C870
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454(%para323_x, %para324_, %para325_, %para326_, %para327_) {
  %1(CNode_1619) = S_Prim_Shape(%para323_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_1620) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_1621) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_1622) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_1623) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_1624) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1625, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1626)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_1627) = %6()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_1628) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454:CNode_1619{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454:CNode_1620{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_1619, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454:CNode_1622{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454:CNode_1623{[0]: ValueNode<Primitive> Cond, [1]: CNode_1622, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454:CNode_1624{[0]: ValueNode<Primitive> Switch, [1]: CNode_1623, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1625, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1626}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454:CNode_1627{[0]: CNode_1624}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454:CNode_1456{[0]: ValueNode<Primitive> Return, [1]: CNode_1628}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1457 : 000001D8B243B430
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1457 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1297]() {
  %1(CNode_1630) = call @mindspore_nn_layer_conv_Conv2d_construct_1629()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1457:CNode_1630{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1629}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1457:CNode_1631{[0]: ValueNode<Primitive> Return, [1]: CNode_1630}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1462 : 000001D997C38E00
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1462 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1294]() {
  %1(CNode_1633) = call @mindspore_nn_layer_conv_Conv2d_construct_1632()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1462:CNode_1633{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1632}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1462:CNode_1634{[0]: ValueNode<Primitive> Return, [1]: CNode_1633}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1465 : 000001D997C37E10
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1465 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1303]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1303):S_Prim_Conv2D[out_channel: I64(256), kernel_size: (I64(5), I64(5)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(2), I64(2), I64(2), I64(2)), pad_mode: I64(0), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para260_x, %para16_features.4.weight)
      : (<null>, <Ref[Tensor[Float32]], (256, 128, 5, 5), ref_key=:features.4.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/4-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/4-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1465:CNode_1635{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1469 : 000001D997C37370
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1469(%para328_, %para329_) {
  %1(CNode_1637) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1636()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1469:CNode_1637{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1636}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1469:CNode_1638{[0]: ValueNode<Primitive> Return, [1]: CNode_1637}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1639 : 000001D997C32E60
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1639 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para330_x) {
  %1(CNode_1641) = call @mindspore_nn_layer_conv_Conv2d_construct_1640()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1639:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.7.downsample.0.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1639:CNode_1641{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1640}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1639:CNode_1642{[0]: ValueNode<Primitive> Return, [1]: CNode_1641}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1643 : 000001D997C2BED0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1643 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para331_x) {
  %1(CNode_1644) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481(%para331_x, %para27_features.7.downsample.1.gamma, %para28_features.7.downsample.1.beta, %para204_features.7.downsample.1.moving_mean, %para205_features.7.downsample.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.downsample.1.gamma>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.downsample.1.beta>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.downsample.1.moving_mean>, <Ref[Tensor[Float32]], (512), ref_key=:features.7.downsample.1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell/1-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1643:CNode_1644{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481, [1]: param_x, [2]: param_features.7.downsample.1.gamma, [3]: param_features.7.downsample.1.beta, [4]: param_features.7.downsample.1.moving_mean, [5]: param_features.7.downsample.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1643:CNode_1645{[0]: ValueNode<Primitive> Return, [1]: CNode_1644}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1476 : 000001D997C2C970
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1476 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1327](%para332_, %para333_) {
  %1(CNode_1479) = $(mindspore_nn_layer_container_SequentialCell_construct_1327):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1639, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1643)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1478) = $(mindspore_nn_layer_container_SequentialCell_construct_1327):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_1646) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para332_@CNode_1647, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1648) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_1649, @mindspore_nn_layer_container_SequentialCell_construct_1650)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_1651) = %4()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1476:CNode_1646{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.73, [1]: param_@CNode_1647, [2]: CNode_1478}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1476:CNode_1648{[0]: ValueNode<Primitive> Switch, [1]: CNode_1646, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1649, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1650}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1476:CNode_1651{[0]: CNode_1648}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1476:CNode_1652{[0]: ValueNode<Primitive> Return, [1]: CNode_1651}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481 : 000001D997C303E0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481(%para334_x, %para335_, %para336_, %para337_, %para338_) {
  %1(CNode_1653) = S_Prim_Shape(%para334_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_1654) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_1655) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_1656) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_1657) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_1658) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1659, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1660)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_1661) = %6()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_1662) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481:CNode_1653{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481:CNode_1654{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_1653, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481:CNode_1656{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481:CNode_1657{[0]: ValueNode<Primitive> Cond, [1]: CNode_1656, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481:CNode_1658{[0]: ValueNode<Primitive> Switch, [1]: CNode_1657, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1659, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1660}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481:CNode_1661{[0]: CNode_1658}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481:CNode_1483{[0]: ValueNode<Primitive> Return, [1]: CNode_1662}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1484 : 000001D997C2AEE0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1484 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1325]() {
  %1(CNode_1664) = call @mindspore_nn_layer_conv_Conv2d_construct_1663()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1484:CNode_1664{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1663}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1484:CNode_1665{[0]: ValueNode<Primitive> Return, [1]: CNode_1664}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487 : 000001D997C29450
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487(%para339_x, %para340_, %para341_, %para342_, %para343_) {
  %1(CNode_1666) = S_Prim_Shape(%para339_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_1667) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_1668) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_1669) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_1670) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_1671) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1672, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1673)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_1674) = %6()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_1675) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487:CNode_1666{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487:CNode_1667{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_1666, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487:CNode_1669{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487:CNode_1670{[0]: ValueNode<Primitive> Cond, [1]: CNode_1669, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487:CNode_1671{[0]: ValueNode<Primitive> Switch, [1]: CNode_1670, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1672, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1673}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487:CNode_1674{[0]: CNode_1671}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487:CNode_1489{[0]: ValueNode<Primitive> Return, [1]: CNode_1675}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1490 : 000001D997C2EEA0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1490 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1323]() {
  %1(CNode_1677) = call @mindspore_nn_layer_conv_Conv2d_construct_1676()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1490:CNode_1677{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1676}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1490:CNode_1678{[0]: ValueNode<Primitive> Return, [1]: CNode_1677}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1495 : 000001D997C29EF0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1495 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1320]() {
  %1(CNode_1680) = call @mindspore_nn_layer_conv_Conv2d_construct_1679()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1495:CNode_1680{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1679}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1495:CNode_1681{[0]: ValueNode<Primitive> Return, [1]: CNode_1680}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1498 : 000001D997C27470
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1498 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1329]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1329):S_Prim_Conv2D[out_channel: I64(768), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(1), I64(1), I64(1), I64(1)), pad_mode: I64(0), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para264_x, %para29_features.8.weight)
      : (<null>, <Ref[Tensor[Float32]], (768, 512, 3, 3), ref_key=:features.8.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/8-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/8-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1498:CNode_1682{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1503 : 000001D997C224C0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1503 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1344](%para344_, %para345_) {
  %1(CNode_1505) = $(mindspore_nn_layer_container_SequentialCell_construct_1344):S_Prim_inner_len([])
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1683) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para344_@CNode_1684, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_1685) = Switch(%2, @mindspore_nn_layer_container_SequentialCell_construct_1686, @mindspore_nn_layer_container_SequentialCell_construct_1687)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1688) = %3()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1503:CNode_1683{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.73, [1]: param_@CNode_1684, [2]: CNode_1505}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1503:CNode_1685{[0]: ValueNode<Primitive> Switch, [1]: CNode_1683, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1686, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1687}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1503:CNode_1688{[0]: CNode_1685}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1503:CNode_1689{[0]: ValueNode<Primitive> Return, [1]: CNode_1688}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1513 : 000001D997C26480
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1513 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343]() {
  %1(CNode_1691) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1690()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1513:CNode_1691{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1690}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1513:CNode_1692{[0]: ValueNode<Primitive> Return, [1]: CNode_1691}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1514 : 000001D997C23F50
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1514 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343]() {
  %1(CNode_1694) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1693()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1514:CNode_1694{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1693}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1514:CNode_1695{[0]: ValueNode<Primitive> Return, [1]: CNode_1694}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1518 : 000001D997C21A20
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1518 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1342]() {
  %1(CNode_1697) = call @mindspore_nn_layer_conv_Conv2d_construct_1696()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1518:CNode_1697{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1696}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1518:CNode_1698{[0]: ValueNode<Primitive> Return, [1]: CNode_1697}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_conv_Conv2d_construct_1523 : 000001D997C19AA0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_conv_Conv2d_construct_1523(%para346_x, %para347_) {
  %1(CNode_1700) = call @L_mindspore_nn_layer_conv_Conv2d_construct_1699()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @L_mindspore_nn_layer_conv_Conv2d_construct_1523:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_L_features.10.conv2.weight}
#   2: @L_mindspore_nn_layer_conv_Conv2d_construct_1523:CNode_1700{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_conv_Conv2d_construct_1699}
#   3: @L_mindspore_nn_layer_conv_Conv2d_construct_1523:CNode_1525{[0]: ValueNode<Primitive> Return, [1]: CNode_1700}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1528 : 000001D997C1DFB0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1528 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1337]() {
  %1(CNode_1702) = call @mindspore_nn_layer_conv_Conv2d_construct_1701()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1528:CNode_1702{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1701}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1528:CNode_1703{[0]: ValueNode<Primitive> Return, [1]: CNode_1702}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1531 : 000001D997C1D510
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1531 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1346]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1346):S_Prim_Conv2D[out_channel: I64(1024), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(1), I64(1), I64(1), I64(1)), pad_mode: I64(0), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para267_x, %para39_features.11.weight)
      : (<null>, <Ref[Tensor[Float32]], (1024, 768, 3, 3), ref_key=:features.11.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/11-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/11-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1531:CNode_1704{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1705 : 000001D997C1AFE0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1705 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para348_x) {
  %1(CNode_1707) = call @mindspore_nn_layer_conv_Conv2d_construct_1706()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1705:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.13.downsample.0.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1705:CNode_1707{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1706}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_1705:CNode_1708{[0]: ValueNode<Primitive> Return, [1]: CNode_1707}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1709 : 000001D997C15040
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1709 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_1037](%para349_x) {
  %1(CNode_1710) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541(%para349_x, %para50_features.13.downsample.1.gamma, %para51_features.13.downsample.1.beta, %para206_features.13.downsample.1.moving_mean, %para207_features.13.downsample.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.downsample.1.gamma>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.downsample.1.beta>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.downsample.1.moving_mean>, <Ref[Tensor[Float32]], (1280), ref_key=:features.13.downsample.1.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell/1-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1709:CNode_1710{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541, [1]: param_x, [2]: param_features.13.downsample.1.gamma, [3]: param_features.13.downsample.1.beta, [4]: param_features.13.downsample.1.moving_mean, [5]: param_features.13.downsample.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1709:CNode_1711{[0]: ValueNode<Primitive> Return, [1]: CNode_1710}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1536 : 000001D997C1BA80
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1536 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1361](%para350_, %para351_) {
  %1(CNode_1539) = $(mindspore_nn_layer_container_SequentialCell_construct_1361):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1705, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1709)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1538) = $(mindspore_nn_layer_container_SequentialCell_construct_1361):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_1712) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para350_@CNode_1713, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1714) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_1715, @mindspore_nn_layer_container_SequentialCell_construct_1716)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_1717) = %4()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1536:CNode_1712{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.73, [1]: param_@CNode_1713, [2]: CNode_1538}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1536:CNode_1714{[0]: ValueNode<Primitive> Switch, [1]: CNode_1712, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1715, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1716}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1536:CNode_1717{[0]: CNode_1714}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1536:CNode_1718{[0]: ValueNode<Primitive> Return, [1]: CNode_1717}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541 : 000001D997C16580
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541(%para352_x, %para353_, %para354_, %para355_, %para356_) {
  %1(CNode_1719) = S_Prim_Shape(%para352_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_1720) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_1721) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_1722) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_1723) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_1724) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1725, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1726)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_1727) = %6()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_1728) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541:CNode_1719{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541:CNode_1720{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_1719, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541:CNode_1722{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541:CNode_1723{[0]: ValueNode<Primitive> Cond, [1]: CNode_1722, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541:CNode_1724{[0]: ValueNode<Primitive> Switch, [1]: CNode_1723, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1725, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1726}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541:CNode_1727{[0]: CNode_1724}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541:CNode_1543{[0]: ValueNode<Primitive> Return, [1]: CNode_1728}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1544 : 000001D997C19FF0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1544 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1359]() {
  %1(CNode_1730) = call @mindspore_nn_layer_conv_Conv2d_construct_1729()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1544:CNode_1730{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1729}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1544:CNode_1731{[0]: ValueNode<Primitive> Return, [1]: CNode_1730}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1553 : 000001D997C10B30
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1553 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1354]() {
  %1(CNode_1733) = call @mindspore_nn_layer_conv_Conv2d_construct_1732()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1553:CNode_1733{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1732}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1553:CNode_1734{[0]: ValueNode<Primitive> Return, [1]: CNode_1733}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_1557 : 000001D997C06BD0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_1557 parent: [subgraph @_apply_adam_1173]() {
  %1(CNode_1736) = call @_apply_adam_1735()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:887/                    if self.use_lazy:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:887/                    if self.use_lazy:/
}
# Order:
#   1: @_apply_adam_1557:CNode_1736{[0]: ValueNode<FuncGraph> _apply_adam_1735}
#   2: @_apply_adam_1557:CNode_1737{[0]: ValueNode<Primitive> Return, [1]: CNode_1736}


subgraph attr:
training : 1
subgraph instance: get_loss_1565 : 000001D8B2452D60
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_1565(%para357_x, %para358_weights) {
  %1(CNode_1739) = call @get_loss_1738()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:143/        if self.reduce and self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:143/        if self.reduce and self.average:/
}
# Order:
#   1: @get_loss_1565:input_dtype{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> dtype}
#   2: @get_loss_1565:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_x, [2]: ValueNode<Float> Float32}
#   3: @get_loss_1565:weights{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_weights, [2]: ValueNode<Float> Float32}
#   4: @get_loss_1565:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Mul, [1]: weights, [2]: x}
#   5: @get_loss_1565:CNode_1739{[0]: ValueNode<FuncGraph> get_loss_1738}
#   6: @get_loss_1565:CNode_1740{[0]: ValueNode<Primitive> Return, [1]: CNode_1739}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_1573 : 000001D8B24468D0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_1573 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_1412]() {
  %1(CNode_1742) = call @L_mindspore_nn_layer_basic_Dense_construct_1741()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_1573:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_L_classifier.1.bias}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_1573:CNode_1742{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1741}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_1573:CNode_1743{[0]: ValueNode<Primitive> Return, [1]: CNode_1742}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1589 : 000001D8B24369D0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1589 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1436]() {
  %1(CNode_1745) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1744()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1589:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_phi_x}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1589:CNode_1745{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1744}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1589:CNode_1746{[0]: ValueNode<Primitive> Return, [1]: CNode_1745}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1593 : 000001D8B24379C0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1593 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1592]() {
  %1(CNode_1748) = call @mindspore_nn_layer_conv_Conv2d_construct_1747()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1593:CNode_1748{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1747}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1593:CNode_1749{[0]: ValueNode<Primitive> Return, [1]: CNode_1748}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1602 : 000001D8B243D410
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1602 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1443]() {
  %1(CNode_1600) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para316_@CNode_1600, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1750) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_1446) = $(mindspore_nn_layer_container_SequentialCell_construct_1301):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1592, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1596)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1751) = call @ms_iter_1428(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para316_@CNode_1600)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para317_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_1752) = call @mindspore_nn_layer_container_SequentialCell_construct_1443(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_1753) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1602:CNode_1751{[0]: ValueNode<FuncGraph> ms_iter_1428, [1]: CNode_1446}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1602:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1751, [2]: param_@CNode_1600}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1602:CNode_1600{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.80, [1]: param_@CNode_1600, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1602:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_1602:CNode_1752{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1443, [1]: CNode_1600, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_1602:CNode_1754{[0]: ValueNode<Primitive> Return, [1]: CNode_1753}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1603 : 000001D8B243A990
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1603 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1443]() {
  Return(%para317_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1603:CNode_1755{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1612 : 000001D997C17570
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1612 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448]() {
  %1(CNode_1757) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1756()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1612:CNode_1757{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1756}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1612:CNode_1758{[0]: ValueNode<Primitive> Return, [1]: CNode_1757}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1613 : 000001D997C12070
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1613 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448]() {
  %1(CNode_1760) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1759()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1613:CNode_1760{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1759}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1613:CNode_1761{[0]: ValueNode<Primitive> Return, [1]: CNode_1760}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1616 : 000001D8B2436480
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1616 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1299]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1299):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para280_x, %para10_features.3.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 32, 1, 1), ref_key=:features.3.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1616:CNode_1762{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1625 : 000001D8B243A440
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1625 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454]() {
  %1(CNode_1764) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1763()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1625:CNode_1764{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1763}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1625:CNode_1765{[0]: ValueNode<Primitive> Return, [1]: CNode_1764}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1626 : 000001D997C35390
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1626 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454]() {
  %1(CNode_1767) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1766()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1626:CNode_1767{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1766}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1626:CNode_1768{[0]: ValueNode<Primitive> Return, [1]: CNode_1767}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1629 : 000001D8B2438F00
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1629 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1297]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1297):S_Prim_Conv2D[out_channel: I64(32), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(1), I64(1), I64(1), I64(1)), pad_mode: I64(0), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para282_x, %para7_features.3.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (32, 32, 3, 3), ref_key=:features.3.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1629:CNode_1769{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1632 : 000001D997C3BDD0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1632 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1294]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1294):S_Prim_Conv2D[out_channel: I64(32), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para284_x, %para4_features.3.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (32, 96, 1, 1), ref_key=:features.3.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1632:CNode_1770{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1636 : 000001D997C368D0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1636 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1469]() {
  %1(CNode_1772) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1771()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1636:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_phi_x}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1636:CNode_1772{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1771}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1636:CNode_1773{[0]: ValueNode<Primitive> Return, [1]: CNode_1772}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1640 : 000001D997C333B0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1640 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1639]() {
  %1(CNode_1775) = call @mindspore_nn_layer_conv_Conv2d_construct_1774()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1640:CNode_1775{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1774}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1640:CNode_1776{[0]: ValueNode<Primitive> Return, [1]: CNode_1775}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1649 : 000001D997C33900
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1649 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1476]() {
  %1(CNode_1647) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para332_@CNode_1647, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1777) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_1479) = $(mindspore_nn_layer_container_SequentialCell_construct_1327):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1639, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1643)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1778) = call @ms_iter_1428(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para332_@CNode_1647)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para333_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_1779) = call @mindspore_nn_layer_container_SequentialCell_construct_1476(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_1780) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1649:CNode_1778{[0]: ValueNode<FuncGraph> ms_iter_1428, [1]: CNode_1479}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1649:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1778, [2]: param_@CNode_1647}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1649:CNode_1647{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.80, [1]: param_@CNode_1647, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1649:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_1649:CNode_1779{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1476, [1]: CNode_1647, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_1649:CNode_1781{[0]: ValueNode<Primitive> Return, [1]: CNode_1780}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1650 : 000001D997C2B980
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1650 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1476]() {
  Return(%para333_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1650:CNode_1782{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1659 : 000001D997C2A990
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1659 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481]() {
  %1(CNode_1784) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1783()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1659:CNode_1784{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1783}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1659:CNode_1785{[0]: ValueNode<Primitive> Return, [1]: CNode_1784}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1660 : 000001D997C313D0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1660 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481]() {
  %1(CNode_1787) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1786()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1660:CNode_1787{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1786}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1660:CNode_1788{[0]: ValueNode<Primitive> Return, [1]: CNode_1787}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1663 : 000001D997C2F940
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1663 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1325]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1325):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para288_x, %para23_features.7.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 64, 1, 1), ref_key=:features.7.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1663:CNode_1789{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1672 : 000001D997C30930
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1672 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487]() {
  %1(CNode_1791) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1790()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1672:CNode_1791{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1790}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1672:CNode_1792{[0]: ValueNode<Primitive> Return, [1]: CNode_1791}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1673 : 000001D997C2DEB0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1673 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487]() {
  %1(CNode_1794) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1793()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1673:CNode_1794{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1793}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1673:CNode_1795{[0]: ValueNode<Primitive> Return, [1]: CNode_1794}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1676 : 000001D997C2E950
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1676 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1323]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1323):S_Prim_Conv2D[out_channel: I64(64), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(1), I64(1), I64(1), I64(1)), pad_mode: I64(0), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para290_x, %para20_features.7.conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:features.7.conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1676:CNode_1796{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1679 : 000001D997C279C0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1679 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1320]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1320):S_Prim_Conv2D[out_channel: I64(64), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para292_x, %para17_features.7.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (64, 256, 1, 1), ref_key=:features.7.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1679:CNode_1797{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1686 : 000001D997C234B0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1686 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1503]() {
  %1(CNode_1684) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para344_@CNode_1684, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1798) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_1799) = call @ms_iter_1428([])
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(cell) = S_Prim_getitem(%3, %para344_@CNode_1684)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(input_data) = %4(%para345_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %6(CNode_1800) = call @mindspore_nn_layer_container_SequentialCell_construct_1503(%1, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %7(CNode_1801) = Depend[side_effect_propagate: I64(1)](%6, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1686:CNode_1799{[0]: ValueNode<FuncGraph> ms_iter_1428, [1]: ValueNode<ValueList> []}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1686:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1799, [2]: param_@CNode_1684}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1686:CNode_1684{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.80, [1]: param_@CNode_1684, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1686:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_1686:CNode_1800{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1503, [1]: CNode_1684, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_1686:CNode_1802{[0]: ValueNode<Primitive> Return, [1]: CNode_1801}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1687 : 000001D997C22A10
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1687 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1503]() {
  Return(%para345_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1687:CNode_1803{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1690 : 000001D997C24F40
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1690 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343]() {
  %1(CNode_1804) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para295_x, %para37_features.10.bn3.gamma, %para38_features.10.bn3.beta, %para182_features.10.bn3.moving_mean, %para183_features.10.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.gamma>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.beta>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.moving_mean>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1805) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1690:CNode_1804{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_features.10.bn3.gamma, [3]: param_features.10.bn3.beta, [4]: param_features.10.bn3.moving_mean, [5]: param_features.10.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1690:CNode_1805{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1804, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1690:CNode_1806{[0]: ValueNode<Primitive> Return, [1]: CNode_1805}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1693 : 000001D997C1C520
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1693 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343]() {
  %1(CNode_1807) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1808) = Switch(%1, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1809, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1810)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1811) = %2()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1693:CNode_1807{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1693:CNode_1808{[0]: ValueNode<Primitive> Switch, [1]: CNode_1807, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1809, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1810}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1693:CNode_1811{[0]: CNode_1808}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1693:CNode_1812{[0]: ValueNode<Primitive> Return, [1]: CNode_1811}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1696 : 000001D997C204E0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1696 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1342]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1342):S_Prim_Conv2D[out_channel: I64(768), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para296_x, %para36_features.10.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (768, 128, 1, 1), ref_key=:features.10.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1696:CNode_1813{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_conv_Conv2d_construct_1699 : 000001D997C18010
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_conv_Conv2d_construct_1699 parent: [subgraph @L_mindspore_nn_layer_conv_Conv2d_construct_1523]() {
  %1(CNode_1815) = call @L_mindspore_nn_layer_conv_Conv2d_construct_1814()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @L_mindspore_nn_layer_conv_Conv2d_construct_1699:CNode_1815{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_conv_Conv2d_construct_1814}
#   2: @L_mindspore_nn_layer_conv_Conv2d_construct_1699:CNode_1816{[0]: ValueNode<Primitive> Return, [1]: CNode_1815}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1701 : 000001D997C214D0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1701 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1337]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1337):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para300_x, %para30_features.10.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 768, 1, 1), ref_key=:features.10.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1701:CNode_1817{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1706 : 000001D997C15590
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1706 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1705]() {
  %1(CNode_1819) = call @mindspore_nn_layer_conv_Conv2d_construct_1818()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1706:CNode_1819{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_1818}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_1706:CNode_1820{[0]: ValueNode<Primitive> Return, [1]: CNode_1819}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1715 : 000001D997C20F80
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1715 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1536]() {
  %1(CNode_1713) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para350_@CNode_1713, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_1821) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_1539) = $(mindspore_nn_layer_container_SequentialCell_construct_1361):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_1705, @mindspore_nn_layer_normalization_BatchNorm2d_construct_1709)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_1822) = call @ms_iter_1428(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para350_@CNode_1713)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para351_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_1823) = call @mindspore_nn_layer_container_SequentialCell_construct_1536(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_1824) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1715:CNode_1822{[0]: ValueNode<FuncGraph> ms_iter_1428, [1]: CNode_1539}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_1715:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1822, [2]: param_@CNode_1713}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_1715:CNode_1713{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.80, [1]: param_@CNode_1713, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_1715:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_1715:CNode_1823{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_1536, [1]: CNode_1713, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_1715:CNode_1825{[0]: ValueNode<Primitive> Return, [1]: CNode_1824}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_1716 : 000001D997C14050
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_1716 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_1536]() {
  Return(%para351_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_1716:CNode_1826{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1725 : 000001D997C12B10
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1725 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541]() {
  %1(CNode_1828) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1827()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1725:CNode_1828{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1827}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1725:CNode_1829{[0]: ValueNode<Primitive> Return, [1]: CNode_1828}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1726 : 000001D997C18AB0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1726 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541]() {
  %1(CNode_1831) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1830()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1726:CNode_1831{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1830}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1726:CNode_1832{[0]: ValueNode<Primitive> Return, [1]: CNode_1831}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1729 : 000001D997C17AC0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1729 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1359]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1359):S_Prim_Conv2D[out_channel: I64(1280), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para304_x, %para46_features.13.conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (1280, 128, 1, 1), ref_key=:features.13.conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv3-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1729:CNode_1833{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1732 : 000001D997C11B20
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1732 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1354]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1354):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para308_x, %para40_features.13.conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 1024, 1, 1), ref_key=:features.13.conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/conv1-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1732:CNode_1834{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_1735 : 000001D997C09BA0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_1735 parent: [subgraph @_apply_adam_1173]() {
  %1(CNode_1836) = call @_apply_adam_1835()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:894/                        if self.use_amsgrad:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:894/                        if self.use_amsgrad:/
}
# Order:
#   1: @_apply_adam_1735:CNode_1836{[0]: ValueNode<FuncGraph> _apply_adam_1835}
#   2: @_apply_adam_1735:CNode_1837{[0]: ValueNode<Primitive> Return, [1]: CNode_1836}


subgraph attr:
training : 1
subgraph instance: get_loss_1738 : 000001D8B2453800
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_1738 parent: [subgraph @get_loss_1565]() {
  %1(CNode_1839) = call @get_loss_1838()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
}
# Order:
#   1: @get_loss_1738:CNode_1840{[0]: ValueNode<FuncGraph> get_axis_1841, [1]: x}
#   2: @get_loss_1738:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReduceMean, [1]: x, [2]: CNode_1840}
#   3: @get_loss_1738:CNode_1839{[0]: ValueNode<FuncGraph> get_loss_1838}
#   4: @get_loss_1738:CNode_1842{[0]: ValueNode<Primitive> Return, [1]: CNode_1839}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_1741 : 000001D8B2442E60
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_1741 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_1573]() {
  %1(CNode_1844) = call @L_mindspore_nn_layer_basic_Dense_construct_1843()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_1741:CNode_1844{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1843}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_1741:CNode_1845{[0]: ValueNode<Primitive> Return, [1]: CNode_1844}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1744 : 000001D8B243E950
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1744 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1589]() {
  %1(CNode_1846) = Cond(%para313_phi_expand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %2(CNode_1847) = Switch(%1, @mindspore_nn_layer_pooling_MaxPool2d_construct_1848, @mindspore_nn_layer_pooling_MaxPool2d_construct_1849)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %3(CNode_1850) = %2()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %4(CNode_1852) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1851(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1744:CNode_1846{[0]: ValueNode<Primitive> Cond, [1]: param_phi_expand_batch, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1744:CNode_1847{[0]: ValueNode<Primitive> Switch, [1]: CNode_1846, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1848, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1849}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1744:CNode_1850{[0]: CNode_1847}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1744:CNode_1852{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1851, [1]: CNode_1850}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_1744:CNode_1853{[0]: ValueNode<Primitive> Return, [1]: CNode_1852}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1747 : 000001D8B243C970
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1747 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1592]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1592):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para314_x, %para13_features.3.downsample.0.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 96, 1, 1), ref_key=:features.3.downsample.0.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1747:CNode_1854{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1756 : 000001D997C17020
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1756 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448]() {
  %1(CNode_1855) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para318_x, %para319_L_features.3.bn3.gamma, %para320_L_features.3.bn3.beta, %para321_L_features.3.bn3.moving_mean, %para322_L_features.3.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1856) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1756:CNode_1855{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.3.bn3.gamma, [3]: param_L_features.3.bn3.beta, [4]: param_L_features.3.bn3.moving_mean, [5]: param_L_features.3.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1756:CNode_1856{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1855, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1756:CNode_1857{[0]: ValueNode<Primitive> Return, [1]: CNode_1856}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1759 : 000001D997C125C0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1759 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448]() {
  %1(CNode_1858) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1859) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1860, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1861)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1862) = %2()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1759:CNode_1858{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1759:CNode_1859{[0]: ValueNode<Primitive> Switch, [1]: CNode_1858, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1860, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1861}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1759:CNode_1862{[0]: CNode_1859}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1759:CNode_1863{[0]: ValueNode<Primitive> Return, [1]: CNode_1862}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1763 : 000001D8B243D960
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1763 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454]() {
  %1(CNode_1864) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para323_x, %para324_L_features.3.bn2.gamma, %para325_L_features.3.bn2.beta, %para326_L_features.3.bn2.moving_mean, %para327_L_features.3.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1865) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1763:CNode_1864{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.3.bn2.gamma, [3]: param_L_features.3.bn2.beta, [4]: param_L_features.3.bn2.moving_mean, [5]: param_L_features.3.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1763:CNode_1865{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1864, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1763:CNode_1866{[0]: ValueNode<Primitive> Return, [1]: CNode_1865}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1766 : 000001D997C36380
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1766 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454]() {
  %1(CNode_1867) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1868) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1869, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1870)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1871) = %2()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1766:CNode_1867{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1766:CNode_1868{[0]: ValueNode<Primitive> Switch, [1]: CNode_1867, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1869, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1870}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1766:CNode_1871{[0]: CNode_1868}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1766:CNode_1872{[0]: ValueNode<Primitive> Return, [1]: CNode_1871}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1771 : 000001D997C36E20
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1771 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1636]() {
  %1(CNode_1873) = Cond(%para329_phi_expand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %2(CNode_1874) = Switch(%1, @mindspore_nn_layer_pooling_MaxPool2d_construct_1875, @mindspore_nn_layer_pooling_MaxPool2d_construct_1876)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %3(CNode_1877) = %2()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %4(CNode_1879) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1878(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1771:CNode_1873{[0]: ValueNode<Primitive> Cond, [1]: param_phi_expand_batch, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1771:CNode_1874{[0]: ValueNode<Primitive> Switch, [1]: CNode_1873, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1875, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1876}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1771:CNode_1877{[0]: CNode_1874}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1771:CNode_1879{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1878, [1]: CNode_1877}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_1771:CNode_1880{[0]: ValueNode<Primitive> Return, [1]: CNode_1879}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1774 : 000001D997C2CEC0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1774 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1639]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1639):S_Prim_Conv2D[out_channel: I64(512), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para330_x, %para26_features.7.downsample.0.weight)
      : (<null>, <Ref[Tensor[Float32]], (512, 256, 1, 1), ref_key=:features.7.downsample.0.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1774:CNode_1881{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1783 : 000001D997C32910
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1783 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481]() {
  %1(CNode_1882) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para334_x, %para335_L_features.7.bn3.gamma, %para336_L_features.7.bn3.beta, %para337_L_features.7.bn3.moving_mean, %para338_L_features.7.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1883) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1783:CNode_1882{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.7.bn3.gamma, [3]: param_L_features.7.bn3.beta, [4]: param_L_features.7.bn3.moving_mean, [5]: param_L_features.7.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1783:CNode_1883{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1882, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1783:CNode_1884{[0]: ValueNode<Primitive> Return, [1]: CNode_1883}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1786 : 000001D997C2E400
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1786 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481]() {
  %1(CNode_1885) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1886) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1887, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1888)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1889) = %2()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1786:CNode_1885{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1786:CNode_1886{[0]: ValueNode<Primitive> Switch, [1]: CNode_1885, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1887, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1888}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1786:CNode_1889{[0]: CNode_1886}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1786:CNode_1890{[0]: ValueNode<Primitive> Return, [1]: CNode_1889}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1790 : 000001D997C299A0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1790 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487]() {
  %1(CNode_1891) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para339_x, %para340_L_features.7.bn2.gamma, %para341_L_features.7.bn2.beta, %para342_L_features.7.bn2.moving_mean, %para343_L_features.7.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1892) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1790:CNode_1891{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.7.bn2.gamma, [3]: param_L_features.7.bn2.beta, [4]: param_L_features.7.bn2.moving_mean, [5]: param_L_features.7.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1790:CNode_1892{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1891, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1790:CNode_1893{[0]: ValueNode<Primitive> Return, [1]: CNode_1892}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1793 : 000001D997C28460
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1793 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487]() {
  %1(CNode_1894) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1895) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1896, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1897)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1898) = %2()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1793:CNode_1894{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1793:CNode_1895{[0]: ValueNode<Primitive> Switch, [1]: CNode_1894, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1896, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1897}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1793:CNode_1898{[0]: CNode_1895}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1793:CNode_1899{[0]: ValueNode<Primitive> Return, [1]: CNode_1898}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1809 : 000001D997C249F0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1809 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343]() {
  %1(CNode_1900) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para295_x, %para37_features.10.bn3.gamma, %para38_features.10.bn3.beta, %para182_features.10.bn3.moving_mean, %para183_features.10.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.gamma>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.beta>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.moving_mean>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1901) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1809:CNode_1900{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_features.10.bn3.gamma, [3]: param_features.10.bn3.beta, [4]: param_features.10.bn3.moving_mean, [5]: param_features.10.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1809:CNode_1901{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1900, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1809:CNode_1902{[0]: ValueNode<Primitive> Return, [1]: CNode_1901}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1810 : 000001D997C244A0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1810 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343]() {
  %1(CNode_1904) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_1903()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1810:CNode_1904{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_1903}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1810:CNode_1905{[0]: ValueNode<Primitive> Return, [1]: CNode_1904}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_conv_Conv2d_construct_1814 : 000001D997C13B00
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_conv_Conv2d_construct_1814 parent: [subgraph @L_mindspore_nn_layer_conv_Conv2d_construct_1523]() {
  %1(output) = $(L_mindspore_nn_layer_conv_Conv2d_construct_1523):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(1), I64(1), I64(1), I64(1)), pad_mode: I64(0), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para346_x, %para347_L_features.10.conv2.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/conv2-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @L_mindspore_nn_layer_conv_Conv2d_construct_1814:CNode_1906{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_1818 : 000001D997C1B530
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_1818 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_1705]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_1705):S_Prim_Conv2D[out_channel: I64(1280), kernel_size: (I64(1), I64(1)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1)](%para348_x, %para49_features.13.downsample.0.weight)
      : (<null>, <Ref[Tensor[Float32]], (1280, 1024, 1, 1), ref_key=:features.13.downsample.0.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/downsample-SequentialCell/0-Conv2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_1818:CNode_1907{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1827 : 000001D997C1AA90
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1827 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541]() {
  %1(CNode_1908) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para352_x, %para353_L_features.13.bn3.gamma, %para354_L_features.13.bn3.beta, %para355_L_features.13.bn3.moving_mean, %para356_L_features.13.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_1909) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1827:CNode_1908{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.13.bn3.gamma, [3]: param_L_features.13.bn3.beta, [4]: param_L_features.13.bn3.moving_mean, [5]: param_L_features.13.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1827:CNode_1909{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1908, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1827:CNode_1910{[0]: ValueNode<Primitive> Return, [1]: CNode_1909}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1830 : 000001D997C19000
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1830 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541]() {
  %1(CNode_1911) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_1912) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1913, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1914)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_1915) = %2()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1830:CNode_1911{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1830:CNode_1912{[0]: ValueNode<Primitive> Switch, [1]: CNode_1911, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1913, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1914}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1830:CNode_1915{[0]: CNode_1912}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1830:CNode_1916{[0]: ValueNode<Primitive> Return, [1]: CNode_1915}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_1835 : 000001D997C0EB50
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_1835 parent: [subgraph @_apply_adam_1173]() {
  %1(CNode_1918) = call @_apply_adam_1917()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
}
# Order:
#   1: @_apply_adam_1835:CNode_1919{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_adam_opt, [2]: ValueNode<DoSignaturePrimitive> S_Prim_Adam, [3]: ValueNode<DoSignaturePrimitive> S_Prim_FusedSparseAdam, [4]: ValueNode<DoSignaturePrimitive> S_Prim_Push, [5]: ValueNode<DoSignaturePrimitive> S_Prim_Pull, [6]: ValueNode<BoolImm> false, [7]: ValueNode<BoolImm> false, [8]: ValueNode<BoolImm> true, [9]: param_beta1_power, [10]: param_beta2_power, [11]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.9), [12]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.999), [13]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1e-08), [14]: param_lr}
#   2: @_apply_adam_1835:success{[0]: ValueNode<DoSignaturePrimitive> S_Prim_map, [1]: CNode_1919, [2]: param_gradients, [3]: param_params, [4]: param_moment1, [5]: param_moment2, [6]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false), [7]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false)}
#   3: @_apply_adam_1835:CNode_1918{[0]: ValueNode<FuncGraph> _apply_adam_1917}
#   4: @_apply_adam_1835:CNode_1920{[0]: ValueNode<Primitive> Return, [1]: CNode_1918}


subgraph attr:
training : 1
subgraph instance: get_axis_1841 : 000001D8B2453D50
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:113/    def get_axis(self, x):/
subgraph @get_axis_1841(%para359_x) {
  %1(shape) = call @shape_1387(%para359_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:120/        shape = F.shape(x)/
  %2(length) = S_Prim_sequence_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:121/        length = F.tuple_len(shape)/
  %3(perm) = S_Prim_make_range(I64(0), %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:122/        perm = F.make_range(0, length)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:123/        return perm/
}
# Order:
#   1: @get_axis_1841:shape{[0]: ValueNode<FuncGraph> shape_1387, [1]: param_x}
#   2: @get_axis_1841:length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_sequence_len, [1]: shape}
#   3: @get_axis_1841:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: length}
#   4: @get_axis_1841:CNode_1921{[0]: ValueNode<Primitive> Return, [1]: perm}


subgraph attr:
training : 1
subgraph instance: get_loss_1838 : 000001D8B244C320
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_1838 parent: [subgraph @get_loss_1738]() {
  %1(CNode_1923) = call @get_loss_1922()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @get_loss_1838:CNode_1923{[0]: ValueNode<FuncGraph> get_loss_1922}
#   2: @get_loss_1838:CNode_1924{[0]: ValueNode<Primitive> Return, [1]: CNode_1923}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_1843 : 000001D8B24433B0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_1843 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_1573]() {
  %1(CNode_1926) = call @L_mindspore_nn_layer_basic_Dense_construct_1925()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_1843:CNode_1926{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1925}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_1843:CNode_1927{[0]: ValueNode<Primitive> Return, [1]: CNode_1926}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1851 : 000001D8B243F3F0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1851(%para360_) {
  %1(CNode_1929) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1928()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1851:CNode_1929{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1928}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1851:CNode_1930{[0]: ValueNode<Primitive> Return, [1]: CNode_1929}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1848 : 000001D8B243EEA0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1848 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1589]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_1589):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(3), I64(3)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para312_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_1931) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_1932) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_1933) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_1934, @mindspore_nn_layer_pooling_MaxPool2d_construct_1935)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_1936) = %4()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_1938) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1937(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1848:CNode_1931{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1848:CNode_1932{[0]: ValueNode<Primitive> Cond, [1]: CNode_1931, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1848:CNode_1933{[0]: ValueNode<Primitive> Switch, [1]: CNode_1932, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1934, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1935}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1848:CNode_1936{[0]: CNode_1933}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_1848:CNode_1938{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1937, [1]: CNode_1936}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_1848:CNode_1939{[0]: ValueNode<Primitive> Return, [1]: CNode_1938}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1849 : 000001D8B2436F20
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1849 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1589]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_1589):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(3), I64(3)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para312_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1849:CNode_1940{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1860 : 000001D997C13060
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1860 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448]() {
  %1(CNode_1941) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para318_x, %para319_L_features.3.bn3.gamma, %para320_L_features.3.bn3.beta, %para321_L_features.3.bn3.moving_mean, %para322_L_features.3.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1942) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1860:CNode_1941{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.3.bn3.gamma, [3]: param_L_features.3.bn3.beta, [4]: param_L_features.3.bn3.moving_mean, [5]: param_L_features.3.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1860:CNode_1942{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1941, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1860:CNode_1943{[0]: ValueNode<Primitive> Return, [1]: CNode_1942}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1861 : 000001D997C135B0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1861 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448]() {
  %1(CNode_1945) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1944()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1861:CNode_1945{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1944}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1861:CNode_1946{[0]: ValueNode<Primitive> Return, [1]: CNode_1945}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1869 : 000001D997C3CDC0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1869 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454]() {
  %1(CNode_1947) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para323_x, %para324_L_features.3.bn2.gamma, %para325_L_features.3.bn2.beta, %para326_L_features.3.bn2.moving_mean, %para327_L_features.3.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1948) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1869:CNode_1947{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.3.bn2.gamma, [3]: param_L_features.3.bn2.beta, [4]: param_L_features.3.bn2.moving_mean, [5]: param_L_features.3.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1869:CNode_1948{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1947, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1869:CNode_1949{[0]: ValueNode<Primitive> Return, [1]: CNode_1948}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1870 : 000001D997C3D310
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1870 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454]() {
  %1(CNode_1951) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1950()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1870:CNode_1951{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1950}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1870:CNode_1952{[0]: ValueNode<Primitive> Return, [1]: CNode_1951}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1878 : 000001D997C3B330
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1878(%para361_) {
  %1(CNode_1954) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1953()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1878:CNode_1954{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1953}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1878:CNode_1955{[0]: ValueNode<Primitive> Return, [1]: CNode_1954}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1875 : 000001D997C343A0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1875 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1636]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_1636):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(3), I64(3)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para328_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_1956) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_1957) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_1958) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_1959, @mindspore_nn_layer_pooling_MaxPool2d_construct_1960)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_1961) = %4()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_1963) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_1962(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1875:CNode_1956{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1875:CNode_1957{[0]: ValueNode<Primitive> Cond, [1]: CNode_1956, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1875:CNode_1958{[0]: ValueNode<Primitive> Switch, [1]: CNode_1957, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1959, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1960}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1875:CNode_1961{[0]: CNode_1958}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_1875:CNode_1963{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_1962, [1]: CNode_1961}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_1875:CNode_1964{[0]: ValueNode<Primitive> Return, [1]: CNode_1963}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1876 : 000001D997C38360
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1876 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1636]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_1636):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(3), I64(3)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para328_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1876:CNode_1965{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1887 : 000001D997C2D410
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1887 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481]() {
  %1(CNode_1966) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para334_x, %para335_L_features.7.bn3.gamma, %para336_L_features.7.bn3.beta, %para337_L_features.7.bn3.moving_mean, %para338_L_features.7.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1967) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1887:CNode_1966{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.7.bn3.gamma, [3]: param_L_features.7.bn3.beta, [4]: param_L_features.7.bn3.moving_mean, [5]: param_L_features.7.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1887:CNode_1967{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1966, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1887:CNode_1968{[0]: ValueNode<Primitive> Return, [1]: CNode_1967}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1888 : 000001D997C2B430
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1888 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481]() {
  %1(CNode_1970) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1969()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1888:CNode_1970{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1969}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1888:CNode_1971{[0]: ValueNode<Primitive> Return, [1]: CNode_1970}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1896 : 000001D997C2A440
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1896 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487]() {
  %1(CNode_1972) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para339_x, %para340_L_features.7.bn2.gamma, %para341_L_features.7.bn2.beta, %para342_L_features.7.bn2.moving_mean, %para343_L_features.7.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1973) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1896:CNode_1972{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.7.bn2.gamma, [3]: param_L_features.7.bn2.beta, [4]: param_L_features.7.bn2.moving_mean, [5]: param_L_features.7.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1896:CNode_1973{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1972, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1896:CNode_1974{[0]: ValueNode<Primitive> Return, [1]: CNode_1973}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1897 : 000001D997C2F3F0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1897 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487]() {
  %1(CNode_1976) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1975()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1897:CNode_1976{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1975}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1897:CNode_1977{[0]: ValueNode<Primitive> Return, [1]: CNode_1976}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_1903 : 000001D997C25F30
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1903 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_1343]() {
  %1(CNode_1978) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para295_x, %para37_features.10.bn3.gamma, %para38_features.10.bn3.beta, %para182_features.10.bn3.moving_mean, %para183_features.10.bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.gamma>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.beta>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.moving_mean>, <Ref[Tensor[Float32]], (768), ref_key=:features.10.bn3.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_1979) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/10-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1903:CNode_1978{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_features.10.bn3.gamma, [3]: param_features.10.bn3.beta, [4]: param_features.10.bn3.moving_mean, [5]: param_features.10.bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1903:CNode_1979{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1978, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_1903:CNode_1980{[0]: ValueNode<Primitive> Return, [1]: CNode_1979}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1913 : 000001D997C1A540
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1913 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541]() {
  %1(CNode_1981) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para352_x, %para353_L_features.13.bn3.gamma, %para354_L_features.13.bn3.beta, %para355_L_features.13.bn3.moving_mean, %para356_L_features.13.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_1982) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1913:CNode_1981{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.13.bn3.gamma, [3]: param_L_features.13.bn3.beta, [4]: param_L_features.13.bn3.moving_mean, [5]: param_L_features.13.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1913:CNode_1982{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_1981, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1913:CNode_1983{[0]: ValueNode<Primitive> Return, [1]: CNode_1982}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1914 : 000001D997C19550
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1914 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541]() {
  %1(CNode_1985) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1984()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1914:CNode_1985{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1984}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1914:CNode_1986{[0]: ValueNode<Primitive> Return, [1]: CNode_1985}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_1917 : 000001D997C0F5F0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_1917 parent: [subgraph @_apply_adam_1835]() {
  %1(CNode_1988) = call @_apply_adam_1987()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:894/                        if self.use_amsgrad:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:894/                        if self.use_amsgrad:/
}
# Order:
#   1: @_apply_adam_1917:CNode_1988{[0]: ValueNode<FuncGraph> _apply_adam_1987}
#   2: @_apply_adam_1917:CNode_1989{[0]: ValueNode<Primitive> Return, [1]: CNode_1988}


subgraph attr:
training : 1
subgraph instance: get_loss_1922 : 000001D8B2454D40
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_1922 parent: [subgraph @get_loss_1738]() {
  %1(CNode_1991) = call @get_loss_1990()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @get_loss_1922:CNode_1991{[0]: ValueNode<FuncGraph> get_loss_1990}
#   2: @get_loss_1922:CNode_1992{[0]: ValueNode<Primitive> Return, [1]: CNode_1991}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_1925 : 000001D8B2446E20
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_1925 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_1573]() {
  %1(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_1238):S_Prim_Shape(%para274_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_1993) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_1994) = S_Prim_not_equal(%2, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_1995) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_1996) = Switch(%4, @L_mindspore_nn_layer_basic_Dense_construct_1997, @L_mindspore_nn_layer_basic_Dense_construct_1998)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %6(CNode_1999) = %5()
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_2001) = call @L_mindspore_nn_layer_basic_Dense_construct_2000(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_1925:CNode_1993{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_1925:CNode_1994{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_1993, [2]: ValueNode<Int64Imm> 2}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_1925:CNode_1995{[0]: ValueNode<Primitive> Cond, [1]: CNode_1994, [2]: ValueNode<BoolImm> false}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_1925:CNode_1996{[0]: ValueNode<Primitive> Switch, [1]: CNode_1995, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1997, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_1998}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_1925:CNode_1999{[0]: CNode_1996}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_1925:CNode_2001{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_2000, [1]: CNode_1999}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_1925:CNode_2002{[0]: ValueNode<Primitive> Return, [1]: CNode_2001}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1928 : 000001D8B2445390
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1928 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1851]() {
  %1(CNode_2004) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_2003()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1928:CNode_2004{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_2003}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1928:CNode_2005{[0]: ValueNode<Primitive> Return, [1]: CNode_2004}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1937 : 000001D8B2440930
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1937(%para362_) {
  Return(%para362_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1937:CNode_2006{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1934 : 000001D8B24443A0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1934 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1589]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_1589):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(3), I64(3)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para312_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_2007) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_2008) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_2009) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_2010) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_2011) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_2012) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1934:CNode_2007{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1934:CNode_2008{[0]: ValueNode<Primitive> getattr, [1]: CNode_2007, [2]: ValueNode<StringImm> squeeze}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1934:CNode_2009{[0]: CNode_2008, [1]: ValueNode<Int64Imm> 0}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1934:CNode_2010{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_1934:CNode_2011{[0]: ValueNode<Primitive> getattr, [1]: CNode_2010, [2]: ValueNode<StringImm> squeeze}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_1934:CNode_2012{[0]: CNode_2011, [1]: ValueNode<Int64Imm> 0}
#   7: @mindspore_nn_layer_pooling_MaxPool2d_construct_1934:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_2009, [2]: CNode_2012}
#   8: @mindspore_nn_layer_pooling_MaxPool2d_construct_1934:CNode_2013{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1935 : 000001D8B2437470
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1935 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1589]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_1589):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(3), I64(3)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para312_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_2014) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1935:CNode_2014{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1935:out{[0]: CNode_2014, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1935:CNode_2015{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1944 : 000001D997C16AD0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1944 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1448]() {
  %1(CNode_2016) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para318_x, %para319_L_features.3.bn3.gamma, %para320_L_features.3.bn3.beta, %para321_L_features.3.bn3.moving_mean, %para322_L_features.3.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_2017) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1944:CNode_2016{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.3.bn3.gamma, [3]: param_L_features.3.bn3.beta, [4]: param_L_features.3.bn3.moving_mean, [5]: param_L_features.3.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1944:CNode_2017{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_2016, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1944:CNode_2018{[0]: ValueNode<Primitive> Return, [1]: CNode_2017}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1950 : 000001D997C3D860
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1950 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1454]() {
  %1(CNode_2019) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para323_x, %para324_L_features.3.bn2.gamma, %para325_L_features.3.bn2.beta, %para326_L_features.3.bn2.moving_mean, %para327_L_features.3.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_2020) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/3-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1950:CNode_2019{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.3.bn2.gamma, [3]: param_L_features.3.bn2.beta, [4]: param_L_features.3.bn2.moving_mean, [5]: param_L_features.3.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1950:CNode_2020{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_2019, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1950:CNode_2021{[0]: ValueNode<Primitive> Return, [1]: CNode_2020}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1953 : 000001D997C39DF0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1953 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1878]() {
  %1(CNode_2023) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_2022()
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1953:CNode_2023{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_2022}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1953:CNode_2024{[0]: ValueNode<Primitive> Return, [1]: CNode_2023}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1962 : 000001D997C3C320
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1962(%para363_) {
  Return(%para363_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1962:CNode_2025{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1959 : 000001D997C378C0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1959 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1636]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_1636):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(3), I64(3)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para328_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_2026) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_2027) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_2028) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_2029) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_2030) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_2031) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1959:CNode_2026{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1959:CNode_2027{[0]: ValueNode<Primitive> getattr, [1]: CNode_2026, [2]: ValueNode<StringImm> squeeze}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1959:CNode_2028{[0]: CNode_2027, [1]: ValueNode<Int64Imm> 0}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_1959:CNode_2029{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_1959:CNode_2030{[0]: ValueNode<Primitive> getattr, [1]: CNode_2029, [2]: ValueNode<StringImm> squeeze}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_1959:CNode_2031{[0]: CNode_2030, [1]: ValueNode<Int64Imm> 0}
#   7: @mindspore_nn_layer_pooling_MaxPool2d_construct_1959:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_2028, [2]: CNode_2031}
#   8: @mindspore_nn_layer_pooling_MaxPool2d_construct_1959:CNode_2032{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_1960 : 000001D997C3ADE0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1960 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1636]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_1636):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(3), I64(3)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para328_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_2033) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_1960:CNode_2033{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_1960:out{[0]: CNode_2033, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_1960:CNode_2034{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1969 : 000001D997C31E70
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1969 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1481]() {
  %1(CNode_2035) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para334_x, %para335_L_features.7.bn3.gamma, %para336_L_features.7.bn3.beta, %para337_L_features.7.bn3.moving_mean, %para338_L_features.7.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_2036) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1969:CNode_2035{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.7.bn3.gamma, [3]: param_L_features.7.bn3.beta, [4]: param_L_features.7.bn3.moving_mean, [5]: param_L_features.7.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1969:CNode_2036{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_2035, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1969:CNode_2037{[0]: ValueNode<Primitive> Return, [1]: CNode_2036}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1975 : 000001D997C27F10
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1975 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1487]() {
  %1(CNode_2038) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para339_x, %para340_L_features.7.bn2.gamma, %para341_L_features.7.bn2.beta, %para342_L_features.7.bn2.moving_mean, %para343_L_features.7.bn2.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_2039) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/7-Bottleneck/bn2-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1975:CNode_2038{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.7.bn2.gamma, [3]: param_L_features.7.bn2.beta, [4]: param_L_features.7.bn2.moving_mean, [5]: param_L_features.7.bn2.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1975:CNode_2039{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_2038, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1975:CNode_2040{[0]: ValueNode<Primitive> Return, [1]: CNode_2039}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1984 : 000001D997C145A0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1984 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1541]() {
  %1(CNode_2041) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para352_x, %para353_L_features.13.bn3.gamma, %para354_L_features.13.bn3.beta, %para355_L_features.13.bn3.moving_mean, %para356_L_features.13.bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_2042) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/13-Bottleneck/bn3-BatchNorm2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1984:CNode_2041{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_features.13.bn3.gamma, [3]: param_L_features.13.bn3.beta, [4]: param_L_features.13.bn3.moving_mean, [5]: param_L_features.13.bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1984:CNode_2042{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_2041, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_1984:CNode_2043{[0]: ValueNode<Primitive> Return, [1]: CNode_2042}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_1987 : 000001D997C0D0C0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_1987 parent: [subgraph @_apply_adam_1835]() {
  %1(CNode_2045) = call @_apply_adam_2044()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:887/                    if self.use_lazy:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:887/                    if self.use_lazy:/
}
# Order:
#   1: @_apply_adam_1987:CNode_2045{[0]: ValueNode<FuncGraph> _apply_adam_2044}
#   2: @_apply_adam_1987:CNode_2046{[0]: ValueNode<Primitive> Return, [1]: CNode_2045}


subgraph attr:
training : 1
subgraph instance: get_loss_1990 : 000001D8B24542A0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_1990 parent: [subgraph @get_loss_1738]() {
  %1(weights) = $(get_loss_1565):S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%para358_weights, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:141/        weights = self.cast(weights, mstype.float32)/
  %2(x) = $(get_loss_1565):S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%para357_x, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:140/        x = self.cast(x, mstype.float32)/
  %3(x) = $(get_loss_1565):S_Prim_Mul[input_names: ["x", "y"], output_names: ["output"]](%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:142/        x = self.mul(weights, x)/
  %4(CNode_1840) = $(get_loss_1738):call @get_axis_1841(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %5(x) = $(get_loss_1738):S_Prim_ReduceMean[keep_dims: Bool(0), input_names: ["input_x", "axis"], output_names: ["y"]](%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %6(input_dtype) = $(get_loss_1565):getattr(%para357_x, "dtype")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:139/        input_dtype = x.dtype/
  %7(x) = S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:147/        x = self.cast(x, input_dtype)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\loss\loss.py:148/        return x/
}
# Order:
#   1: @get_loss_1990:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: x, [2]: input_dtype}
#   2: @get_loss_1990:CNode_2047{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_2000 : 000001D8B2447E10
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_2000(%para364_) {
  Return(%para364_phi_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:635/        return x/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_2000:CNode_2048{[0]: ValueNode<Primitive> Return, [1]: param_phi_x}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_1997 : 000001D8B24478C0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_1997 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_1573]() {
  %1(x) = $(L_mindspore_nn_layer_basic_Dense_construct_1412):S_Prim_MatMul[transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_b: Bool(1), output_names: ["output"], transpose_x1: Bool(0), transpose_x2: Bool(1)](%para310_phi_x, %para276_L_classifier.1.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_mindspore_nn_layer_basic_Dense_construct_1573):S_Prim_BiasAdd[format: "NCHW", input_names: ["x", "b"], output_names: ["output"]](%1, %para275_L_classifier.1.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  %3(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_1238):S_Prim_Shape(%para274_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %4(CNode_2049) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %5(CNode_2050) = S_Prim_make_slice(None, %4, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %6(CNode_2051) = S_Prim_getitem(%3, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %7(CNode_2053) = call @L_shape_2052(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %8(CNode_2054) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %9(CNode_2055) = S_Prim_getitem(%7, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %10(CNode_2056) = S_Prim_MakeTuple(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %11(out_shape) = S_Prim_add(%6, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %12(x) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%2, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:634/            x = self.reshape(x, out_shape)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_1997:CNode_2049{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_1997:CNode_2050{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: CNode_2049, [3]: ValueNode<None> None}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_1997:CNode_2051{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_2050}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_1997:CNode_2053{[0]: ValueNode<FuncGraph> L_shape_2052, [1]: x}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_1997:CNode_2054{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_1997:CNode_2055{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_2053, [2]: CNode_2054}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_1997:CNode_2056{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_2055}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_1997:out_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_2051, [2]: CNode_2056}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_1997:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: x, [2]: out_shape}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_1997:CNode_2057{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_1998 : 000001D8B243F940
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_1998 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_1573]() {
  %1(x) = $(L_mindspore_nn_layer_basic_Dense_construct_1412):S_Prim_MatMul[transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_b: Bool(1), output_names: ["output"], transpose_x1: Bool(0), transpose_x2: Bool(1)](%para310_phi_x, %para276_L_classifier.1.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_mindspore_nn_layer_basic_Dense_construct_1573):S_Prim_BiasAdd[format: "NCHW", input_names: ["x", "b"], output_names: ["output"]](%1, %para275_L_classifier.1.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/classifier-SequentialCell/1-Dense)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_1998:CNode_2058{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_2003 : 000001D8B243FE90
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_2003 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1851]() {
  Return(%para360_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/2-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:589/        return out/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_2003:CNode_2059{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_2022 : 000001D997C3A890
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_2022 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_1878]() {
  Return(%para361_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-Net/features-SequentialCell/6-MaxPool2d)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\layer\pooling.py:589/        return out/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_2022:CNode_2060{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_2044 : 000001D997C0C620
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_2044 parent: [subgraph @_apply_adam_1835]() {
  %1(CNode_2062) = call @_apply_adam_2061()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:866/                if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:866/                if self.is_group_lr:/
}
# Order:
#   1: @_apply_adam_2044:CNode_2062{[0]: ValueNode<FuncGraph> _apply_adam_2061}
#   2: @_apply_adam_2044:CNode_2063{[0]: ValueNode<Primitive> Return, [1]: CNode_2062}


subgraph attr:
subgraph instance: L_shape_2052 : 000001D8B2448360
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\ops\function\array_func.py:1483/def shape(input_x):/
subgraph @L_shape_2052(%para365_input_x) {
  %1(CNode_1562) = S_Prim_Shape(%para365_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\ops\function\array_func.py:1509/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\ops\function\array_func.py:1509/    return shape_(input_x)/
}
# Order:
#   1: @L_shape_2052:CNode_1562{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @L_shape_2052:CNode_1563{[0]: ValueNode<Primitive> Return, [1]: CNode_1562}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_2061 : 000001D997C0AB90
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_2061 parent: [subgraph @_apply_adam_1835]() {
  %1(CNode_2065) = call @_apply_adam_2064()
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:826/            if self.use_dist_optimizer:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:826/            if self.use_dist_optimizer:/
}
# Order:
#   1: @_apply_adam_2061:CNode_2065{[0]: ValueNode<FuncGraph> _apply_adam_2064}
#   2: @_apply_adam_2061:CNode_2066{[0]: ValueNode<Primitive> Return, [1]: CNode_2065}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_2064 : 000001D997C0F0A0
# In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_2064 parent: [subgraph @_apply_adam_1835]() {
  %1(CNode_1919) = $(_apply_adam_1835):S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_adam_opt, S_Prim_Adam[use_locking: Bool(0), use_nesterov: Bool(0), side_effect_mem: Bool(1)], S_Prim_FusedSparseAdam[use_locking: Bool(0), use_nesterov: Bool(0), input_names: ["var", "m", "v", "beta1_power", "beta2_power", "lr", "beta1", "beta2", "epsilon", "grad", "indices"], output_names: ["var", "m", "v"], side_effect_mem: Bool(1), primitive_target: "CPU"], S_Prim_Push[optim_type: "Adam", only_shape_indices: [I64(0), I64(1), I64(2)], output_names: ["key"], side_effect_hidden: Bool(1), primitive_target: "CPU", input_names: ["optim_inputs", "optim_input_shapes"], use_nesterov: Bool(0)], S_Prim_Pull[primitive_target: "CPU", input_names: ["key", "weight"], output_names: ["output"]], Bool(0), Bool(0), Bool(1), %para237_beta1_power, %para238_beta2_power, Tensor(shape=[], dtype=Float32, value=0.9), Tensor(shape=[], dtype=Float32, value=0.999), Tensor(shape=[], dtype=Float32, value=1e-08), %para241_lr)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
  %2(success) = $(_apply_adam_1835):S_Prim_map(%1, %para242_gradients, %para236_params, %para239_moment1, %para240_moment2, (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)), (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)))
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
  Return(%2)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file C:\Users\EC319\.conda\envs\mindspore220_py39\lib\site-packages\mindspore\nn\optim\adam.py:907/        return success/
}
# Order:
#   1: @_apply_adam_2064:CNode_2067{[0]: ValueNode<Primitive> Return, [1]: success}


