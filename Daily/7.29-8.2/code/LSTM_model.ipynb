{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import jieba"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:38:49.215386700Z",
     "start_time": "2024-07-30T08:38:49.196356900Z"
    }
   },
   "id": "d3b6665d2ec45d9a",
   "execution_count": 152
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LSTM_model( nn.Module ):\n",
    "    def __init__( self, input_size, hidden_size, num_layers, output_size ):\n",
    "        super( LSTM_model, self ).__init__()\n",
    "        self.lstm = nn.LSTM( input_size, hidden_size, num_layers, batch_first = True )\n",
    "        self.fc = nn.Linear( hidden_size, output_size )\n",
    "        \n",
    "    def forward( self, x ):\n",
    "        output, _ = self.lstm( x )\n",
    "        output = self.fc( output[ :, -1, : ] )                              # 取序列的最后一个输出\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:38:49.215912700Z",
     "start_time": "2024-07-30T08:38:49.203830400Z"
    }
   },
   "id": "initial_id",
   "execution_count": 153
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_tsv( path ):\n",
    "    data = pd.read_csv( path, sep = '\\t' )\n",
    "    data_x = data.iloc[ :, -1 ]\n",
    "    data_y = data.iloc[ :, 1 ]\n",
    "    return data_x, data_y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:38:49.240778Z",
     "start_time": "2024-07-30T08:38:49.212200500Z"
    }
   },
   "id": "e38f1494638c9222",
   "execution_count": 154
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_txt( path ):\n",
    "    with open( path, 'r', encoding = 'utf-8' ) as f:\n",
    "        data = [ [ line.strip() ] for line in f.readlines() ]\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:38:49.241779400Z",
     "start_time": "2024-07-30T08:38:49.220480400Z"
    }
   },
   "id": "41ac713bd6217389",
   "execution_count": 155
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_x = load_txt( '../data/train.txt' )\n",
    "test_x = load_txt( '../data/test.txt' )\n",
    "train = train_x + test_x\n",
    "x_all = []\n",
    "for x in train:\n",
    "    for i in x:\n",
    "        x_all.append( i )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:38:49.313317100Z",
     "start_time": "2024-07-30T08:38:49.247785400Z"
    }
   },
   "id": "e1d700081425e096",
   "execution_count": 156
  },
  {
   "cell_type": "markdown",
   "source": [
    "文本转向量"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dc14a0059c35ba6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "_, train_y = load_tsv( '../data/train.tsv' )\n",
    "_, test_y = load_tsv( '../data/test.tsv' )\n",
    "word2vec_model = Word2Vec( sentences = x_all, vector_size = 100, window = 5, min_count = 1, workers = 4 )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:38:53.135647800Z",
     "start_time": "2024-07-30T08:38:49.315321Z"
    }
   },
   "id": "b751ca19cac59c14",
   "execution_count": 157
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def text_to_vector( text ):\n",
    "    vector = []\n",
    "    for word in text:\n",
    "        if word in word2vec_model.wv:\n",
    "            vector.append( word2vec_model.wv[ word ] )\n",
    "    \n",
    "    if vector != []:\n",
    "        return np.mean( vector, axis = 0 )\n",
    "    else:\n",
    "        return np.zeros( word2vec_model.vector_size )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:38:53.144191500Z",
     "start_time": "2024-07-30T08:38:53.138035200Z"
    }
   },
   "id": "38966fc95ef51197",
   "execution_count": 158
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 将训练集、测试集都转成词向量\n",
    "x_train_w2v = []\n",
    "x_test_w2v = []\n",
    "\n",
    "for line in train_x:\n",
    "    for text in line:\n",
    "        x_train_w2v.append( [text_to_vector( text )] )\n",
    "\n",
    "for line in test_x:\n",
    "    for text in line:\n",
    "        x_test_w2v.append( [text_to_vector( text )] )\n",
    "        \n",
    "# 词向量转成pytorch张量\n",
    "x_train_tensor = torch.Tensor( np.array( x_train_w2v, dtype = np.float32 ) )\n",
    "x_test_tensor = torch.Tensor( np.array( x_test_w2v, dtype = np.float32 ) )\n",
    "\n",
    "# 打包成dataset\n",
    "train_dataset = TensorDataset( x_train_tensor, torch.LongTensor( train_y ) )\n",
    "test_dataset = TensorDataset( x_test_tensor, torch.LongTensor( test_y ) )\n",
    "\n",
    "# 转入dataloader用于训练\n",
    "train_dataloader = DataLoader( train_dataset, batch_size = 32, shuffle = True )\n",
    "test_dataloader = DataLoader( test_dataset, batch_size = 32, shuffle = True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:38:56.823304400Z",
     "start_time": "2024-07-30T08:38:53.169171300Z"
    }
   },
   "id": "8e3a7ea66661445a",
   "execution_count": 159
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [0/1772], Loss: 0.6947\n",
      "Epoch [1/10], Batch [100/1772], Loss: 0.6585\n",
      "Epoch [1/10], Batch [200/1772], Loss: 0.4965\n",
      "Epoch [1/10], Batch [300/1772], Loss: 0.3257\n",
      "Epoch [1/10], Batch [400/1772], Loss: 0.2842\n",
      "Epoch [1/10], Batch [500/1772], Loss: 0.1308\n",
      "Epoch [1/10], Batch [600/1772], Loss: 0.3537\n",
      "Epoch [1/10], Batch [700/1772], Loss: 0.2484\n",
      "Epoch [1/10], Batch [800/1772], Loss: 0.3437\n",
      "Epoch [1/10], Batch [900/1772], Loss: 0.2028\n",
      "Epoch [1/10], Batch [1000/1772], Loss: 0.1780\n",
      "Epoch [1/10], Batch [1100/1772], Loss: 0.2822\n",
      "Epoch [1/10], Batch [1200/1772], Loss: 0.3636\n",
      "Epoch [1/10], Batch [1300/1772], Loss: 0.3043\n",
      "Epoch [1/10], Batch [1400/1772], Loss: 0.1185\n",
      "Epoch [1/10], Batch [1500/1772], Loss: 0.0941\n",
      "Epoch [1/10], Batch [1600/1772], Loss: 0.2111\n",
      "Epoch [1/10], Batch [1700/1772], Loss: 0.2013\n",
      "Epoch [2/10], Batch [0/1772], Loss: 0.2447\n",
      "Epoch [2/10], Batch [100/1772], Loss: 0.3135\n",
      "Epoch [2/10], Batch [200/1772], Loss: 0.2884\n",
      "Epoch [2/10], Batch [300/1772], Loss: 0.4718\n",
      "Epoch [2/10], Batch [400/1772], Loss: 0.3310\n",
      "Epoch [2/10], Batch [500/1772], Loss: 0.2060\n",
      "Epoch [2/10], Batch [600/1772], Loss: 0.1044\n",
      "Epoch [2/10], Batch [700/1772], Loss: 0.1212\n",
      "Epoch [2/10], Batch [800/1772], Loss: 0.1632\n",
      "Epoch [2/10], Batch [900/1772], Loss: 0.2158\n",
      "Epoch [2/10], Batch [1000/1772], Loss: 0.2592\n",
      "Epoch [2/10], Batch [1100/1772], Loss: 0.2485\n",
      "Epoch [2/10], Batch [1200/1772], Loss: 0.2233\n",
      "Epoch [2/10], Batch [1300/1772], Loss: 0.1379\n",
      "Epoch [2/10], Batch [1400/1772], Loss: 0.2026\n",
      "Epoch [2/10], Batch [1500/1772], Loss: 0.1602\n",
      "Epoch [2/10], Batch [1600/1772], Loss: 0.2706\n",
      "Epoch [2/10], Batch [1700/1772], Loss: 0.4167\n",
      "Epoch [3/10], Batch [0/1772], Loss: 0.1531\n",
      "Epoch [3/10], Batch [100/1772], Loss: 0.1447\n",
      "Epoch [3/10], Batch [200/1772], Loss: 0.1143\n",
      "Epoch [3/10], Batch [300/1772], Loss: 0.3384\n",
      "Epoch [3/10], Batch [400/1772], Loss: 0.2588\n",
      "Epoch [3/10], Batch [500/1772], Loss: 0.1474\n",
      "Epoch [3/10], Batch [600/1772], Loss: 0.2565\n",
      "Epoch [3/10], Batch [700/1772], Loss: 0.4581\n",
      "Epoch [3/10], Batch [800/1772], Loss: 0.1278\n",
      "Epoch [3/10], Batch [900/1772], Loss: 0.3925\n",
      "Epoch [3/10], Batch [1000/1772], Loss: 0.4166\n",
      "Epoch [3/10], Batch [1100/1772], Loss: 0.2124\n",
      "Epoch [3/10], Batch [1200/1772], Loss: 0.4056\n",
      "Epoch [3/10], Batch [1300/1772], Loss: 0.2233\n",
      "Epoch [3/10], Batch [1400/1772], Loss: 0.4119\n",
      "Epoch [3/10], Batch [1500/1772], Loss: 0.1610\n",
      "Epoch [3/10], Batch [1600/1772], Loss: 0.2616\n",
      "Epoch [3/10], Batch [1700/1772], Loss: 0.1417\n",
      "Epoch [4/10], Batch [0/1772], Loss: 0.1132\n",
      "Epoch [4/10], Batch [100/1772], Loss: 0.1331\n",
      "Epoch [4/10], Batch [200/1772], Loss: 0.2378\n",
      "Epoch [4/10], Batch [300/1772], Loss: 0.3019\n",
      "Epoch [4/10], Batch [400/1772], Loss: 0.2603\n",
      "Epoch [4/10], Batch [500/1772], Loss: 0.1554\n",
      "Epoch [4/10], Batch [600/1772], Loss: 0.2395\n",
      "Epoch [4/10], Batch [700/1772], Loss: 0.2356\n",
      "Epoch [4/10], Batch [800/1772], Loss: 0.2436\n",
      "Epoch [4/10], Batch [900/1772], Loss: 0.2873\n",
      "Epoch [4/10], Batch [1000/1772], Loss: 0.3927\n",
      "Epoch [4/10], Batch [1100/1772], Loss: 0.2871\n",
      "Epoch [4/10], Batch [1200/1772], Loss: 0.3761\n",
      "Epoch [4/10], Batch [1300/1772], Loss: 0.1322\n",
      "Epoch [4/10], Batch [1400/1772], Loss: 0.3588\n",
      "Epoch [4/10], Batch [1500/1772], Loss: 0.3524\n",
      "Epoch [4/10], Batch [1600/1772], Loss: 0.2391\n",
      "Epoch [4/10], Batch [1700/1772], Loss: 0.1448\n",
      "Epoch [5/10], Batch [0/1772], Loss: 0.3211\n",
      "Epoch [5/10], Batch [100/1772], Loss: 0.3652\n",
      "Epoch [5/10], Batch [200/1772], Loss: 0.1808\n",
      "Epoch [5/10], Batch [300/1772], Loss: 0.3022\n",
      "Epoch [5/10], Batch [400/1772], Loss: 0.3806\n",
      "Epoch [5/10], Batch [500/1772], Loss: 0.2544\n",
      "Epoch [5/10], Batch [600/1772], Loss: 0.2552\n",
      "Epoch [5/10], Batch [700/1772], Loss: 0.1138\n",
      "Epoch [5/10], Batch [800/1772], Loss: 0.4578\n",
      "Epoch [5/10], Batch [900/1772], Loss: 0.2389\n",
      "Epoch [5/10], Batch [1000/1772], Loss: 0.1677\n",
      "Epoch [5/10], Batch [1100/1772], Loss: 0.3171\n",
      "Epoch [5/10], Batch [1200/1772], Loss: 0.3907\n",
      "Epoch [5/10], Batch [1300/1772], Loss: 0.2116\n",
      "Epoch [5/10], Batch [1400/1772], Loss: 0.4789\n",
      "Epoch [5/10], Batch [1500/1772], Loss: 0.1966\n",
      "Epoch [5/10], Batch [1600/1772], Loss: 0.0774\n",
      "Epoch [5/10], Batch [1700/1772], Loss: 0.2306\n",
      "Epoch [6/10], Batch [0/1772], Loss: 0.2529\n",
      "Epoch [6/10], Batch [100/1772], Loss: 0.2678\n",
      "Epoch [6/10], Batch [200/1772], Loss: 0.6703\n",
      "Epoch [6/10], Batch [300/1772], Loss: 0.2274\n",
      "Epoch [6/10], Batch [400/1772], Loss: 0.2051\n",
      "Epoch [6/10], Batch [500/1772], Loss: 0.3338\n",
      "Epoch [6/10], Batch [600/1772], Loss: 0.4244\n",
      "Epoch [6/10], Batch [700/1772], Loss: 0.2116\n",
      "Epoch [6/10], Batch [800/1772], Loss: 0.1439\n",
      "Epoch [6/10], Batch [900/1772], Loss: 0.2619\n",
      "Epoch [6/10], Batch [1000/1772], Loss: 0.3223\n",
      "Epoch [6/10], Batch [1100/1772], Loss: 0.1295\n",
      "Epoch [6/10], Batch [1200/1772], Loss: 0.3893\n",
      "Epoch [6/10], Batch [1300/1772], Loss: 0.3328\n",
      "Epoch [6/10], Batch [1400/1772], Loss: 0.1114\n",
      "Epoch [6/10], Batch [1500/1772], Loss: 0.2775\n",
      "Epoch [6/10], Batch [1600/1772], Loss: 0.1823\n",
      "Epoch [6/10], Batch [1700/1772], Loss: 0.2134\n",
      "Epoch [7/10], Batch [0/1772], Loss: 0.1853\n",
      "Epoch [7/10], Batch [100/1772], Loss: 0.3698\n",
      "Epoch [7/10], Batch [200/1772], Loss: 0.1612\n",
      "Epoch [7/10], Batch [300/1772], Loss: 0.0740\n",
      "Epoch [7/10], Batch [400/1772], Loss: 0.4001\n",
      "Epoch [7/10], Batch [500/1772], Loss: 0.2277\n",
      "Epoch [7/10], Batch [600/1772], Loss: 0.1696\n",
      "Epoch [7/10], Batch [700/1772], Loss: 0.2059\n",
      "Epoch [7/10], Batch [800/1772], Loss: 0.1282\n",
      "Epoch [7/10], Batch [900/1772], Loss: 0.2829\n",
      "Epoch [7/10], Batch [1000/1772], Loss: 0.1937\n",
      "Epoch [7/10], Batch [1100/1772], Loss: 0.2460\n",
      "Epoch [7/10], Batch [1200/1772], Loss: 0.3464\n",
      "Epoch [7/10], Batch [1300/1772], Loss: 0.1242\n",
      "Epoch [7/10], Batch [1400/1772], Loss: 0.1322\n",
      "Epoch [7/10], Batch [1500/1772], Loss: 0.1504\n",
      "Epoch [7/10], Batch [1600/1772], Loss: 0.1306\n",
      "Epoch [7/10], Batch [1700/1772], Loss: 0.2395\n",
      "Epoch [8/10], Batch [0/1772], Loss: 0.1670\n",
      "Epoch [8/10], Batch [100/1772], Loss: 0.2821\n",
      "Epoch [8/10], Batch [200/1772], Loss: 0.1877\n",
      "Epoch [8/10], Batch [300/1772], Loss: 0.4687\n",
      "Epoch [8/10], Batch [400/1772], Loss: 0.1843\n",
      "Epoch [8/10], Batch [500/1772], Loss: 0.1823\n",
      "Epoch [8/10], Batch [600/1772], Loss: 0.3755\n",
      "Epoch [8/10], Batch [700/1772], Loss: 0.2466\n",
      "Epoch [8/10], Batch [800/1772], Loss: 0.1515\n",
      "Epoch [8/10], Batch [900/1772], Loss: 0.5035\n",
      "Epoch [8/10], Batch [1000/1772], Loss: 0.3346\n",
      "Epoch [8/10], Batch [1100/1772], Loss: 0.3728\n",
      "Epoch [8/10], Batch [1200/1772], Loss: 0.2415\n",
      "Epoch [8/10], Batch [1300/1772], Loss: 0.2626\n",
      "Epoch [8/10], Batch [1400/1772], Loss: 0.3867\n",
      "Epoch [8/10], Batch [1500/1772], Loss: 0.3102\n",
      "Epoch [8/10], Batch [1600/1772], Loss: 0.3012\n",
      "Epoch [8/10], Batch [1700/1772], Loss: 0.2287\n",
      "Epoch [9/10], Batch [0/1772], Loss: 0.1212\n",
      "Epoch [9/10], Batch [100/1772], Loss: 0.1224\n",
      "Epoch [9/10], Batch [200/1772], Loss: 0.1229\n",
      "Epoch [9/10], Batch [300/1772], Loss: 0.3303\n",
      "Epoch [9/10], Batch [400/1772], Loss: 0.1378\n",
      "Epoch [9/10], Batch [500/1772], Loss: 0.2373\n",
      "Epoch [9/10], Batch [600/1772], Loss: 0.3315\n",
      "Epoch [9/10], Batch [700/1772], Loss: 0.2299\n",
      "Epoch [9/10], Batch [800/1772], Loss: 0.1986\n",
      "Epoch [9/10], Batch [900/1772], Loss: 0.3649\n",
      "Epoch [9/10], Batch [1000/1772], Loss: 0.2423\n",
      "Epoch [9/10], Batch [1100/1772], Loss: 0.0847\n",
      "Epoch [9/10], Batch [1200/1772], Loss: 0.2757\n",
      "Epoch [9/10], Batch [1300/1772], Loss: 0.3157\n",
      "Epoch [9/10], Batch [1400/1772], Loss: 0.2281\n",
      "Epoch [9/10], Batch [1500/1772], Loss: 0.1326\n",
      "Epoch [9/10], Batch [1600/1772], Loss: 0.2804\n",
      "Epoch [9/10], Batch [1700/1772], Loss: 0.2309\n",
      "Epoch [10/10], Batch [0/1772], Loss: 0.1154\n",
      "Epoch [10/10], Batch [100/1772], Loss: 0.2327\n",
      "Epoch [10/10], Batch [200/1772], Loss: 0.2108\n",
      "Epoch [10/10], Batch [300/1772], Loss: 0.1171\n",
      "Epoch [10/10], Batch [400/1772], Loss: 0.2387\n",
      "Epoch [10/10], Batch [500/1772], Loss: 0.1502\n",
      "Epoch [10/10], Batch [600/1772], Loss: 0.2880\n",
      "Epoch [10/10], Batch [700/1772], Loss: 0.4217\n",
      "Epoch [10/10], Batch [800/1772], Loss: 0.1301\n",
      "Epoch [10/10], Batch [900/1772], Loss: 0.2445\n",
      "Epoch [10/10], Batch [1000/1772], Loss: 0.3128\n",
      "Epoch [10/10], Batch [1100/1772], Loss: 0.1306\n",
      "Epoch [10/10], Batch [1200/1772], Loss: 0.1778\n",
      "Epoch [10/10], Batch [1300/1772], Loss: 0.0649\n",
      "Epoch [10/10], Batch [1400/1772], Loss: 0.1540\n",
      "Epoch [10/10], Batch [1500/1772], Loss: 0.0925\n",
      "Epoch [10/10], Batch [1600/1772], Loss: 0.1664\n",
      "Epoch [10/10], Batch [1700/1772], Loss: 0.2468\n"
     ]
    }
   ],
   "source": [
    "# 定义模型参数、实例化模型\n",
    "input_size = word2vec_model.vector_size\n",
    "# print( input_size )\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 2                                         # 情感态度二分类问题\n",
    "\n",
    "model = LSTM_model( input_size, hidden_size, num_layers, output_size )\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam( model.parameters(), lr = 0.0002 )\n",
    "\n",
    "loss_min = 1000\n",
    "for epoch in range( 10 ):\n",
    "    model.train()\n",
    "    for i, ( data, target ) in enumerate( train_dataloader ):\n",
    "        # print( data.shape )\n",
    "        outputs = model( data )\n",
    "        loss = loss_f( outputs, target )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print( 'Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'.format( epoch + 1, 10, i, len( train_dataloader ), loss.item() ) )\n",
    "        if loss.item() < loss_min:\n",
    "            loss_min = loss.item()\n",
    "            torch.save( model, '../data/LSTM_model.pth' )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:40:15.883145Z",
     "start_time": "2024-07-30T08:38:56.828302700Z"
    }
   },
   "id": "2c8fc0b48cfb13bf",
   "execution_count": 160
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.10%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    r = 0\n",
    "    total = 0\n",
    "    for data, target in test_dataloader:\n",
    "        outputs = model( data )\n",
    "        _, predicted = torch.max( outputs.data, 1 )\n",
    "        total += target.size( 0 )\n",
    "        r += ( predicted == target ).sum().item()\n",
    "        \n",
    "    acc = r / total\n",
    "    print( 'Test Accuracy: {:.2%}'.format( acc ) )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:40:16.089958200Z",
     "start_time": "2024-07-30T08:40:15.887280400Z"
    }
   },
   "id": "4fe88ead69f0c532",
   "execution_count": 161
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8448ced8586e414a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def drop_stopword( datas ):                                 # 去掉停用词、无意义符号\n",
    "    with open( '../data/cn_stopwords.txt', 'r', encoding = 'UTF8' ) as f:\n",
    "        stop_words = [ word.strip() for word in f.readlines() ]\n",
    "    pdata = []\n",
    "    for x in datas:\n",
    "        if x not in stop_words:\n",
    "            pdata.append( x )\n",
    "    return pdata\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:53:26.236095Z",
     "start_time": "2024-07-30T08:53:26.230525600Z"
    }
   },
   "id": "8be06783fcded09c",
   "execution_count": 165
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def input_text_process( text ):\n",
    "    text = list( jieba.cut( text ) )\n",
    "    text = drop_stopword( text )\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:50:46.788164Z",
     "start_time": "2024-07-30T08:50:46.782446600Z"
    }
   },
   "id": "d111605c691896db",
   "execution_count": 163
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果：positive\n"
     ]
    }
   ],
   "source": [
    "input_text = '非常好'\n",
    "# input_text = '糟糕透了'\n",
    "input_text = drop_stopword( input_text )\n",
    "input_text = [ [ text_to_vector( input_text ) ] ]\n",
    "\n",
    "\n",
    "model = torch.load( '../data/LSTM_model.pth' )\n",
    "label = [ 'negative', 'positive' ]\n",
    "input_tensor = torch.Tensor( np.array( input_text, dtype = np.float32 ) )\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model( input_tensor )\n",
    "pclass = label[ torch.argmax( output ).item() ]\n",
    "print( f'预测结果：{pclass}' )\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:55:12.298990900Z",
     "start_time": "2024-07-30T08:55:12.280563800Z"
    }
   },
   "id": "4bf0ac79bc264f76",
   "execution_count": 172
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "165747e71f368180"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
